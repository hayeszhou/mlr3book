[{"path":"index.html","id":"citation-info","chapter":"Citation Info","heading":"Citation Info","text":"cite book, please use following information:package mlr3, please cite JOSS paper:","code":"@misc{\n  title = {mlr3 book},\n  author = {Marc Becker and Martin Binder and Bernd Bischl and Michel Lang and Florian Pfisterer and Nicholas G. Reich and Jakob Richter and Patrick Schratz and Raphael Sonabend and Damir Pulatov},\n  url = {https://mlr3book.mlr-org.com},\n  year = {2021},\n  month = {09},\n  day = {19},\n}\nprint(citation(\"mlr3\"), style = \"bibtex\")## @Article{mlr3,\n##   title = {{mlr3}: A modern object-oriented machine learning framework in {R}},\n##   author = {Michel Lang and Martin Binder and Jakob Richter and Patrick Schratz and Florian Pfisterer and Stefan Coors and Quay Au and Giuseppe Casalicchio and Lars Kotthoff and Bernd Bischl},\n##   journal = {Journal of Open Source Software},\n##   year = {2019},\n##   month = {dec},\n##   doi = {10.21105/joss.01903},\n##   url = {https://joss.theoj.org/papers/10.21105/joss.01903},\n## }"},{"path":"quickstart.html","id":"quickstart","chapter":"Quickstart","heading":"Quickstart","text":"30-second introductory example, train decision tree model first 120 rows iris data set make predictions final 30, measuring accuracy trained model.examples can found mlr3gallery, collection use cases examples.learning mlr3, highly recommend print cheatsheets.","code":"\nlibrary(\"mlr3\")\ntask = tsk(\"iris\")\nlearner = lrn(\"classif.rpart\")\n\n# train a model of this learner for a subset of the task\nlearner$train(task, row_ids = 1:120)\n# this is what the decision tree looks like\nlearner$model## n= 120 \n## \n## node), split, n, loss, yval, (yprob)\n##       * denotes terminal node\n## \n## 1) root 120 70 setosa (0.41667 0.41667 0.16667)  \n##   2) Petal.Length< 2.45 50  0 setosa (1.00000 0.00000 0.00000) *\n##   3) Petal.Length>=2.45 70 20 versicolor (0.00000 0.71429 0.28571)  \n##     6) Petal.Length< 4.95 49  1 versicolor (0.00000 0.97959 0.02041) *\n##     7) Petal.Length>=4.95 21  2 virginica (0.00000 0.09524 0.90476) *\npredictions = learner$predict(task, row_ids = 121:150)\npredictions## <PredictionClassif> for 30 observations:\n##     row_ids     truth   response\n##         121 virginica  virginica\n##         122 virginica versicolor\n##         123 virginica  virginica\n## ---                             \n##         148 virginica  virginica\n##         149 virginica  virginica\n##         150 virginica  virginica\n# accuracy of our model on the test set of the final 30 rows\npredictions$score(msr(\"classif.acc\"))## classif.acc \n##      0.8333"},{"path":"introduction.html","id":"introduction","chapter":"1 Introduction and Overview","heading":"1 Introduction and Overview","text":"mlr3 (Lang et al. 2019) package ecosystem provide generic, object-oriented, extensible framework classification, regression, survival analysis, machine learning tasks R language (R Core Team 2019).\nimplement learners , provide unified interface many existing learners R.\nunified interface provides functionality extend combine existing learners, intelligently select tune appropriate technique task, perform large-scale comparisons enable meta-learning.\nExamples advanced functionality include hyperparameter tuning feature selection. Parallelization many operations natively supported.Target Audiencemlr3 provides domain-specific language machine learning R.\ntarget practitioners want quickly apply machine learning algorithms researchers want implement, benchmark, compare new methods structured environment.\npackage complete rewrite earlier version mlr leverages many years experience provide state---art system easy use extend.\nintended users basic knowledge machine learning R interested complex projects use advanced functionality well one-liners quickly prototype specific tasks.Rewrite?mlr (Bischl et al. 2016) first released CRAN 2013, core design architecture dating back much .\ntime, addition many features led considerably complex design made harder build, maintain, extent hoped .\nhindsight, saw design architecture choices mlr made difficult support new features, particular respect pipelines.\nFurthermore, R ecosystem well helpful packages data.table undergone major changes meantime.\nnearly impossible integrate changes original design mlr.\nInstead, decided start working reimplementation 2018, resulted first release mlr3 CRAN July 2019.\nnew design integration newly developed R packages (especially R6, future, data.table) makes mlr3 much easier use, maintain, efficient compared predecessor mlr.Design PrinciplesWe follow general design principles mlr3 package ecosystem.Backend frontend.\npackages mlr3 ecosystem focus processing transforming data, applying machine learning algorithms, computing results.\nprovide graphical user interfaces (GUIs); visualizations data results provided extra packages.Embrace R6 clean, object-oriented design, object state-changes, reference semantics.Embrace data.table fast convenient data frame computations.Unify container result classes much possible provide result data data.tables.\nconsiderably simplifies API allows easy selection “split-apply-combine” (aggregation) operations.\ncombine data.table R6 place references non-atomic compound objects tables make heavy use list columns.Defensive programming type safety.\nuser input checked checkmate (Lang 2017).\nReturn types documented, mechanisms popular base R “simplify” result unpredictably (e.g., sapply() drop argument [.data.frame) avoided.light dependencies.\nOne main maintenance burdens mlr keep changing learner interfaces behavior many packages depended .\nrequire far fewer packages mlr3 make installation maintenance easier.Package Ecosystemmlr3 builds upon following packages developed core members mlr3 team:R6:\nReference class objects.data.table:\nExtension R’s data.frame.digest:\nHash digests.uuid:\nUnique string identifiers.lgr:\nLogging facility.mlbench:\ncollection machine learning data sets.packages well curated mature, problems packet dependencies expected .\nAdditionally, following packages suggested extra functionality:parallelization: future / future.apply.progress bars: progressr.capturing output, warnings, exceptions: evaluate callr.mlr3 provides base functionality fundamental building blocks machine learning, following packages extend mlr3 capabilities preprocessing, pipelining, visualizations, additional learners additional task types:complete list links respective repositories can found wiki page extension packages.","code":""},{"path":"basics.html","id":"basics","chapter":"2 Basics","heading":"2 Basics","text":"chapter teach essential building blocks mlr3, well R6 classes operations used machine learning.\ntypical machine learning workflow looks like :data, mlr3 encapsulates tasks, split non-overlapping training test sets.\nSince interested models extrapolate new data rather just memorizing training data, separate test data allows objectively evaluate models respect generalization.\ntraining data given machine learning algorithm, call learner mlr3.\nlearner uses training data build model relationship input features output target values.\nmodel used produce predictions test data, compared ground truth values assess quality model.\nmlr3 offers number different measures quantify well model performs based difference predicted actual values.\nUsually measure numeric score.process splitting data training test sets, building model, evaluating may repeated several times, resampling different training test sets original data time.\nMultiple resampling iterations allow us get better, generalizable performance estimate particular type model tested different conditions less likely get lucky unlucky particular way data resampled.many cases, simple workflow sufficient deal real-world data, may require normalization, imputation missing values, feature selection.\ncover complex workflows allow even later book.chapter covers following subtopics:TasksTasks encapsulate data meta-information, name prediction target column.\ncover :access predefined tasks,specify task type,create task,work task’s API,assign roles rows columns task,implement task mutators, andretrieve data stored task.LearnersLearners encapsulate machine learning algorithms train models make predictions task.\nprovided R packages.\ncover :access set classification regression learners come mlr3 retrieve specific learner,access set hyperparameter values learner modify .modify extend learners covered supplemental advanced technical section.Train predictThe section train predict methods illustrates use tasks learners train model make predictions new data set.\nparticular, cover :properly set tasks learners,set train test splits task,train learner training set produce model,generate predictions test set, andassess performance model comparing predicted actual values.ResamplingA resampling method create training test splits.\ncover toaccess select resampling strategies,instantiate split training test sets applying resampling, andexecute resampling obtain results.Additional information resampling can found section nested resampling chapter model optimization.BenchmarkingBenchmarking used compare performance different models, example models trained different learners, different tasks, different resampling methods.\ncover tocreate benchmarking design,execute design aggregate results, andconvert benchmarking objects resample objects.Binary classificationBinary classification special case classification target variable predict two possible values.\ncase, additional considerations apply; particular:ROC curves threshold predict one class versus , andthreshold tuning (WIP).get details use mlr3 machine learning, give brief introduction R6 relatively new part R.\nmlr3 heavily relies R6 basic building blocks provides R6 classes:tasks,learners,measures, andresamplings.","code":""},{"path":"basics.html","id":"r6","chapter":"2 Basics","heading":"2.1 Quick R6 Intro for Beginners","text":"R6 one R’s recent dialects object-oriented programming (OO).\naddresses shortcomings earlier OO implementations R, S3, used mlr.\ndone object-oriented programming , R6 feel familiar.\nfocus parts R6 need know use mlr3 .Objects created calling constructor R6::R6Class() object, specifically initialization method $new().\nexample, foo = Foo$new(bar = 1) creates new object class Foo, setting bar argument constructor value 1.\nobjects mlr3 created special functions (e.g. ‘lrn(“regr.rpart”)’) also referred sugar functions.Objects mutable state, encapsulated fields, can accessed dollar operator.\ncan access bar value Foo class foo$bar set value assigning field, e.g. foo$bar = 2.addition fields, objects expose methods may allow inspect object’s state, retrieve information, perform action may change internal state object.\nexample, $train method learner changes internal state learner building storing trained model, can used make predictions given data.Objects can public private fields methods.\npublic fields methods define API interact object.\nPrivate methods relevant want extend mlr3, e.g. new learners.R6 objects internally environments, reference semantics.\nexample, foo2 = foo create copy foo foo2, another reference actual object.\nSetting foo$bar = 3 also change foo2$bar 3 vice versa.copy object, use $clone() method deep = TRUE argument nested objects, example, foo2 = foo$clone(deep = TRUE).details R6, look excellent R6 vignettes, especially introduction.","code":""},{"path":"basics.html","id":"tasks","chapter":"2 Basics","heading":"2.2 Tasks","text":"Tasks objects contain (usually tabular) data additional meta-data define machine learning problem.\nmeta-data , example, name target variable supervised machine learning problems, type dataset (e.g. spatial survival).\ninformation used specific operations can performed task.","code":""},{"path":"basics.html","id":"tasks-types","chapter":"2 Basics","heading":"2.2.1 Task Types","text":"create task data.frame(), data.table() Matrix(), first need select right task type:Classification Task: target label (stored character()orfactor()) distinct values.\n→ TaskClassif.Classification Task: target label (stored character()orfactor()) distinct values.\n→ TaskClassif.Regression Task: target numeric quantity (stored integer() double()).\n→ TaskRegr.Regression Task: target numeric quantity (stored integer() double()).\n→ TaskRegr.Survival Task: target (right-censored) time event. censoring types currently development.\n→ mlr3proba::TaskSurv add-package mlr3proba.Survival Task: target (right-censored) time event. censoring types currently development.\n→ mlr3proba::TaskSurv add-package mlr3proba.Density Task: unsupervised task estimate density.\n→ mlr3proba::TaskDens add-package mlr3proba.Density Task: unsupervised task estimate density.\n→ mlr3proba::TaskDens add-package mlr3proba.Cluster Task: unsupervised task type; target aim identify similar groups within feature space.\n→ mlr3cluster::TaskClust add-package mlr3cluster.Cluster Task: unsupervised task type; target aim identify similar groups within feature space.\n→ mlr3cluster::TaskClust add-package mlr3cluster.Spatial Task: Observations task spatio-temporal information (e.g. coordinates).\n→ mlr3spatiotempcv::TaskRegrST mlr3spatiotempcv::TaskClassifST add-package mlr3spatiotempcv.Spatial Task: Observations task spatio-temporal information (e.g. coordinates).\n→ mlr3spatiotempcv::TaskRegrST mlr3spatiotempcv::TaskClassifST add-package mlr3spatiotempcv.Ordinal Regression Task: target ordinal.\n→ TaskOrdinal add-package mlr3ordinal (still development).Ordinal Regression Task: target ordinal.\n→ TaskOrdinal add-package mlr3ordinal (still development).","code":""},{"path":"basics.html","id":"tasks-creation","chapter":"2 Basics","heading":"2.2.2 Task Creation","text":"example, create regression task using mtcars data set package datasets predict numeric target variable \"mpg\" (miles per gallon).\nconsider first two features dataset brevity.First, load prepare data.Next, create regression task, .e. construct new instance R6 class TaskRegr.\nUsually, done calling constructor TaskRegr$new().\nInstead, calling converter as_task_regr() convert data.frame() stored data task provide following information:x: Object convert. Works data.frame()/data.table()/tibble() abstract data backends implemented class DataBackendDataTable.\nlatter allows connect --memory storage systems like SQL servers via extension package mlr3db.target: name target column regression problem.id (optional): arbitrary identifier task, used plots summaries.\nprovided, deparsed substituted name x used.print() method gives short summary task:\n32 observations 3 columns, 2 features.can also plot task using mlr3viz package, gives graphical summary properties:Note instead loading extension packages individually, often convenient load mlr3verse package instead.\nmlr3verse imports mlr3 packages re-exports functions used regular machine learning data science tasks.","code":"\ndata(\"mtcars\", package = \"datasets\")\ndata = mtcars[, 1:3]\nstr(data)## 'data.frame':    32 obs. of  3 variables:\n##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n##  $ disp: num  160 160 108 258 360 ...\nlibrary(\"mlr3\")\n\ntask_mtcars = as_task_regr(data, target = \"mpg\", id = \"cars\")\nprint(task_mtcars)## <TaskRegr:cars> (32 x 3)\n## * Target: mpg\n## * Properties: -\n## * Features (2):\n##   - dbl (2): cyl, disp\nlibrary(\"mlr3viz\")\nautoplot(task_mtcars, type = \"pairs\")"},{"path":"basics.html","id":"tasks-predefined","chapter":"2 Basics","heading":"2.2.3 Predefined tasks","text":"mlr3 ships predefined machine learning tasks.\ntasks stored R6 Dictionary (key-value store) named mlr_tasks.\nPrinting gives keys (names datasets):can get informative summary example tasks converting dictionary data.table() object:display, columns \"lgl\" (logical), \"int\" (integer), \"dbl\" (double), \"chr\" (character), \"fct\" (factor), \"ord\" (ordered factor) \"pxc\" (POSIXct time) display number features dataset corresponding storage type.get task dictionary, one can use $get() method mlr_tasks class assign return value new object.\nSince mlr3 arranges object instances dictionaries extraction common task, shortcut : function tsk().\n, retrieve palmer penguins task originating package palmerpenguins:Note dictionaries mlr_tasks can get populated extension packages.\nE.g., mlr3data comes example toy tasks regression classification, mlr3proba ships additional survival density estimation tasks.\npackages get loaded load mlr3verse package, look available tasks :get information respective task, corresponding man page can found mlr_tasks_[id], e.g. mlr_tasks_german_credit.","code":"\nmlr_tasks## <DictionaryTask> with 11 stored values\n## Keys: boston_housing, breast_cancer, german_credit, iris, mtcars,\n##   penguins, pima, sonar, spam, wine, zoo\nas.data.table(mlr_tasks)##                key task_type nrow ncol properties lgl int dbl chr fct ord pxc\n##  1: boston_housing      regr  506   19              0   3  13   0   2   0   0\n##  2:  breast_cancer   classif  683   10   twoclass   0   0   0   0   0   9   0\n##  3:  german_credit   classif 1000   21   twoclass   0   3   0   0  14   3   0\n##  4:           iris   classif  150    5 multiclass   0   0   4   0   0   0   0\n##  5:         mtcars      regr   32   11              0   0  10   0   0   0   0\n##  6:       penguins   classif  344    8 multiclass   0   3   2   0   2   0   0\n##  7:           pima   classif  768    9   twoclass   0   0   8   0   0   0   0\n##  8:          sonar   classif  208   61   twoclass   0   0  60   0   0   0   0\n##  9:           spam   classif 4601   58   twoclass   0   0  57   0   0   0   0\n## 10:           wine   classif  178   14 multiclass   0   2  11   0   0   0   0\n## 11:            zoo   classif  101   17 multiclass  15   1   0   0   0   0   0\ntask_penguins = tsk(\"penguins\")\nprint(task_penguins)## <TaskClassif:penguins> (344 x 8)\n## * Target: species\n## * Properties: multiclass\n## * Features (7):\n##   - int (3): body_mass, flipper_length, year\n##   - dbl (2): bill_depth, bill_length\n##   - fct (2): island, sex\nlibrary(\"mlr3verse\")\nas.data.table(mlr_tasks)[, 1:4]##                key task_type  nrow ncol\n##  1:           actg      surv  1151   13\n##  2:   bike_sharing      regr 17379   14\n##  3: boston_housing      regr   506   19\n##  4:  breast_cancer   classif   683   10\n##  5:       faithful      dens   272    1\n##  6:           gbcs      surv   686   10\n##  7:  german_credit   classif  1000   21\n##  8:          grace      surv  1000    8\n##  9:           ilpd   classif   583   11\n## 10:           iris   classif   150    5\n## 11:     kc_housing      regr 21613   20\n## 12:           lung      surv   228   10\n## 13:      moneyball      regr  1232   15\n## 14:         mtcars      regr    32   11\n## 15:      optdigits   classif  5620   65\n## 16:       penguins   classif   344    8\n## 17:           pima   classif   768    9\n## 18:         precip      dens    70    1\n## 19:           rats      surv   300    5\n## 20:          sonar   classif   208   61\n## 21:           spam   classif  4601   58\n## 22:        titanic   classif  1309   11\n## 23:   unemployment      surv  3343    6\n## 24:      usarrests     clust    50    4\n## 25:           whas      surv   481   11\n## 26:           wine   classif   178   14\n## 27:            zoo   classif   101   17\n##                key task_type  nrow ncol"},{"path":"basics.html","id":"tasks-api","chapter":"2 Basics","heading":"2.2.4 Task API","text":"task properties characteristics can queried using task’s public fields methods (see Task).\nMethods can also used change stored data behavior task.","code":""},{"path":"basics.html","id":"tasks-retrieving","chapter":"2 Basics","heading":"2.2.4.1 Retrieving Data","text":"data stored task can retrieved directly fields, example:information can obtained methods object, example:mlr3, row (observation) unique identifier, stored integer().\ncan passed arguments $data() method select specific rows:Note although row ids typically just sequence 1 nrow(data), guaranteed unique natural numbers.\nKeep mind, especially work data stored real data base management system (see backends).Similarly row ids, target feature columns also unique identifiers, .e. names (stored character()).\nnames can accessed via public slots $feature_names $target_names.\n, “target” refers variable want predict “feature” predictors task.row_ids column names can combined selecting subset data:extract complete data task, one can also simply convert data.table:","code":"\ntask_mtcars## <TaskRegr:cars> (32 x 3)\n## * Target: mpg\n## * Properties: -\n## * Features (2):\n##   - dbl (2): cyl, disp\ntask_mtcars$nrow## [1] 32\ntask_mtcars$ncol## [1] 3\ntask_mtcars$data()##      mpg cyl  disp\n##  1: 21.0   6 160.0\n##  2: 21.0   6 160.0\n##  3: 22.8   4 108.0\n##  4: 21.4   6 258.0\n##  5: 18.7   8 360.0\n##  6: 18.1   6 225.0\n##  7: 14.3   8 360.0\n##  8: 24.4   4 146.7\n##  9: 22.8   4 140.8\n## 10: 19.2   6 167.6\n## 11: 17.8   6 167.6\n## 12: 16.4   8 275.8\n## 13: 17.3   8 275.8\n## 14: 15.2   8 275.8\n## 15: 10.4   8 472.0\n## 16: 10.4   8 460.0\n## 17: 14.7   8 440.0\n## 18: 32.4   4  78.7\n## 19: 30.4   4  75.7\n## 20: 33.9   4  71.1\n## 21: 21.5   4 120.1\n## 22: 15.5   8 318.0\n## 23: 15.2   8 304.0\n## 24: 13.3   8 350.0\n## 25: 19.2   8 400.0\n## 26: 27.3   4  79.0\n## 27: 26.0   4 120.3\n## 28: 30.4   4  95.1\n## 29: 15.8   8 351.0\n## 30: 19.7   6 145.0\n## 31: 15.0   8 301.0\n## 32: 21.4   4 121.0\n##      mpg cyl  disp\nhead(task_mtcars$row_ids)## [1] 1 2 3 4 5 6\n# retrieve data for rows with ids 1, 5, and 10\ntask_mtcars$data(rows = c(1, 5, 10))##     mpg cyl  disp\n## 1: 21.0   6 160.0\n## 2: 18.7   8 360.0\n## 3: 19.2   6 167.6\ntask_mtcars$feature_names## [1] \"cyl\"  \"disp\"\ntask_mtcars$target_names## [1] \"mpg\"\n# retrieve data for rows 1, 5, and 10 and only select column \"mpg\"\ntask_mtcars$data(rows = c(1, 5, 10), cols = \"mpg\")##     mpg\n## 1: 21.0\n## 2: 18.7\n## 3: 19.2\nsummary(as.data.table(task_mtcars))##       mpg            cyl            disp      \n##  Min.   :10.4   Min.   :4.00   Min.   : 71.1  \n##  1st Qu.:15.4   1st Qu.:4.00   1st Qu.:120.8  \n##  Median :19.2   Median :6.00   Median :196.3  \n##  Mean   :20.1   Mean   :6.19   Mean   :230.7  \n##  3rd Qu.:22.8   3rd Qu.:8.00   3rd Qu.:326.0  \n##  Max.   :33.9   Max.   :8.00   Max.   :472.0"},{"path":"basics.html","id":"tasks-roles","chapter":"2 Basics","heading":"2.2.4.2 Roles (Rows and Columns)","text":"possible assign different roles rows columns.\nroles affect behavior task different operations.\nalready seen target feature columns serve different purpose.example, previously-constructed mtcars task following column roles:Columns can also role (ignored) multiple roles.\nadd row names mtcars additional feature, first add data table regular column recreate task new column.row names now feature whose values stored column \"rn\".\ninclude column educational purposes .\nGenerally speaking, point feature uniquely identifies row.\nFurthermore, character data type cause problems many types machine learning algorithms.hand, identifier may useful label points plots, example identify label outliers.\nTherefore change role rn column removing list features assign new role \"name\".\ntwo ways :Use Task method $set_col_roles() (recommended).Simply modify field $col_roles, named list vectors column names.\nvector list corresponds column role, column names contained vector designated role.Supported column roles can found manual Task, just printing names field $col_roles:.Changing role change underlying data, just updates view .\ndata copied code .\nview changed -place though, .e. task object modified.Just like columns, also possible assign different roles rows.Rows can two different roles:Role use:\nRows generally available model fitting (although may also used test set resampling).\nrole default role.Role use:\nRows generally available model fitting (although may also used test set resampling).\nrole default role.Role validation:\nRows used training.\nRows missing values target column task creation automatically set validation role.Role validation:\nRows used training.\nRows missing values target column task creation automatically set validation role.several reasons hold observations back treat differently:often good practice validate final model external validation set identify possible overfitting.observations may unlabeled, e.g. competitions like Kaggle.observations used training model, can used get predictions.","code":"\nprint(task_mtcars$col_roles)## $feature\n## [1] \"cyl\"  \"disp\"\n## \n## $target\n## [1] \"mpg\"\n## \n## $name\n## character(0)\n## \n## $order\n## character(0)\n## \n## $stratum\n## character(0)\n## \n## $group\n## character(0)\n## \n## $weight\n## character(0)\n# with `keep.rownames`, data.table stores the row names in an extra column \"rn\"\ndata = as.data.table(datasets::mtcars[, 1:3], keep.rownames = TRUE)\ntask_mtcars = as_task_regr(data, target = \"mpg\", id = \"cars\")\n\n# there is a new feature called \"rn\"\ntask_mtcars$feature_names## [1] \"cyl\"  \"disp\" \"rn\"\n# supported column roles, see ?Task\nnames(task_mtcars$col_roles)## [1] \"feature\" \"target\"  \"name\"    \"order\"   \"stratum\" \"group\"   \"weight\"\n# assign column \"rn\" the role \"name\", remove from other roles\ntask_mtcars$set_col_roles(\"rn\", roles = \"name\")\n\n# note that \"rn\" not listed as feature anymore\ntask_mtcars$feature_names## [1] \"cyl\"  \"disp\"\n# \"rn\" also does not appear anymore when we access the data\ntask_mtcars$data(rows = 1:2)##    mpg cyl disp\n## 1:  21   6  160\n## 2:  21   6  160\ntask_mtcars$head(2)##    mpg cyl disp\n## 1:  21   6  160\n## 2:  21   6  160"},{"path":"basics.html","id":"tasks-mutators","chapter":"2 Basics","heading":"2.2.4.3 Task Mutators","text":"shown , modifying $col_roles $row_roles (either via set_col_roles()/set_row_roles() directly modifying named list) changes view data.\nadditional convenience method $filter() subsets current view based row ids $select() subsets view based feature names.methods discussed allow subset data, methods $rbind() $cbind() allow add extra rows columns task.\n, original data changed.\nadditional rows columns added view data.","code":"\ntask_penguins = tsk(\"penguins\")\ntask_penguins$select(c(\"body_mass\", \"flipper_length\")) # keep only these features\ntask_penguins$filter(1:3) # keep only these rows\ntask_penguins$head()##    species body_mass flipper_length\n## 1:  Adelie      3750            181\n## 2:  Adelie      3800            186\n## 3:  Adelie      3250            195\ntask_penguins$cbind(data.frame(letters = letters[1:3])) # add column foo\ntask_penguins$head()##    species body_mass flipper_length letters\n## 1:  Adelie      3750            181       a\n## 2:  Adelie      3800            186       b\n## 3:  Adelie      3250            195       c"},{"path":"basics.html","id":"autoplot-task","chapter":"2 Basics","heading":"2.2.5 Plotting Tasks","text":"mlr3viz package provides plotting facilities many classes implemented mlr3.\navailable plot types depend inherited class, plots returned ggplot2 objects can easily customized.classification tasks (inheriting TaskClassif), see documentation mlr3viz::autoplot.TaskClassif implemented plot types.\nexamples get impression:course, can regression tasks (inheriting TaskRegr) documented mlr3viz::autoplot.TaskRegr:","code":"\nlibrary(\"mlr3viz\")\n\n# get the pima indians task\ntask = tsk(\"pima\")\n\n# subset task to only use the 3 first features\ntask$select(head(task$feature_names, 3))\n\n# default plot: class frequencies\nautoplot(task)\n# pairs plot (requires package GGally)\nautoplot(task, type = \"pairs\")\n# duo plot (requires package GGally)\nautoplot(task, type = \"duo\")\nlibrary(\"mlr3viz\")\n\n# get the complete mtcars task\ntask = tsk(\"mtcars\")\n\n# subset task to only use the 3 first features\ntask$select(head(task$feature_names, 3))\n\n# default plot: boxplot of target variable\nautoplot(task)\n# pairs plot (requires package GGally)\nautoplot(task, type = \"pairs\")"},{"path":"basics.html","id":"learners","chapter":"2 Basics","heading":"2.3 Learners","text":"Objects class Learner provide unified interface many popular machine learning algorithms R.\nconsist methods train predict model Task provide meta-information learners, hyperparameters can set.base class learner Learner, specialized regression LearnerRegr classification LearnerClassif.\nExtension packages inherit Learner base class, e.g. mlr3proba::LearnerSurv mlr3cluster::LearnerClust.\ncontrast Task, creation custom Learner usually required advanced topic.\nHence, refer reader Section 6.1 proceed overview interface already implemented learners.training step: training data (features target) passed Learner’s $train() function trains stores model, .e. relationship target feature.predict step: new slice data, inference data, passed $predict() method Learner.\nmodel trained first step used predict missing target feature, e.g. labels classification problems numerical outcome regression problems.","code":""},{"path":"basics.html","id":"predefined-learners","chapter":"2 Basics","heading":"2.3.1 Predefined Learners","text":"mlr3 package ships following minimal set classification regression learners avoid unnecessary dependencies:mlr_learners_classif.featureless: Simple baseline classification learner (inheriting LearnerClassif).\ndefaults, constantly predicts label frequent training set.mlr_learners_regr.featureless: Simple baseline regression learner (inheriting LearnerRegr).\ndefaults, constantly predicts mean outcome training set.mlr_learners_classif.rpart: Single classification tree package rpart.mlr_learners_regr.rpart: Single regression tree package rpart.set baseline learners usually insufficient real data analysis.\nThus, cherry-picked one implementation popular machine learning method connected mlr3learners package:Linear logistic regressionPenalized Generalized Linear Models\\(k\\)-Nearest Neighbors regression classificationKrigingLinear Quadratic Discriminant AnalysisNaive BayesSupport-Vector machinesGradient BoostingRandom Forests regression, classification survivalMore machine learning methods alternative implementations collected mlr3extralearners repository.\nfull list implemented learners across packages given interactive list also via mlr3extralearners::list_mlr3learners().\nlatest build status learners listed .create object one predefined learners, need access mlr_learners Dictionary , similar mlr_tasks, gets automatically populated learners extension packages.obtain object dictionary can use lrn() generic mlr_learners$get() method, e.g. lrn(\"classif.rpart\").","code":"\nhead(mlr3extralearners::list_mlr3learners())##          name   class                 id      mlr3_package\n## 1: AdaBoostM1 classif classif.AdaBoostM1 mlr3extralearners\n## 2:       bart classif       classif.bart mlr3extralearners\n## 3:        C50 classif        classif.C50 mlr3extralearners\n## 4:   catboost classif   classif.catboost mlr3extralearners\n## 5:    cforest classif    classif.cforest mlr3extralearners\n## 6:      ctree classif      classif.ctree mlr3extralearners\n##         required_packages                                      properties\n## 1:                  RWeka                             multiclass,twoclass\n## 2:                 dbarts                                twoclass,weights\n## 3:                    C50            missings,multiclass,twoclass,weights\n## 4:               catboost importance,missings,multiclass,twoclass,weights\n## 5: partykit,sandwich,coin           multiclass,oob_error,twoclass,weights\n## 6: partykit,sandwich,coin                     multiclass,twoclass,weights\n##                     feature_types predict_types\n## 1:         numeric,factor,ordered response,prob\n## 2: integer,numeric,factor,ordered response,prob\n## 3:         numeric,factor,ordered response,prob\n## 4:         numeric,factor,ordered response,prob\n## 5: integer,numeric,factor,ordered response,prob\n## 6: integer,numeric,factor,ordered response,prob\n# load most mlr3 packages to populate the dictionary\nlibrary(\"mlr3verse\")\nmlr_learners## <DictionaryLearner> with 135 stored values\n## Keys: classif.AdaBoostM1, classif.bart, classif.C50, classif.catboost,\n##   classif.cforest, classif.ctree, classif.cv_glmnet, classif.debug,\n##   classif.earth, classif.extratrees, classif.featureless, classif.fnn,\n##   classif.gam, classif.gamboost, classif.gausspr, classif.gbm,\n##   classif.glmboost, classif.glmnet, classif.IBk, classif.J48,\n##   classif.JRip, classif.kknn, classif.ksvm, classif.lda,\n##   classif.liblinear, classif.lightgbm, classif.LMT, classif.log_reg,\n##   classif.lssvm, classif.mob, classif.multinom, classif.naive_bayes,\n##   classif.nnet, classif.OneR, classif.PART, classif.qda,\n##   classif.randomForest, classif.ranger, classif.rfsrc, classif.rpart,\n##   classif.svm, classif.xgboost, clust.agnes, clust.ap, clust.cmeans,\n##   clust.cobweb, clust.dbscan, clust.diana, clust.em, clust.fanny,\n##   clust.featureless, clust.ff, clust.hclust, clust.kkmeans,\n##   clust.kmeans, clust.MBatchKMeans, clust.meanshift, clust.pam,\n##   clust.SimpleKMeans, clust.xmeans, dens.hist, dens.kde, dens.kde_kd,\n##   dens.kde_ks, dens.locfit, dens.logspline, dens.mixed, dens.nonpar,\n##   dens.pen, dens.plug, dens.spline, regr.bart, regr.catboost,\n##   regr.cforest, regr.ctree, regr.cubist, regr.cv_glmnet, regr.earth,\n##   regr.extratrees, regr.featureless, regr.fnn, regr.gam, regr.gamboost,\n##   regr.gausspr, regr.gbm, regr.glm, regr.glmboost, regr.glmnet,\n##   regr.IBk, regr.kknn, regr.km, regr.ksvm, regr.liblinear,\n##   regr.lightgbm, regr.lm, regr.M5Rules, regr.mars, regr.mob,\n##   regr.randomForest, regr.ranger, regr.rfsrc, regr.rpart, regr.rvm,\n##   regr.svm, regr.xgboost, surv.akritas, surv.blackboost, surv.cforest,\n##   surv.coxboost, surv.coxph, surv.coxtime, surv.ctree,\n##   surv.cv_coxboost, surv.cv_glmnet, surv.deephit, surv.deepsurv,\n##   surv.dnnsurv, surv.flexible, surv.gamboost, surv.gbm, surv.glmboost,\n##   surv.glmnet, surv.kaplan, surv.loghaz, surv.mboost, surv.nelson,\n##   surv.obliqueRSF, surv.parametric, surv.pchazard, surv.penalized,\n##   surv.ranger, surv.rfsrc, surv.rpart, surv.svm, surv.xgboost"},{"path":"basics.html","id":"learner-api","chapter":"2 Basics","heading":"2.3.2 Learner API","text":"learner provides following meta-information:feature_types: type features learner can deal .packages: packages required train model learner make predictions.properties: additional properties capabilities.\nexample, learner property “missings” able handle missing feature values, “importance” computes allows extract data relative importance features.\ncomplete list available mlr3 reference.predict_types: possible prediction types. example, classification learner can predict labels (“response”) probabilities (“prob”). complete list possible predict types see mlr3 reference.can retrieve specific learner using id:field param_set stores description hyperparameters learner , ranges, defaults, current values:set current hyperparameter values stored values field param_set field.\ncan change current hyperparameter values assigning named list field:Note operation just overwrites previously set parameters.\njust want add new hyperparameter, retrieve current set parameter values, modify named list write back learner:updates cp 0.02 keeps previously set parameter xval.Note lrn() function also accepts additional arguments used update hyperparameters set fields learner one go:","code":"\nlearner = lrn(\"classif.rpart\")\nprint(learner)## <LearnerClassifRpart:classif.rpart>\n## * Model: -\n## * Parameters: xval=0\n## * Packages: rpart\n## * Predict Type: response\n## * Feature types: logical, integer, numeric, factor, ordered\n## * Properties: importance, missings, multiclass, selected_features,\n##   twoclass, weights\nlearner$param_set## <ParamSet>\n##                 id    class lower upper nlevels        default value\n##  1:             cp ParamDbl     0     1     Inf           0.01      \n##  2:     keep_model ParamLgl    NA    NA       2          FALSE      \n##  3:     maxcompete ParamInt     0   Inf     Inf              4      \n##  4:       maxdepth ParamInt     1    30      30             30      \n##  5:   maxsurrogate ParamInt     0   Inf     Inf              5      \n##  6:      minbucket ParamInt     1   Inf     Inf <NoDefault[3]>      \n##  7:       minsplit ParamInt     1   Inf     Inf             20      \n##  8: surrogatestyle ParamInt     0     1       2              0      \n##  9:   usesurrogate ParamInt     0     2       3              2      \n## 10:           xval ParamInt     0   Inf     Inf             10     0\nlearner$param_set$values = list(cp = 0.01, xval = 0)\nlearner## <LearnerClassifRpart:classif.rpart>\n## * Model: -\n## * Parameters: cp=0.01, xval=0\n## * Packages: rpart\n## * Predict Type: response\n## * Feature types: logical, integer, numeric, factor, ordered\n## * Properties: importance, missings, multiclass, selected_features,\n##   twoclass, weights\npv = learner$param_set$values\npv$cp = 0.02\nlearner$param_set$values = pv\nlearner = lrn(\"classif.rpart\", id = \"rp\", cp = 0.001)\nlearner$id## [1] \"rp\"\nlearner$param_set$values## $xval\n## [1] 0\n## \n## $cp\n## [1] 0.001"},{"path":"basics.html","id":"train-predict","chapter":"2 Basics","heading":"2.4 Train, Predict, Score","text":"section, explain tasks learners can used train model predict new dataset.\nconcept demonstrated supervised classification using penguins dataset rpart learner, builds singe classification tree.Training learner means fitting model given data set.\nSubsequently, want predict label new observations.\npredictions compared ground truth values order assess predictive performance model.","code":""},{"path":"basics.html","id":"train-predict-objects","chapter":"2 Basics","heading":"2.4.1 Creating Task and Learner Objects","text":"First , load mlr3verse package.Next, retrieve task learner mlr_tasks (shortcut tsk()) mlr_learners (shortcut lrn()), respectively:classification task:learner classification tree:","code":"\nlibrary(\"mlr3verse\")\ntask = tsk(\"penguins\")\nlearner = lrn(\"classif.rpart\")"},{"path":"basics.html","id":"split-data","chapter":"2 Basics","heading":"2.4.2 Setting up the train/test splits of the data","text":"common train majority data.\nuse 80% available observations predict remaining 20%.\npurpose, create two index vectors:Section 2.5 learn mlr3 can automatically create training test sets based different resampling strategies.","code":"\ntrain_set = sample(task$nrow, 0.8 * task$nrow)\ntest_set = setdiff(seq_len(task$nrow), train_set)"},{"path":"basics.html","id":"training","chapter":"2 Basics","heading":"2.4.3 Training the learner","text":"field $model stores model produced training step.\n$train() method called learner object, field NULL:Next, classification tree trained using train set sonar task calling $train() method Learner:operation modifies learner -place.\ncan now access stored model via field $model:","code":"\nlearner$model## NULL\nlearner$train(task, row_ids = train_set)\nprint(learner$model)## n= 275 \n## \n## node), split, n, loss, yval, (yprob)\n##       * denotes terminal node\n## \n## 1) root 275 158 Adelie (0.42545 0.20000 0.37455)  \n##   2) flipper_length< 207.5 167  52 Adelie (0.68862 0.31138 0.00000)  \n##     4) bill_length< 43.35 115   3 Adelie (0.97391 0.02609 0.00000) *\n##     5) bill_length>=43.35 52   3 Chinstrap (0.05769 0.94231 0.00000) *\n##   3) flipper_length>=207.5 108   5 Gentoo (0.01852 0.02778 0.95370) *"},{"path":"basics.html","id":"predicting","chapter":"2 Basics","heading":"2.4.4 Predicting","text":"model trained, use remaining part data prediction.\nRemember initially split data train_set test_set.$predict() method Learner returns Prediction object.\nprecisely, LearnerClassif returns PredictionClassif object.prediction objects holds row ids test data, respective true label target column respective predictions.\nsimplest way extract information converting Prediction object data.table():classification, can also extract confusion matrix:","code":"\nprediction = learner$predict(task, row_ids = test_set)\nprint(prediction)## <PredictionClassif> for 69 observations:\n##     row_ids     truth  response\n##           3    Adelie    Adelie\n##           8    Adelie    Adelie\n##          15    Adelie    Adelie\n## ---                            \n##         335 Chinstrap Chinstrap\n##         336 Chinstrap Chinstrap\n##         344 Chinstrap Chinstrap\nhead(as.data.table(prediction))##    row_ids  truth  response\n## 1:       3 Adelie    Adelie\n## 2:       8 Adelie    Adelie\n## 3:      15 Adelie    Adelie\n## 4:      20 Adelie Chinstrap\n## 5:      23 Adelie    Adelie\n## 6:      24 Adelie    Adelie\nprediction$confusion##            truth\n## response    Adelie Chinstrap Gentoo\n##   Adelie        34         2      0\n##   Chinstrap      1        10      2\n##   Gentoo         0         1     19"},{"path":"basics.html","id":"predict-type","chapter":"2 Basics","heading":"2.4.5 Changing the Predict Type","text":"Classification learners default predicting class label.\nHowever, many classifiers additionally also tell sure predicted label providing posterior probabilities.\nswitch predicting probabilities, predict_type field LearnerClassif must changed \"response\" \"prob\" training:prediction object now contains probabilities class labels:Analogously predicting probabilities, many regression learners support extraction standard error estimates setting predict type \"se\".","code":"\nlearner$predict_type = \"prob\"\n\n# re-fit the model\nlearner$train(task, row_ids = train_set)\n\n# rebuild prediction object\nprediction = learner$predict(task, row_ids = test_set)\n# data.table conversion\nhead(as.data.table(prediction))##    row_ids  truth  response prob.Adelie prob.Chinstrap prob.Gentoo\n## 1:       3 Adelie    Adelie     0.97391        0.02609           0\n## 2:       8 Adelie    Adelie     0.97391        0.02609           0\n## 3:      15 Adelie    Adelie     0.97391        0.02609           0\n## 4:      20 Adelie Chinstrap     0.05769        0.94231           0\n## 5:      23 Adelie    Adelie     0.97391        0.02609           0\n## 6:      24 Adelie    Adelie     0.97391        0.02609           0\n# directly access the predicted labels:\nhead(prediction$response)## [1] Adelie    Adelie    Adelie    Chinstrap Adelie    Adelie   \n## Levels: Adelie Chinstrap Gentoo\n# directly access the matrix of probabilities:\nhead(prediction$prob)##       Adelie Chinstrap Gentoo\n## [1,] 0.97391   0.02609      0\n## [2,] 0.97391   0.02609      0\n## [3,] 0.97391   0.02609      0\n## [4,] 0.05769   0.94231      0\n## [5,] 0.97391   0.02609      0\n## [6,] 0.97391   0.02609      0"},{"path":"basics.html","id":"autoplot-prediction","chapter":"2 Basics","heading":"2.4.6 Plotting Predictions","text":"Analogously plotting tasks, mlr3viz provides autoplot() method Prediction objects.\navailable types listed manual page autoplot.PredictionClassif() autoplot.PredictionRegr(), respectively.","code":"\ntask = tsk(\"penguins\")\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\")\nlearner$train(task)\nprediction = learner$predict(task)\nautoplot(prediction)"},{"path":"basics.html","id":"measure","chapter":"2 Basics","heading":"2.4.7 Performance assessment","text":"last step modeling usually performance assessment.\nassess quality predictions, predicted labels compared true labels.\ncomparison calculated defined measure, given Measure object.\nNote prediction made dataset without target column, .e. without true labels, performance can calculated.Predefined available measures stored mlr_measures (convenience getter msr()):choose accuracy (classif.acc) specific performance measure call method $score() Prediction object quantify predictive performance.Note , measure specified, classification defaults classification error (classif.ce) regression defaults mean squared error (regr.mse).","code":"\nmlr_measures## <DictionaryMeasure> with 85 stored values\n## Keys: aic, bic, classif.acc, classif.auc, classif.bacc, classif.bbrier,\n##   classif.ce, classif.costs, classif.dor, classif.fbeta, classif.fdr,\n##   classif.fn, classif.fnr, classif.fomr, classif.fp, classif.fpr,\n##   classif.logloss, classif.mbrier, classif.mcc, classif.npv,\n##   classif.ppv, classif.prauc, classif.precision, classif.recall,\n##   classif.sensitivity, classif.specificity, classif.tn, classif.tnr,\n##   classif.tp, classif.tpr, clust.ch, clust.db, clust.dunn,\n##   clust.silhouette, clust.wss, debug, dens.logloss, oob_error,\n##   regr.bias, regr.ktau, regr.mae, regr.mape, regr.maxae, regr.medae,\n##   regr.medse, regr.mse, regr.msle, regr.pbias, regr.rae, regr.rmse,\n##   regr.rmsle, regr.rrse, regr.rse, regr.rsq, regr.sae, regr.smape,\n##   regr.srho, regr.sse, selected_features, surv.brier, surv.calib_alpha,\n##   surv.calib_beta, surv.chambless_auc, surv.cindex, surv.dcalib,\n##   surv.graf, surv.hung_auc, surv.intlogloss, surv.logloss, surv.mae,\n##   surv.mse, surv.nagelk_r2, surv.oquigley_r2, surv.rmse, surv.schmid,\n##   surv.song_auc, surv.song_tnr, surv.song_tpr, surv.uno_auc,\n##   surv.uno_tnr, surv.uno_tpr, surv.xu_r2, time_both, time_predict,\n##   time_train\nmeasure = msr(\"classif.acc\")\nprint(measure)## <MeasureClassifSimple:classif.acc>\n## * Packages: mlr3measures\n## * Range: [0, 1]\n## * Minimize: FALSE\n## * Parameters: list()\n## * Properties: -\n## * Predict type: response\nprediction$score(measure)## classif.acc \n##      0.9651"},{"path":"basics.html","id":"resampling","chapter":"2 Basics","heading":"2.5 Resampling","text":"Resampling strategies usually used assess performance learning algorithm.\nmlr3 entails following predefined resampling strategies:cross validation (\"cv\"),leave-one-cross validation (\"loo\"),repeated cross validation (\"repeated_cv\"),bootstrapping (\"bootstrap\"),subsampling (\"subsampling\"),holdout (\"holdout\"),-sample resampling (\"insample\"), andcustom resampling (\"custom\").following sections provide guidance set select resampling strategy subsequently instantiate resampling process.graphical illustration resampling process:","code":""},{"path":"basics.html","id":"resampling-settings","chapter":"2 Basics","heading":"2.5.1 Settings","text":"example use penguins task simple classification tree rpart package .performing resampling dataset, first need define approach used.\nmlr3 resampling strategies parameters can queried looking data.table output mlr_resamplings dictionary:Additional resampling methods special use cases available via extension packages, mlr3spatiotemporal spatial data.model fit conducted train/predict/score chapter equivalent “holdout resampling”, let’s consider one first.\n, can retrieve elements dictionary mlr_resamplings via $get() convenience functionrsmp():Note $is_instantiated field set FALSE.\nmeans actually apply strategy dataset yet.\nApplying strategy dataset done next section Instantiation.default get .66/.33 split data.\ntwo ways ratio can changed:Overwriting slot $param_set$values using named list:Specifying resampling parameters directly construction:","code":"\nlibrary(\"mlr3verse\")\n\ntask = tsk(\"penguins\")\nlearner = lrn(\"classif.rpart\")\nas.data.table(mlr_resamplings)##            key        params iters\n## 1:   bootstrap ratio,repeats    30\n## 2:      custom                  NA\n## 3:   custom_cv                  NA\n## 4:          cv         folds    10\n## 5:     holdout         ratio     1\n## 6:    insample                   1\n## 7:         loo                  NA\n## 8: repeated_cv folds,repeats   100\n## 9: subsampling ratio,repeats    30\nresampling = rsmp(\"holdout\")\nprint(resampling)## <ResamplingHoldout> with 1 iterations\n## * Instantiated: FALSE\n## * Parameters: ratio=0.6667\nresampling$param_set$values = list(ratio = 0.8)\nrsmp(\"holdout\", ratio = 0.8)## <ResamplingHoldout> with 1 iterations\n## * Instantiated: FALSE\n## * Parameters: ratio=0.8"},{"path":"basics.html","id":"resampling-inst","chapter":"2 Basics","heading":"2.5.2 Instantiation","text":"far just set stage selected resampling strategy.actually perform splitting obtain indices training test split resampling needs Task.\ncalling method instantiate(), split indices data indices training test sets.\nresulting indices stored Resampling objects.\nbetter illustrate following operations, switch 3-fold cross-validation:Note want compare multiple Learners fair manner, using instantiated resampling learner mandatory.\nway greatly simplify comparison multiple learners discussed next section benchmarking.","code":"\nresampling = rsmp(\"cv\", folds = 3)\nresampling$instantiate(task)\nresampling$iters## [1] 3\nstr(resampling$train_set(1))##  int [1:229] 3 9 11 16 19 20 21 25 29 33 ...\nstr(resampling$test_set(1))##  int [1:115] 1 2 8 12 15 22 23 24 36 38 ..."},{"path":"basics.html","id":"resampling-exec","chapter":"2 Basics","heading":"2.5.3 Execution","text":"Task, Learner Resampling object can call resample(), repeatedly fits learner task hand according given resampling strategy.\nturn creates ResampleResult object.\ntell resample() keep fitted models setting store_models option TRUEand start computation:returned ResampleResult stored rr provides various getters access stored information:Calculate average performance across resampling iterations:\n\nrr$aggregate(msr(\"classif.ce\"))\n## classif.ce \n##    0.06974Calculate average performance across resampling iterations:Extract performance individual resampling iterations:\n\nrr$score(msr(\"classif.ce\"))\n##                 task  task_id                   learner    learner_id\n## 1: <TaskClassif[47]> penguins <LearnerClassifRpart[36]> classif.rpart\n## 2: <TaskClassif[47]> penguins <LearnerClassifRpart[36]> classif.rpart\n## 3: <TaskClassif[47]> penguins <LearnerClassifRpart[36]> classif.rpart\n##            resampling resampling_id iteration              prediction\n## 1: <ResamplingCV[19]>            cv         1 <PredictionClassif[19]>\n## 2: <ResamplingCV[19]>            cv         2 <PredictionClassif[19]>\n## 3: <ResamplingCV[19]>            cv         3 <PredictionClassif[19]>\n##    classif.ce\n## 1:    0.11304\n## 2:    0.03478\n## 3:    0.06140Extract performance individual resampling iterations:Check warnings errors:\n\nrr$warnings\n## Empty data.table (0 rows 2 cols): iteration,msg\n\nrr$errors\n## Empty data.table (0 rows 2 cols): iteration,msgCheck warnings errors:Extract inspect resampling splits:\n\nrr$resampling\n## <ResamplingCV> 3 iterations\n## * Instantiated: TRUE\n## * Parameters: folds=3\n\nrr$resampling$iters\n## [1] 3\n\nstr(rr$resampling$test_set(1))\n##  int [1:115] 1 3 9 10 11 13 19 23 26 27 ...\n\nstr(rr$resampling$train_set(1))\n##  int [1:229] 2 4 14 15 16 22 25 29 30 36 ...Extract inspect resampling splits:Retrieve learner specific iteration inspect :\n\nlrn = rr$learners[[1]]\nlrn$model\n## n= 229 \n## \n## node), split, n, loss, yval, (yprob)\n##       * denotes terminal node\n## \n##  1) root 229 131 Adelie (0.427948 0.196507 0.375546)  \n##    2) flipper_length< 207.5 142  44 Adelie (0.690141 0.302817 0.007042)  \n##      4) bill_length< 43.15 94   1 Adelie (0.989362 0.010638 0.000000) *\n##      5) bill_length>=43.15 48   6 Chinstrap (0.104167 0.875000 0.020833)  \n##       10) body_mass>=4125 9   4 Adelie (0.555556 0.333333 0.111111) *\n##       11) body_mass< 4125 39   0 Chinstrap (0.000000 1.000000 0.000000) *\n##    3) flipper_length>=207.5 87   2 Gentoo (0.000000 0.022989 0.977011) *Retrieve learner specific iteration inspect :Extract predictions:\n\nrr$prediction() # predictions merged single Prediction object\n## <PredictionClassif> 344 observations:\n##     row_ids     truth  response prob.Adelie prob.Chinstrap prob.Gentoo\n##           1    Adelie    Adelie      0.9894        0.01064           0\n##           3    Adelie    Adelie      0.9894        0.01064           0\n##           9    Adelie    Adelie      0.9894        0.01064           0\n## ---                                                                   \n##         337 Chinstrap Chinstrap      0.1000        0.90000           0\n##         340 Chinstrap Chinstrap      0.1000        0.90000           0\n##         341 Chinstrap Chinstrap      0.1000        0.90000           0\n\nrr$predictions()[[1]] # prediction first resampling iteration\n## <PredictionClassif> 115 observations:\n##     row_ids     truth  response prob.Adelie prob.Chinstrap prob.Gentoo\n##           1    Adelie    Adelie      0.9894        0.01064           0\n##           3    Adelie    Adelie      0.9894        0.01064           0\n##           9    Adelie    Adelie      0.9894        0.01064           0\n## ---                                                                   \n##         338 Chinstrap Chinstrap      0.0000        1.00000           0\n##         339 Chinstrap Chinstrap      0.0000        1.00000           0\n##         344 Chinstrap Chinstrap      0.0000        1.00000           0Extract predictions:Filter keep specified iterations:\n\nrr$filter(c(1, 3))\nprint(rr)\n## <ResampleResult> 2 iterations\n## * Task: penguins\n## * Learner: classif.rpart\n## * Warnings: 0 0 iterations\n## * Errors: 0 0 iterationsFilter keep specified iterations:","code":"\ntask = tsk(\"penguins\")\nlearner = lrn(\"classif.rpart\", maxdepth = 3, predict_type = \"prob\")\nresampling = rsmp(\"cv\", folds = 3)\n\nrr = resample(task, learner, resampling, store_models = TRUE)\nprint(rr)## <ResampleResult> of 3 iterations\n## * Task: penguins\n## * Learner: classif.rpart\n## * Warnings: 0 in 0 iterations\n## * Errors: 0 in 0 iterations\nrr$aggregate(msr(\"classif.ce\"))## classif.ce \n##    0.06974\nrr$score(msr(\"classif.ce\"))##                 task  task_id                   learner    learner_id\n## 1: <TaskClassif[47]> penguins <LearnerClassifRpart[36]> classif.rpart\n## 2: <TaskClassif[47]> penguins <LearnerClassifRpart[36]> classif.rpart\n## 3: <TaskClassif[47]> penguins <LearnerClassifRpart[36]> classif.rpart\n##            resampling resampling_id iteration              prediction\n## 1: <ResamplingCV[19]>            cv         1 <PredictionClassif[19]>\n## 2: <ResamplingCV[19]>            cv         2 <PredictionClassif[19]>\n## 3: <ResamplingCV[19]>            cv         3 <PredictionClassif[19]>\n##    classif.ce\n## 1:    0.11304\n## 2:    0.03478\n## 3:    0.06140\nrr$warnings## Empty data.table (0 rows and 2 cols): iteration,msg\nrr$errors## Empty data.table (0 rows and 2 cols): iteration,msg\nrr$resampling## <ResamplingCV> with 3 iterations\n## * Instantiated: TRUE\n## * Parameters: folds=3\nrr$resampling$iters## [1] 3\nstr(rr$resampling$test_set(1))##  int [1:115] 1 3 9 10 11 13 19 23 26 27 ...\nstr(rr$resampling$train_set(1))##  int [1:229] 2 4 14 15 16 22 25 29 30 36 ...\nlrn = rr$learners[[1]]\nlrn$model## n= 229 \n## \n## node), split, n, loss, yval, (yprob)\n##       * denotes terminal node\n## \n##  1) root 229 131 Adelie (0.427948 0.196507 0.375546)  \n##    2) flipper_length< 207.5 142  44 Adelie (0.690141 0.302817 0.007042)  \n##      4) bill_length< 43.15 94   1 Adelie (0.989362 0.010638 0.000000) *\n##      5) bill_length>=43.15 48   6 Chinstrap (0.104167 0.875000 0.020833)  \n##       10) body_mass>=4125 9   4 Adelie (0.555556 0.333333 0.111111) *\n##       11) body_mass< 4125 39   0 Chinstrap (0.000000 1.000000 0.000000) *\n##    3) flipper_length>=207.5 87   2 Gentoo (0.000000 0.022989 0.977011) *\nrr$prediction() # all predictions merged into a single Prediction object## <PredictionClassif> for 344 observations:\n##     row_ids     truth  response prob.Adelie prob.Chinstrap prob.Gentoo\n##           1    Adelie    Adelie      0.9894        0.01064           0\n##           3    Adelie    Adelie      0.9894        0.01064           0\n##           9    Adelie    Adelie      0.9894        0.01064           0\n## ---                                                                   \n##         337 Chinstrap Chinstrap      0.1000        0.90000           0\n##         340 Chinstrap Chinstrap      0.1000        0.90000           0\n##         341 Chinstrap Chinstrap      0.1000        0.90000           0\nrr$predictions()[[1]] # prediction of first resampling iteration## <PredictionClassif> for 115 observations:\n##     row_ids     truth  response prob.Adelie prob.Chinstrap prob.Gentoo\n##           1    Adelie    Adelie      0.9894        0.01064           0\n##           3    Adelie    Adelie      0.9894        0.01064           0\n##           9    Adelie    Adelie      0.9894        0.01064           0\n## ---                                                                   \n##         338 Chinstrap Chinstrap      0.0000        1.00000           0\n##         339 Chinstrap Chinstrap      0.0000        1.00000           0\n##         344 Chinstrap Chinstrap      0.0000        1.00000           0\nrr$filter(c(1, 3))\nprint(rr)## <ResampleResult> of 2 iterations\n## * Task: penguins\n## * Learner: classif.rpart\n## * Warnings: 0 in 0 iterations\n## * Errors: 0 in 0 iterations"},{"path":"basics.html","id":"resamp-custom","chapter":"2 Basics","heading":"2.5.4 Custom resampling","text":"Sometimes necessary perform resampling custom splits, e.g. reproduce results reported study.\nmanual resampling instance can created using \"custom\" template.","code":"\nresampling = rsmp(\"custom\")\nresampling$instantiate(task,\n  train = list(c(1:10, 51:60, 101:110)),\n  test = list(c(11:20, 61:70, 111:120))\n)\nresampling$iters## [1] 1\nresampling$train_set(1)##  [1]   1   2   3   4   5   6   7   8   9  10  51  52  53  54  55  56  57  58  59\n## [20]  60 101 102 103 104 105 106 107 108 109 110\nresampling$test_set(1)##  [1]  11  12  13  14  15  16  17  18  19  20  61  62  63  64  65  66  67  68  69\n## [20]  70 111 112 113 114 115 116 117 118 119 120"},{"path":"basics.html","id":"resampling-with-predefined-groups","chapter":"2 Basics","heading":"2.5.5 Resampling with (predefined) groups","text":"One can denote certain observations grouped resampling, meaning always either belong train test set.\ncharacteristic must defined via column role \"group\" Task creation (see also help page column role).\n{mlr} previously called “blocking”.\nSee also mlr3gallery post topic practical example.even strict approach supply custom grouping structure solely makes one fold.\n{mlr3} supports via \"custom_cv\" resampling method.\nmethod accepts factor vector length task$nrow() string existing variable data.\nNote approach allow setting “folds” argument number folds determined number factor levels.\npredefined approach called “grouping” mlr2.\nterm “blocking” overloaded resampling context. field spatial resampling (see r mlr_pkg(“mlr3spatiotempcv”)), “blocking” used general concept grouping observations hand also specific resampling methods “blocking” name (`“spcv_block”) focus rectangular partitioning.\n","code":""},{"path":"basics.html","id":"autoplot-resampleresult","chapter":"2 Basics","heading":"2.5.6 Plotting Resample Results","text":"mlr3viz provides autoplot() method.\nshowcase plots, create binary classification task two features, perform resampling 10-fold cross validation visualize results:available plot types listed manual page autoplot.ResampleResult().","code":"\ntask = tsk(\"pima\")\ntask$select(c(\"glucose\", \"mass\"))\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\")\nrr = resample(task, learner, rsmp(\"cv\"), store_models = TRUE)\n\n# boxplot of AUC values across the 10 folds\nautoplot(rr, measure = msr(\"classif.auc\"))\n# ROC curve, averaged over 10 folds\nautoplot(rr, type = \"roc\")\n# learner predictions for first fold\nrr$filter(1)\nautoplot(rr, type = \"prediction\")## Warning: Removed 1 rows containing missing values (geom_point)."},{"path":"basics.html","id":"autoplot-resample-partition","chapter":"2 Basics","heading":"2.5.7 Plotting Resample Partitions","text":"mlr3spatiotempcv provides autoplot() methods visualize resampling partitions spatiotemporal datasets.\nSee function reference vignette “Spatiotemporal visualization” info.","code":"## Warning: Ignoring unknown parameters: crs\n\n## Warning: Ignoring unknown parameters: crs"},{"path":"basics.html","id":"benchmarking","chapter":"2 Basics","heading":"2.6 Benchmarking","text":"Comparing performance different learners multiple tasks /different resampling schemes common task.\noperation usually referred “benchmarking” field machine-learning.\nmlr3 package offers benchmark() convenience function.","code":""},{"path":"basics.html","id":"bm-design","chapter":"2 Basics","heading":"2.6.1 Design Creation","text":"mlr3 require supply “design” benchmark experiment.\ndesign essentially table settings want execute.\nconsists unique combinations Task, Learner Resampling triplets.use benchmark_grid() function create exhaustive design instantiate resampling properly, learners executed train/test split tasks.\nset learners predict probabilities also tell predict observations training set (setting predict_sets c(\"train\", \"test\")).\nAdditionally, use tsks(), lrns(), rsmps() retrieve lists Task, Learner Resampling fashion tsk(), lrn() rsmp().created design can passed benchmark() start computation.\nalso possible create custom design manually.\nHowever, create custom task data.table(), train/test splits different row design manually instantiate resampling creating design.\nSee help page benchmark_grid() example.","code":"\nlibrary(\"mlr3verse\")\n\ndesign = benchmark_grid(\n  tasks = tsks(c(\"spam\", \"german_credit\", \"sonar\")),\n  learners = lrns(c(\"classif.ranger\", \"classif.rpart\", \"classif.featureless\"),\n    predict_type = \"prob\", predict_sets = c(\"train\", \"test\")),\n  resamplings = rsmps(\"cv\", folds = 3)\n)\nprint(design)##                 task                         learner         resampling\n## 1: <TaskClassif[47]>      <LearnerClassifRanger[36]> <ResamplingCV[19]>\n## 2: <TaskClassif[47]>       <LearnerClassifRpart[36]> <ResamplingCV[19]>\n## 3: <TaskClassif[47]> <LearnerClassifFeatureless[36]> <ResamplingCV[19]>\n## 4: <TaskClassif[47]>      <LearnerClassifRanger[36]> <ResamplingCV[19]>\n## 5: <TaskClassif[47]>       <LearnerClassifRpart[36]> <ResamplingCV[19]>\n## 6: <TaskClassif[47]> <LearnerClassifFeatureless[36]> <ResamplingCV[19]>\n## 7: <TaskClassif[47]>      <LearnerClassifRanger[36]> <ResamplingCV[19]>\n## 8: <TaskClassif[47]>       <LearnerClassifRpart[36]> <ResamplingCV[19]>\n## 9: <TaskClassif[47]> <LearnerClassifFeatureless[36]> <ResamplingCV[19]>"},{"path":"basics.html","id":"bm-exec","chapter":"2 Basics","heading":"2.6.2 Execution and Aggregation of Results","text":"benchmark design ready, can directly call benchmark():Note instantiate resampling instance manually.\nbenchmark_grid() took care us:\nresampling strategy instantiated task construction exhaustive grid.benchmarking done, can aggregate performance $aggregate().\ncreate two measures calculate AUC training set predict set:can aggregate results even .\nexample, might interested know learner performed best tasks simultaneously.\nSimply aggregating performances mean usually statistically sound.\nInstead, calculate rank statistic learner grouped task.\ncalculated ranks grouped learner aggregated data.table.\nSince AUC needs maximized, multiply values \\(-1\\) best learner rank \\(1\\).Unsurprisingly, featureless learner outperformed training test set.\nclassification forest also outperforms single classification tree.","code":"\n# execute the benchmark\nbmr = benchmark(design)\nmeasures = list(\n  msr(\"classif.auc\", predict_sets = \"train\", id = \"auc_train\"),\n  msr(\"classif.auc\", id = \"auc_test\")\n)\n\ntab = bmr$aggregate(measures)\nprint(tab)##    nr      resample_result       task_id          learner_id resampling_id\n## 1:  1 <ResampleResult[20]>          spam      classif.ranger            cv\n## 2:  2 <ResampleResult[20]>          spam       classif.rpart            cv\n## 3:  3 <ResampleResult[20]>          spam classif.featureless            cv\n## 4:  4 <ResampleResult[20]> german_credit      classif.ranger            cv\n## 5:  5 <ResampleResult[20]> german_credit       classif.rpart            cv\n## 6:  6 <ResampleResult[20]> german_credit classif.featureless            cv\n## 7:  7 <ResampleResult[20]>         sonar      classif.ranger            cv\n## 8:  8 <ResampleResult[20]>         sonar       classif.rpart            cv\n## 9:  9 <ResampleResult[20]>         sonar classif.featureless            cv\n##    iters auc_train auc_test\n## 1:     3    0.9994   0.9858\n## 2:     3    0.9104   0.9026\n## 3:     3    0.5000   0.5000\n## 4:     3    0.9986   0.7944\n## 5:     3    0.8143   0.7012\n## 6:     3    0.5000   0.5000\n## 7:     3    1.0000   0.9326\n## 8:     3    0.9129   0.7478\n## 9:     3    0.5000   0.5000\nlibrary(\"data.table\")\n# group by levels of task_id, return columns:\n# - learner_id\n# - rank of col '-auc_train' (per level of learner_id)\n# - rank of col '-auc_test' (per level of learner_id)\nranks = tab[, .(learner_id, rank_train = rank(-auc_train), rank_test = rank(-auc_test)), by = task_id]\nprint(ranks)##          task_id          learner_id rank_train rank_test\n## 1:          spam      classif.ranger          1         1\n## 2:          spam       classif.rpart          2         2\n## 3:          spam classif.featureless          3         3\n## 4: german_credit      classif.ranger          1         1\n## 5: german_credit       classif.rpart          2         2\n## 6: german_credit classif.featureless          3         3\n## 7:         sonar      classif.ranger          1         1\n## 8:         sonar       classif.rpart          2         2\n## 9:         sonar classif.featureless          3         3\n# group by levels of learner_id, return columns:\n# - mean rank of col 'rank_train' (per level of learner_id)\n# - mean rank of col 'rank_test' (per level of learner_id)\nranks = ranks[, .(mrank_train = mean(rank_train), mrank_test = mean(rank_test)), by = learner_id]\n\n# print the final table, ordered by mean rank of AUC test\nranks[order(mrank_test)]##             learner_id mrank_train mrank_test\n## 1:      classif.ranger           1          1\n## 2:       classif.rpart           2          2\n## 3: classif.featureless           3          3"},{"path":"basics.html","id":"autoplot-benchmarkresult","chapter":"2 Basics","heading":"2.6.3 Plotting Benchmark Results","text":"Analogously plotting tasks, predictions resample results, mlr3viz also provides autoplot() method benchmark results.can also plot ROC curves.\n, first need filter BenchmarkResult contain single Task:available plot types listed manual page autoplot.BenchmarkResult().","code":"\nautoplot(bmr) + ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1))\nbmr_small = bmr$clone()$filter(task_id = \"german_credit\")\nautoplot(bmr_small, type = \"roc\")"},{"path":"basics.html","id":"bm-resamp","chapter":"2 Basics","heading":"2.6.4 Extracting ResampleResults","text":"BenchmarkResult object essentially collection multiple ResampleResult objects.\nstored column aggregated data.table(), can easily extract :can now investigate resampling even single resampling iterations using one approaches shown previous section:","code":"\ntab = bmr$aggregate(measures)\nrr = tab[task_id == \"german_credit\" & learner_id == \"classif.ranger\"]$resample_result[[1]]\nprint(rr)## <ResampleResult> of 3 iterations\n## * Task: german_credit\n## * Learner: classif.ranger\n## * Warnings: 0 in 0 iterations\n## * Errors: 0 in 0 iterations\nmeasure = msr(\"classif.auc\")\nrr$aggregate(measure)## classif.auc \n##      0.7944\n# get the iteration with worst AUC\nperf = rr$score(measure)\ni = which.min(perf$classif.auc)\n\n# get the corresponding learner and train set\nprint(rr$learners[[i]])## <LearnerClassifRanger:classif.ranger>\n## * Model: -\n## * Parameters: num.threads=1\n## * Packages: ranger\n## * Predict Type: prob\n## * Feature types: logical, integer, numeric, character, factor, ordered\n## * Properties: importance, multiclass, oob_error, twoclass, weights\nhead(rr$resampling$train_set(i))## [1]  4  5  6  7  9 16"},{"path":"basics.html","id":"converting-and-merging","chapter":"2 Basics","heading":"2.6.5 Converting and Merging","text":"ResampleResult can casted BenchmarkResult using converter as_benchmark_result().\nAdditionally, two BenchmarkResults can merged larger result object.","code":"\ntask = tsk(\"iris\")\nresampling = rsmp(\"holdout\")$instantiate(task)\n\nrr1 = resample(task, lrn(\"classif.rpart\"), resampling)\nrr2 = resample(task, lrn(\"classif.featureless\"), resampling)\n\n# Cast both ResampleResults to BenchmarkResults\nbmr1 = as_benchmark_result(rr1)\nbmr2 = as_benchmark_result(rr2)\n\n# Merge 2nd BMR into the first BMR\nbmr1$combine(bmr2)\n\nbmr1## <BenchmarkResult> of 2 rows with 2 resampling runs\n##  nr task_id          learner_id resampling_id iters warnings errors\n##   1    iris       classif.rpart       holdout     1        0      0\n##   2    iris classif.featureless       holdout     1        0      0"},{"path":"basics.html","id":"binary-classification","chapter":"2 Basics","heading":"2.7 Binary classification","text":"Classification problems target variable containing two classes called “binary”.\nbinary target variables, can specify positive class within classification task object task creation.\nexplicitly set construction, positive class defaults first level target variable.","code":"\n# during construction\ndata(\"Sonar\", package = \"mlbench\")\ntask = as_task_classif(Sonar, target = \"Class\", positive = \"R\")\n\n# switch positive class to level 'M'\ntask$positive = \"M\""},{"path":"basics.html","id":"binary-roc","chapter":"2 Basics","heading":"2.7.1 ROC Curve and Thresholds","text":"ROC Analysis, stands “receiver operating characteristics”, subfield machine learning studies evaluation binary prediction systems.\nsaw earlier one can retrieve confusion matrix Prediction accessing $confusion field:confusion matrix contains counts correct incorrect class assignments, grouped class labels.\ncolumns illustrate true (observed) labels rows display predicted labels.\npositive always first row column confusion matrix.\nThus, element \\(C_{11}\\) number times model predicted positive class right .\nAnalogously, element \\(C_{22}\\) number times model predicted negative class also right .\nelements diagonal called True Positives (TP) True Negatives (TN).\nelement \\(C_{12}\\) number times falsely predicted positive label, called False Positives (FP).\nelement \\(C_{21}\\) called False Negatives (FN).can now normalize rows columns confusion matrix derive several informative metrics:True Positive Rate (TPR): many true positives predict positive?True Negative Rate (TNR): many true negatives predict negative?Positive Predictive Value PPV: predict positive likely true positive?Negative Predictive Value NPV: predict negative likely true negative?Source: WikipediaIt difficult achieve high TPR low FPR conjunction, one uses constructing ROC Curve.\ncharacterize classifier TPR FPR values plot coordinate system.\nbest classifier lies top-left corner.\nworst classifier lies diagonal.\n\nClassifiers lying diagonal produce random labels (different proportions).\npositive \\(x\\) randomly classified 25% “positive”, get TPR 0.25.\nassign negative \\(x\\) randomly “positive” get FPR 0.25.\npractice, never obtain classifier diagonal, inverting predicted labels result reflection diagonal.\nscoring classifier model produces scores probabilities, instead discrete labels.\nobtain probabilities learner mlr3, set predict_type = \"prob\" ref(\"LearnerClassif\").\nWhether classifier can predict probabilities given $predict_types field.\nThresholding flexibly converts measured probabilities labels.\nPredict \\(1\\) (positive class) \\(\\hat{f}(x) > \\tau\\) else predict \\(0\\).\nNormally, one use \\(\\tau = 0.5\\) convert probabilities labels, imbalanced cost-sensitive situations another threshold suitable.\nthresholding, metric defined labels can used.mlr3 prediction objects, ROC curve can easily created mlr3viz relies precrec calculate plot ROC curves:","code":"\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\")\npred = learner$train(task)$predict(task)\nC = pred$confusion\nprint(C)##         truth\n## response  M  R\n##        M 95 10\n##        R 16 87\nlibrary(\"mlr3viz\")\n\n# TPR vs FPR / Sensitivity vs (1 - Specificity)\nautoplot(pred, type = \"roc\")\n# Precision vs Recall\nautoplot(pred, type = \"prc\")"},{"path":"basics.html","id":"threshold-tuning","chapter":"2 Basics","heading":"2.7.2 Threshold Tuning","text":"Learners can predict probability positive class usually use simple rule determine predicted class label: probability exceeds threshold \\(t = 0.5\\), pick positive label, select negative label otherwise.\nmodel well calibrated class labels heavily unbalanced, selecting different threshold can help improve predictive performance w.r.t. chosen performance measure., change threshold \\(t = 0.2\\), improving True Positive Rate (TPR).\nNote new threshold observations positive class get correctly classified positive label, time True Negative Rate (TNR) decreases.\nDepending application, may desired trade-.Thresholds can also tuned mlr3pipelines package, .e. using PipeOpTuneThreshold.","code":"\nmeasures = msrs(c(\"classif.tpr\", \"classif.tnr\"))\npred$confusion##         truth\n## response  M  R\n##        M 95 10\n##        R 16 87\npred$score(measures)## classif.tpr classif.tnr \n##      0.8559      0.8969\npred$set_threshold(0.2)\npred$confusion##         truth\n## response   M   R\n##        M 104  25\n##        R   7  72\npred$score(measures)## classif.tpr classif.tnr \n##      0.9369      0.7423"},{"path":"optimization.html","id":"optimization","chapter":"3 Model Optimization","heading":"3 Model Optimization","text":"Model TuningMachine learning algorithms default values set hyperparameters.\nIrrespective, hyperparameters need changed user achieve optimal performance given dataset.\nmanual selection hyperparameter values recommended approach rarely leads best performance.\nsubstantiate validity selected hyperparameters (= tuning), data-driven optimization recommended.\norder tune machine learning algorithm, one specify (1) search space, (2) optimization algorithm (aka tuning method), (3) evaluation method, .e., resampling strategy (4) performance measure.summary, sub-chapter tuning illustrates :undertake empirically sound hyperparameter selectionselect optimizing algorithmwrite search spaces conciselytrigger tuningautomate tuningThis sub-chapter also requires package mlr3tuning, extension package supports hyperparameter tuning.Feature SelectionThe second part chapter explains feature selection, also known variable selection.\nFeature selection process finding subset relevant features data.\nreasons perform selection:enhance interpretability model,speed model fitting orimprove learner performance reducing noise data.book focus mainly last aspect.\nDifferent approaches exist identify relevant features.\nsub-chapter feature selection, emphasize three methods:Filter algorithms select features independently learner according score.Variable importance filters select features important according learner.Wrapper methods iteratively select features optimize performance measure.Note, filters require learner.\nVariable importance filters require learner can calculate feature importance values trained.\nobtained importance values can used subset data, can used train learner.\nWrapper methods can used learner need train learner multiple times.Nested ResamplingIn order get good estimate generalization performance avoid data leakage, outer (performance) inner (tuning/feature selection) resampling process necessary.\nfollowing features discussed chapter:Inner outer resampling strategies nested resamplingThe execution nested resamplingThe evaluation executed resampling iterationsThis sub-chapter provide instructions implement nested resampling, accounting inner outer resampling mlr3.","code":""},{"path":"optimization.html","id":"tuning","chapter":"3 Model Optimization","heading":"3.1 Hyperparameter Tuning","text":"Hyperparameters second-order parameters machine learning models , often explicitly optimized model estimation process, can important impact outcome predictive performance model.\nTypically, hyperparameters fixed training model.\nHowever, output model can sensitive specification hyperparameters, often recommended make informed decision hyperparameter settings may yield better model performance.\nmany cases, hyperparameter settings may chosen priori, can advantageous try different settings fitting model training data.\nprocess often called model ‘tuning’.Hyperparameter tuning supported via mlr3tuning extension package.\ncan find illustration process:heart mlr3tuning R6 classes:TuningInstanceSingleCrit, TuningInstanceMultiCrit: two classes describe tuning problem store results.Tuner: class base class implementations tuning algorithms.","code":""},{"path":"optimization.html","id":"tuning-optimization","chapter":"3 Model Optimization","heading":"3.1.1 The TuningInstance* Classes","text":"following sub-section examines optimization simple classification tree Pima Indian Diabetes data set.use classification tree rpart choose subset hyperparameters want tune.\noften referred “tuning space”., opt tune two parameters:complexity cpThe termination criterion minsplitThe tuning space needs bounded, therefore one set lower upper bounds:Next, need specify evaluate performance.\n, need choose resampling strategy performance measure.Finally, one select budget available, solve tuning instance.\ndone selecting one available Terminators:Terminate given time (TerminatorClockTime)Terminate given amount iterations (TerminatorEvals)Terminate specific performance reached (TerminatorPerfReached)Terminate tuning improve (TerminatorStagnation)combination fashion (TerminatorCombo)short introduction, specify budget 20 evaluations put everything together TuningInstanceSingleCrit:start tuning, still need select optimization take place.\nwords, need choose optimization algorithm via Tuner class.","code":"\nlibrary(\"mlr3verse\")\ntask = tsk(\"pima\")\nprint(task)## <TaskClassif:pima> (768 x 9)\n## * Target: diabetes\n## * Properties: twoclass\n## * Features (8):\n##   - dbl (8): age, glucose, insulin, mass, pedigree, pregnant, pressure,\n##     triceps\nlearner = lrn(\"classif.rpart\")\nlearner$param_set## <ParamSet>\n##                 id    class lower upper nlevels        default value\n##  1:             cp ParamDbl     0     1     Inf           0.01      \n##  2:     keep_model ParamLgl    NA    NA       2          FALSE      \n##  3:     maxcompete ParamInt     0   Inf     Inf              4      \n##  4:       maxdepth ParamInt     1    30      30             30      \n##  5:   maxsurrogate ParamInt     0   Inf     Inf              5      \n##  6:      minbucket ParamInt     1   Inf     Inf <NoDefault[3]>      \n##  7:       minsplit ParamInt     1   Inf     Inf             20      \n##  8: surrogatestyle ParamInt     0     1       2              0      \n##  9:   usesurrogate ParamInt     0     2       3              2      \n## 10:           xval ParamInt     0   Inf     Inf             10     0\nsearch_space = ps(\n  cp = p_dbl(lower = 0.001, upper = 0.1),\n  minsplit = p_int(lower = 1, upper = 10)\n)\nsearch_space## <ParamSet>\n##          id    class lower upper nlevels        default value\n## 1:       cp ParamDbl 0.001   0.1     Inf <NoDefault[3]>      \n## 2: minsplit ParamInt 1.000  10.0      10 <NoDefault[3]>\nhout = rsmp(\"holdout\")\nmeasure = msr(\"classif.ce\")\nlibrary(\"mlr3tuning\")## Loading required package: paradox\nevals20 = trm(\"evals\", n_evals = 20)\n\ninstance = TuningInstanceSingleCrit$new(\n  task = task,\n  learner = learner,\n  resampling = hout,\n  measure = measure,\n  search_space = search_space,\n  terminator = evals20\n)\ninstance## <TuningInstanceSingleCrit>\n## * State:  Not optimized\n## * Objective: <ObjectiveTuning:classif.rpart_on_pima>\n## * Search Space:\n## <ParamSet>\n##          id    class lower upper nlevels        default value\n## 1:       cp ParamDbl 0.001   0.1     Inf <NoDefault[3]>      \n## 2: minsplit ParamInt 1.000  10.0      10 <NoDefault[3]>      \n## * Terminator: <TerminatorEvals>\n## * Terminated: FALSE\n## * Archive:\n## <ArchiveTuning>\n## Null data.table (0 rows and 0 cols)"},{"path":"optimization.html","id":"tuning-algorithms","chapter":"3 Model Optimization","heading":"3.1.2 The Tuner Class","text":"following algorithms currently implemented mlr3tuning:Grid Search (TunerGridSearch)Random Search (TunerRandomSearch) (Bergstra Bengio 2012)Generalized Simulated Annealing (TunerGenSA)Non-Linear Optimization (TunerNLoptr)example, use simple grid search grid resolution 5.Since numeric parameters, TunerGridSearch create equidistant grid respective upper lower bounds.\ntwo hyperparameters resolution 5, two-dimensional grid consists \\(5^2 = 25\\) configurations.\nconfiguration serves hyperparameter setting previously defined Learner fitted task using provided Resampling.\nconfigurations examined tuner (random order), either configurations evaluated Terminator signals budget exhausted.","code":"\ntuner = tnr(\"grid_search\", resolution = 5)"},{"path":"optimization.html","id":"tuning-triggering","chapter":"3 Model Optimization","heading":"3.1.3 Triggering the Tuning","text":"start tuning, simply pass TuningInstanceSingleCrit $optimize() method initialized Tuner.\ntuner proceeds follows:Tuner proposes least one hyperparameter configuration (Tuner may propose multiple points improve parallelization, can controlled via setting batch_size).configuration, given Learner fitted Task using provided Resampling.\nevaluations stored archive TuningInstanceSingleCrit.Terminator queried budget exhausted.\nbudget exhausted, restart 1) .Determine configuration best observed performance.Store best configurations result instance object.\nbest hyperparameter settings ($result_learner_param_vals) corresponding measured performance ($result_y) can accessed instance.One can investigate resamplings undertaken, stored archive TuningInstanceSingleCrit can accessed using .data.table():sum, grid search evaluated 20/25 different configurations grid random order Terminator stopped tuning.associated resampling iterations can accessed BenchmarkResult:uhash column links resampling iterations evaluated configurations stored instance$archive$data. allows e.g. score included ResampleResults different measure.Now optimized hyperparameters can take previously created Learner, set returned hyperparameters train full dataset.trained model can now used make prediction external data.\nNote predicting observations present task, avoided.\nmodel seen observations already tuning therefore results statistically biased.\nHence, resulting performance measure -optimistic.\nInstead, get statistically unbiased performance estimates current task, nested resampling required.","code":"\ntuner$optimize(instance)## INFO  [14:21:46.366] [bbotk] Starting to optimize 2 parameter(s) with '<OptimizerGridSearch>' and '<TerminatorEvals> [n_evals=20, k=0]' \n## INFO  [14:21:46.402] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:46.579] [bbotk] Result of batch 1: \n## INFO  [14:21:46.581] [bbotk]      cp minsplit classif.ce runtime_learners \n## INFO  [14:21:46.581] [bbotk]  0.0505       10       0.25            0.017 \n## INFO  [14:21:46.581] [bbotk]                                 uhash \n## INFO  [14:21:46.581] [bbotk]  063add42-bb3b-4e95-bea0-5949ff3db42c \n## INFO  [14:21:46.583] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:46.685] [bbotk] Result of batch 2: \n## INFO  [14:21:46.687] [bbotk]      cp minsplit classif.ce runtime_learners \n## INFO  [14:21:46.687] [bbotk]  0.0505        1       0.25            0.009 \n## INFO  [14:21:46.687] [bbotk]                                 uhash \n## INFO  [14:21:46.687] [bbotk]  f7a9dbb3-adc0-47f7-9f7f-bce6a868901d \n## INFO  [14:21:46.689] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:46.779] [bbotk] Result of batch 3: \n## INFO  [14:21:46.781] [bbotk]      cp minsplit classif.ce runtime_learners \n## INFO  [14:21:46.781] [bbotk]  0.0505        3       0.25            0.009 \n## INFO  [14:21:46.781] [bbotk]                                 uhash \n## INFO  [14:21:46.781] [bbotk]  7e16fb27-e9ee-4242-a5cb-24ab96448c54 \n## INFO  [14:21:46.782] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:46.878] [bbotk] Result of batch 4: \n## INFO  [14:21:46.879] [bbotk]       cp minsplit classif.ce runtime_learners \n## INFO  [14:21:46.879] [bbotk]  0.02575        8     0.2148            0.009 \n## INFO  [14:21:46.879] [bbotk]                                 uhash \n## INFO  [14:21:46.879] [bbotk]  14dd5c15-2660-403c-bd45-930364d7b8e0 \n## INFO  [14:21:46.881] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:46.971] [bbotk] Result of batch 5: \n## INFO  [14:21:46.973] [bbotk]       cp minsplit classif.ce runtime_learners \n## INFO  [14:21:46.973] [bbotk]  0.07525        5       0.25             0.01 \n## INFO  [14:21:46.973] [bbotk]                                 uhash \n## INFO  [14:21:46.973] [bbotk]  0c498a40-916e-4407-a57e-6ac55ca26140 \n## INFO  [14:21:46.975] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:47.068] [bbotk] Result of batch 6: \n## INFO  [14:21:47.070] [bbotk]       cp minsplit classif.ce runtime_learners \n## INFO  [14:21:47.070] [bbotk]  0.07525        8       0.25            0.009 \n## INFO  [14:21:47.070] [bbotk]                                 uhash \n## INFO  [14:21:47.070] [bbotk]  76b60a6b-26f0-485e-8193-4bf34e23fb6f \n## INFO  [14:21:47.078] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:47.164] [bbotk] Result of batch 7: \n## INFO  [14:21:47.166] [bbotk]   cp minsplit classif.ce runtime_learners                                uhash \n## INFO  [14:21:47.166] [bbotk]  0.1        5       0.25            0.009 fd978152-4f4f-4e57-98e7-b262c98332ca \n## INFO  [14:21:47.167] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:47.263] [bbotk] Result of batch 8: \n## INFO  [14:21:47.265] [bbotk]     cp minsplit classif.ce runtime_learners \n## INFO  [14:21:47.265] [bbotk]  0.001        1     0.3008            0.011 \n## INFO  [14:21:47.265] [bbotk]                                 uhash \n## INFO  [14:21:47.265] [bbotk]  547394f0-657f-452d-8801-6bba95e6d7bf \n## INFO  [14:21:47.266] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:47.443] [bbotk] Result of batch 9: \n## INFO  [14:21:47.445] [bbotk]     cp minsplit classif.ce runtime_learners \n## INFO  [14:21:47.445] [bbotk]  0.001        8     0.3008            0.011 \n## INFO  [14:21:47.445] [bbotk]                                 uhash \n## INFO  [14:21:47.445] [bbotk]  45ddb76e-9907-454d-a18b-6cc16be789fa \n## INFO  [14:21:47.447] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:47.542] [bbotk] Result of batch 10: \n## INFO  [14:21:47.544] [bbotk]       cp minsplit classif.ce runtime_learners \n## INFO  [14:21:47.544] [bbotk]  0.07525        1       0.25             0.01 \n## INFO  [14:21:47.544] [bbotk]                                 uhash \n## INFO  [14:21:47.544] [bbotk]  d52c48ac-10fa-4f62-b948-4328c68c5790 \n## INFO  [14:21:47.546] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:47.636] [bbotk] Result of batch 11: \n## INFO  [14:21:47.638] [bbotk]   cp minsplit classif.ce runtime_learners                                uhash \n## INFO  [14:21:47.638] [bbotk]  0.1        3       0.25            0.015 9fca5392-f1cc-4ff1-a337-79d0b543bb6c \n## INFO  [14:21:47.639] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:47.727] [bbotk] Result of batch 12: \n## INFO  [14:21:47.728] [bbotk]      cp minsplit classif.ce runtime_learners \n## INFO  [14:21:47.728] [bbotk]  0.0505        5       0.25            0.009 \n## INFO  [14:21:47.728] [bbotk]                                 uhash \n## INFO  [14:21:47.728] [bbotk]  2b70991a-1b2b-415b-a6bf-3b8f15f671a1 \n## INFO  [14:21:47.730] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:47.822] [bbotk] Result of batch 13: \n## INFO  [14:21:47.824] [bbotk]     cp minsplit classif.ce runtime_learners \n## INFO  [14:21:47.824] [bbotk]  0.001        5      0.293            0.011 \n## INFO  [14:21:47.824] [bbotk]                                 uhash \n## INFO  [14:21:47.824] [bbotk]  1c10594d-82c7-4740-aa7d-26aff0525cc0 \n## INFO  [14:21:47.825] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:47.911] [bbotk] Result of batch 14: \n## INFO  [14:21:47.913] [bbotk]       cp minsplit classif.ce runtime_learners \n## INFO  [14:21:47.913] [bbotk]  0.02575        5     0.2148            0.008 \n## INFO  [14:21:47.913] [bbotk]                                 uhash \n## INFO  [14:21:47.913] [bbotk]  dd97cfc0-1c3e-4a53-afa7-4b7499ba37fc \n## INFO  [14:21:47.915] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:48.011] [bbotk] Result of batch 15: \n## INFO  [14:21:48.013] [bbotk]       cp minsplit classif.ce runtime_learners \n## INFO  [14:21:48.013] [bbotk]  0.02575        3     0.2148            0.009 \n## INFO  [14:21:48.013] [bbotk]                                 uhash \n## INFO  [14:21:48.013] [bbotk]  423587ef-fdc5-4a67-80fa-01d2c856a679 \n## INFO  [14:21:48.014] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:48.104] [bbotk] Result of batch 16: \n## INFO  [14:21:48.105] [bbotk]       cp minsplit classif.ce runtime_learners \n## INFO  [14:21:48.105] [bbotk]  0.07525       10       0.25            0.009 \n## INFO  [14:21:48.105] [bbotk]                                 uhash \n## INFO  [14:21:48.105] [bbotk]  8abdbab9-914c-4850-8a6d-888d26852a19 \n## INFO  [14:21:48.107] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:48.201] [bbotk] Result of batch 17: \n## INFO  [14:21:48.203] [bbotk]   cp minsplit classif.ce runtime_learners                                uhash \n## INFO  [14:21:48.203] [bbotk]  0.1        1       0.25            0.009 371d7ab6-69bf-4fe5-ba1f-b49468d4c8d0 \n## INFO  [14:21:48.205] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:48.295] [bbotk] Result of batch 18: \n## INFO  [14:21:48.297] [bbotk]       cp minsplit classif.ce runtime_learners \n## INFO  [14:21:48.297] [bbotk]  0.07525        3       0.25            0.009 \n## INFO  [14:21:48.297] [bbotk]                                 uhash \n## INFO  [14:21:48.297] [bbotk]  0ff48422-8552-490b-b5f4-60c65ae3c295 \n## INFO  [14:21:48.299] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:48.398] [bbotk] Result of batch 19: \n## INFO  [14:21:48.400] [bbotk]     cp minsplit classif.ce runtime_learners \n## INFO  [14:21:48.400] [bbotk]  0.001        3     0.2969            0.012 \n## INFO  [14:21:48.400] [bbotk]                                 uhash \n## INFO  [14:21:48.400] [bbotk]  eb6ead2b-d1c5-4428-bcf3-dbc91a8df1a7 \n## INFO  [14:21:48.402] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:48.492] [bbotk] Result of batch 20: \n## INFO  [14:21:48.494] [bbotk]       cp minsplit classif.ce runtime_learners \n## INFO  [14:21:48.494] [bbotk]  0.02575       10     0.2148            0.009 \n## INFO  [14:21:48.494] [bbotk]                                 uhash \n## INFO  [14:21:48.494] [bbotk]  1ac1273f-90b5-41ce-8672-4d673169185d \n## INFO  [14:21:48.499] [bbotk] Finished optimizing after 20 evaluation(s) \n## INFO  [14:21:48.500] [bbotk] Result: \n## INFO  [14:21:48.502] [bbotk]       cp minsplit learner_param_vals  x_domain classif.ce \n## INFO  [14:21:48.502] [bbotk]  0.02575        8          <list[3]> <list[2]>     0.2148##         cp minsplit learner_param_vals  x_domain classif.ce\n## 1: 0.02575        8          <list[3]> <list[2]>     0.2148\ninstance$result_learner_param_vals## $xval\n## [1] 0\n## \n## $cp\n## [1] 0.02575\n## \n## $minsplit\n## [1] 8\ninstance$result_y## classif.ce \n##     0.2148\nas.data.table(instance$archive)##          cp minsplit classif.ce x_domain_cp x_domain_minsplit runtime_learners\n##  1: 0.05050       10     0.2500     0.05050                10            0.017\n##  2: 0.05050        1     0.2500     0.05050                 1            0.009\n##  3: 0.05050        3     0.2500     0.05050                 3            0.009\n##  4: 0.02575        8     0.2148     0.02575                 8            0.009\n##  5: 0.07525        5     0.2500     0.07525                 5            0.010\n##  6: 0.07525        8     0.2500     0.07525                 8            0.009\n##  7: 0.10000        5     0.2500     0.10000                 5            0.009\n##  8: 0.00100        1     0.3008     0.00100                 1            0.011\n##  9: 0.00100        8     0.3008     0.00100                 8            0.011\n## 10: 0.07525        1     0.2500     0.07525                 1            0.010\n## 11: 0.10000        3     0.2500     0.10000                 3            0.015\n## 12: 0.05050        5     0.2500     0.05050                 5            0.009\n## 13: 0.00100        5     0.2930     0.00100                 5            0.011\n## 14: 0.02575        5     0.2148     0.02575                 5            0.008\n## 15: 0.02575        3     0.2148     0.02575                 3            0.009\n## 16: 0.07525       10     0.2500     0.07525                10            0.009\n## 17: 0.10000        1     0.2500     0.10000                 1            0.009\n## 18: 0.07525        3     0.2500     0.07525                 3            0.009\n## 19: 0.00100        3     0.2969     0.00100                 3            0.012\n## 20: 0.02575       10     0.2148     0.02575                10            0.009\n##               timestamp batch_nr      resample_result\n##  1: 2021-09-19 14:21:46        1 <ResampleResult[20]>\n##  2: 2021-09-19 14:21:46        2 <ResampleResult[20]>\n##  3: 2021-09-19 14:21:46        3 <ResampleResult[20]>\n##  4: 2021-09-19 14:21:46        4 <ResampleResult[20]>\n##  5: 2021-09-19 14:21:46        5 <ResampleResult[20]>\n##  6: 2021-09-19 14:21:47        6 <ResampleResult[20]>\n##  7: 2021-09-19 14:21:47        7 <ResampleResult[20]>\n##  8: 2021-09-19 14:21:47        8 <ResampleResult[20]>\n##  9: 2021-09-19 14:21:47        9 <ResampleResult[20]>\n## 10: 2021-09-19 14:21:47       10 <ResampleResult[20]>\n## 11: 2021-09-19 14:21:47       11 <ResampleResult[20]>\n## 12: 2021-09-19 14:21:47       12 <ResampleResult[20]>\n## 13: 2021-09-19 14:21:47       13 <ResampleResult[20]>\n## 14: 2021-09-19 14:21:47       14 <ResampleResult[20]>\n## 15: 2021-09-19 14:21:48       15 <ResampleResult[20]>\n## 16: 2021-09-19 14:21:48       16 <ResampleResult[20]>\n## 17: 2021-09-19 14:21:48       17 <ResampleResult[20]>\n## 18: 2021-09-19 14:21:48       18 <ResampleResult[20]>\n## 19: 2021-09-19 14:21:48       19 <ResampleResult[20]>\n## 20: 2021-09-19 14:21:48       20 <ResampleResult[20]>\ninstance$archive$benchmark_result## <BenchmarkResult> of 20 rows with 20 resampling runs\n##  nr task_id    learner_id resampling_id iters warnings errors\n##   1    pima classif.rpart       holdout     1        0      0\n##   2    pima classif.rpart       holdout     1        0      0\n##   3    pima classif.rpart       holdout     1        0      0\n##   4    pima classif.rpart       holdout     1        0      0\n##   5    pima classif.rpart       holdout     1        0      0\n##   6    pima classif.rpart       holdout     1        0      0\n##   7    pima classif.rpart       holdout     1        0      0\n##   8    pima classif.rpart       holdout     1        0      0\n##   9    pima classif.rpart       holdout     1        0      0\n##  10    pima classif.rpart       holdout     1        0      0\n##  11    pima classif.rpart       holdout     1        0      0\n##  12    pima classif.rpart       holdout     1        0      0\n##  13    pima classif.rpart       holdout     1        0      0\n##  14    pima classif.rpart       holdout     1        0      0\n##  15    pima classif.rpart       holdout     1        0      0\n##  16    pima classif.rpart       holdout     1        0      0\n##  17    pima classif.rpart       holdout     1        0      0\n##  18    pima classif.rpart       holdout     1        0      0\n##  19    pima classif.rpart       holdout     1        0      0\n##  20    pima classif.rpart       holdout     1        0      0\ninstance$archive$benchmark_result$score(msr(\"classif.acc\"))##                                    uhash nr              task task_id\n##  1: 063add42-bb3b-4e95-bea0-5949ff3db42c  1 <TaskClassif[47]>    pima\n##  2: f7a9dbb3-adc0-47f7-9f7f-bce6a868901d  2 <TaskClassif[47]>    pima\n##  3: 7e16fb27-e9ee-4242-a5cb-24ab96448c54  3 <TaskClassif[47]>    pima\n##  4: 14dd5c15-2660-403c-bd45-930364d7b8e0  4 <TaskClassif[47]>    pima\n##  5: 0c498a40-916e-4407-a57e-6ac55ca26140  5 <TaskClassif[47]>    pima\n##  6: 76b60a6b-26f0-485e-8193-4bf34e23fb6f  6 <TaskClassif[47]>    pima\n##  7: fd978152-4f4f-4e57-98e7-b262c98332ca  7 <TaskClassif[47]>    pima\n##  8: 547394f0-657f-452d-8801-6bba95e6d7bf  8 <TaskClassif[47]>    pima\n##  9: 45ddb76e-9907-454d-a18b-6cc16be789fa  9 <TaskClassif[47]>    pima\n## 10: d52c48ac-10fa-4f62-b948-4328c68c5790 10 <TaskClassif[47]>    pima\n## 11: 9fca5392-f1cc-4ff1-a337-79d0b543bb6c 11 <TaskClassif[47]>    pima\n## 12: 2b70991a-1b2b-415b-a6bf-3b8f15f671a1 12 <TaskClassif[47]>    pima\n## 13: 1c10594d-82c7-4740-aa7d-26aff0525cc0 13 <TaskClassif[47]>    pima\n## 14: dd97cfc0-1c3e-4a53-afa7-4b7499ba37fc 14 <TaskClassif[47]>    pima\n## 15: 423587ef-fdc5-4a67-80fa-01d2c856a679 15 <TaskClassif[47]>    pima\n## 16: 8abdbab9-914c-4850-8a6d-888d26852a19 16 <TaskClassif[47]>    pima\n## 17: 371d7ab6-69bf-4fe5-ba1f-b49468d4c8d0 17 <TaskClassif[47]>    pima\n## 18: 0ff48422-8552-490b-b5f4-60c65ae3c295 18 <TaskClassif[47]>    pima\n## 19: eb6ead2b-d1c5-4428-bcf3-dbc91a8df1a7 19 <TaskClassif[47]>    pima\n## 20: 1ac1273f-90b5-41ce-8672-4d673169185d 20 <TaskClassif[47]>    pima\n##                       learner    learner_id              resampling\n##  1: <LearnerClassifRpart[36]> classif.rpart <ResamplingHoldout[19]>\n##  2: <LearnerClassifRpart[36]> classif.rpart <ResamplingHoldout[19]>\n##  3: <LearnerClassifRpart[36]> classif.rpart <ResamplingHoldout[19]>\n##  4: <LearnerClassifRpart[36]> classif.rpart <ResamplingHoldout[19]>\n##  5: <LearnerClassifRpart[36]> classif.rpart <ResamplingHoldout[19]>\n##  6: <LearnerClassifRpart[36]> classif.rpart <ResamplingHoldout[19]>\n##  7: <LearnerClassifRpart[36]> classif.rpart <ResamplingHoldout[19]>\n##  8: <LearnerClassifRpart[36]> classif.rpart <ResamplingHoldout[19]>\n##  9: <LearnerClassifRpart[36]> classif.rpart <ResamplingHoldout[19]>\n## 10: <LearnerClassifRpart[36]> classif.rpart <ResamplingHoldout[19]>\n## 11: <LearnerClassifRpart[36]> classif.rpart <ResamplingHoldout[19]>\n## 12: <LearnerClassifRpart[36]> classif.rpart <ResamplingHoldout[19]>\n## 13: <LearnerClassifRpart[36]> classif.rpart <ResamplingHoldout[19]>\n## 14: <LearnerClassifRpart[36]> classif.rpart <ResamplingHoldout[19]>\n## 15: <LearnerClassifRpart[36]> classif.rpart <ResamplingHoldout[19]>\n## 16: <LearnerClassifRpart[36]> classif.rpart <ResamplingHoldout[19]>\n## 17: <LearnerClassifRpart[36]> classif.rpart <ResamplingHoldout[19]>\n## 18: <LearnerClassifRpart[36]> classif.rpart <ResamplingHoldout[19]>\n## 19: <LearnerClassifRpart[36]> classif.rpart <ResamplingHoldout[19]>\n## 20: <LearnerClassifRpart[36]> classif.rpart <ResamplingHoldout[19]>\n##     resampling_id iteration              prediction classif.acc\n##  1:       holdout         1 <PredictionClassif[19]>      0.7500\n##  2:       holdout         1 <PredictionClassif[19]>      0.7500\n##  3:       holdout         1 <PredictionClassif[19]>      0.7500\n##  4:       holdout         1 <PredictionClassif[19]>      0.7852\n##  5:       holdout         1 <PredictionClassif[19]>      0.7500\n##  6:       holdout         1 <PredictionClassif[19]>      0.7500\n##  7:       holdout         1 <PredictionClassif[19]>      0.7500\n##  8:       holdout         1 <PredictionClassif[19]>      0.6992\n##  9:       holdout         1 <PredictionClassif[19]>      0.6992\n## 10:       holdout         1 <PredictionClassif[19]>      0.7500\n## 11:       holdout         1 <PredictionClassif[19]>      0.7500\n## 12:       holdout         1 <PredictionClassif[19]>      0.7500\n## 13:       holdout         1 <PredictionClassif[19]>      0.7070\n## 14:       holdout         1 <PredictionClassif[19]>      0.7852\n## 15:       holdout         1 <PredictionClassif[19]>      0.7852\n## 16:       holdout         1 <PredictionClassif[19]>      0.7500\n## 17:       holdout         1 <PredictionClassif[19]>      0.7500\n## 18:       holdout         1 <PredictionClassif[19]>      0.7500\n## 19:       holdout         1 <PredictionClassif[19]>      0.7031\n## 20:       holdout         1 <PredictionClassif[19]>      0.7852\nlearner$param_set$values = instance$result_learner_param_vals\nlearner$train(task)"},{"path":"optimization.html","id":"autotuner","chapter":"3 Model Optimization","heading":"3.1.4 Automating the Tuning","text":"AutoTuner wraps learner augments automatic tuning given set hyperparameters.\nAutoTuner inherits Learner base class, can used like learner.\nAnalogously previous subsection, new classification tree learner created.\nclassification tree learner automatically tunes parameters cp minsplit using inner resampling (holdout).\ncreate terminator allows 10 evaluations, use simple random search tuning algorithm:can now use learner like learner, calling $train() $predict() method.can also pass resample() benchmark(). called nested resampling discussed next chapter.","code":"\nlearner = lrn(\"classif.rpart\")\nsearch_space = ps(\n  cp = p_dbl(lower = 0.001, upper = 0.1),\n  minsplit = p_int(lower = 1, upper = 10)\n)\nterminator = trm(\"evals\", n_evals = 10)\ntuner = tnr(\"random_search\")\n\nat = AutoTuner$new(\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\"),\n  search_space = search_space,\n  terminator = terminator,\n  tuner = tuner\n)\nat## <AutoTuner:classif.rpart.tuned>\n## * Model: -\n## * Search Space:\n## <ParamSet>\n##          id    class lower upper nlevels        default value\n## 1:       cp ParamDbl 0.001   0.1     Inf <NoDefault[3]>      \n## 2: minsplit ParamInt 1.000  10.0      10 <NoDefault[3]>      \n## * Packages: rpart\n## * Predict Type: response\n## * Feature Types: logical, integer, numeric, factor, ordered\n## * Properties: importance, missings, multiclass, selected_features,\n##   twoclass, weights\nat$train(task)## INFO  [14:21:48.916] [bbotk] Starting to optimize 2 parameter(s) with '<OptimizerRandomSearch>' and '<TerminatorEvals> [n_evals=10, k=0]' \n## INFO  [14:21:48.933] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:49.021] [bbotk] Result of batch 1: \n## INFO  [14:21:49.022] [bbotk]      cp minsplit classif.ce runtime_learners \n## INFO  [14:21:49.022] [bbotk]  0.0591        8     0.3516            0.008 \n## INFO  [14:21:49.022] [bbotk]                                 uhash \n## INFO  [14:21:49.022] [bbotk]  f09c031c-c221-46a0-b2b2-af4beb536abc \n## INFO  [14:21:49.026] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:49.112] [bbotk] Result of batch 2: \n## INFO  [14:21:49.114] [bbotk]       cp minsplit classif.ce runtime_learners \n## INFO  [14:21:49.114] [bbotk]  0.05978        6     0.3516            0.008 \n## INFO  [14:21:49.114] [bbotk]                                 uhash \n## INFO  [14:21:49.114] [bbotk]  2ae1bd8f-7d7d-4519-bc47-701895da0872 \n## INFO  [14:21:49.118] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:49.213] [bbotk] Result of batch 3: \n## INFO  [14:21:49.215] [bbotk]       cp minsplit classif.ce runtime_learners \n## INFO  [14:21:49.215] [bbotk]  0.03989       10     0.3516            0.009 \n## INFO  [14:21:49.215] [bbotk]                                 uhash \n## INFO  [14:21:49.215] [bbotk]  078b74b8-8b69-4a13-bd02-b38bc82d08a2 \n## INFO  [14:21:49.219] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:49.315] [bbotk] Result of batch 4: \n## INFO  [14:21:49.317] [bbotk]       cp minsplit classif.ce runtime_learners \n## INFO  [14:21:49.317] [bbotk]  0.07781        3     0.3516             0.01 \n## INFO  [14:21:49.317] [bbotk]                                 uhash \n## INFO  [14:21:49.317] [bbotk]  565ea92e-e4db-41f1-ad4b-a05925a6031b \n## INFO  [14:21:49.321] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:49.409] [bbotk] Result of batch 5: \n## INFO  [14:21:49.410] [bbotk]        cp minsplit classif.ce runtime_learners \n## INFO  [14:21:49.410] [bbotk]  0.007905        8     0.2812            0.009 \n## INFO  [14:21:49.410] [bbotk]                                 uhash \n## INFO  [14:21:49.410] [bbotk]  71151613-012f-4f77-92c2-53ec265eafc5 \n## INFO  [14:21:49.413] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:49.510] [bbotk] Result of batch 6: \n## INFO  [14:21:49.512] [bbotk]       cp minsplit classif.ce runtime_learners \n## INFO  [14:21:49.512] [bbotk]  0.05173        7     0.3516            0.008 \n## INFO  [14:21:49.512] [bbotk]                                 uhash \n## INFO  [14:21:49.512] [bbotk]  a6cccc55-eccd-4a8e-892d-863120934abc \n## INFO  [14:21:49.516] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:49.600] [bbotk] Result of batch 7: \n## INFO  [14:21:49.602] [bbotk]       cp minsplit classif.ce runtime_learners \n## INFO  [14:21:49.602] [bbotk]  0.09692        4     0.3516            0.009 \n## INFO  [14:21:49.602] [bbotk]                                 uhash \n## INFO  [14:21:49.602] [bbotk]  1dba9a91-a010-4199-a9ec-b66fcc8fbb67 \n## INFO  [14:21:49.605] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:49.704] [bbotk] Result of batch 8: \n## INFO  [14:21:49.706] [bbotk]       cp minsplit classif.ce runtime_learners \n## INFO  [14:21:49.706] [bbotk]  0.09489        6     0.3516            0.009 \n## INFO  [14:21:49.706] [bbotk]                                 uhash \n## INFO  [14:21:49.706] [bbotk]  61219708-190e-48a0-95ee-7a0ed55f1513 \n## INFO  [14:21:49.709] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:49.799] [bbotk] Result of batch 9: \n## INFO  [14:21:49.801] [bbotk]       cp minsplit classif.ce runtime_learners \n## INFO  [14:21:49.801] [bbotk]  0.04475        8     0.3516            0.009 \n## INFO  [14:21:49.801] [bbotk]                                 uhash \n## INFO  [14:21:49.801] [bbotk]  d088af4c-dd07-434c-974c-6c0cf1206fc1 \n## INFO  [14:21:49.806] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:49.909] [bbotk] Result of batch 10: \n## INFO  [14:21:49.911] [bbotk]       cp minsplit classif.ce runtime_learners \n## INFO  [14:21:49.911] [bbotk]  0.04027        6     0.3516             0.01 \n## INFO  [14:21:49.911] [bbotk]                                 uhash \n## INFO  [14:21:49.911] [bbotk]  fe49da15-d129-476d-b994-a031e4d390a2 \n## INFO  [14:21:49.919] [bbotk] Finished optimizing after 10 evaluation(s) \n## INFO  [14:21:49.919] [bbotk] Result: \n## INFO  [14:21:49.921] [bbotk]        cp minsplit learner_param_vals  x_domain classif.ce \n## INFO  [14:21:49.921] [bbotk]  0.007905        8          <list[3]> <list[2]>     0.2812"},{"path":"optimization.html","id":"searchspace","chapter":"3 Model Optimization","heading":"3.2 Tuning Search Spaces","text":"running optimization, important inform tuning algorithm hyperparameters valid.\nnames, types, valid ranges hyperparameter important.\ninformation communicated objects class ParamSet, defined paradox.\npossible create ParamSet-objects using $new-constructor, much shorter readable use ps-shortcut, presented .\n-depth description paradox classes, see paradox chapter.Note, ParamSet objects exist two contexts.\nFirst, ParamSet-objects used define space valid parameter setting learner (objects).\nSecond, used define search space tuning.\nmainly interested latter.\nexample can consider minsplit parameter classif.rpart Learner.\nParamSet associated learner lower upper bound.\nHowever, tuning value, lower upper bound must given tuning search spaces need bounded.\nLearner PipeOp objects, typically “unbounded” ParamSets used.\n, however, mainly focus creating “bounded” ParamSets can used tuning.\nSee -depth paradox chapter details using ParamSets define parameter ranges use-cases besides tuning.","code":""},{"path":"optimization.html","id":"creating-paramsets","chapter":"3 Model Optimization","heading":"3.2.1 Creating ParamSets","text":"empty ParamSet – yet useful – can constructed using just ps call:ps takes named Domain arguments turned parameters. possible search space \"classif.svm\" learner example :five domain constructors produce parameters given ps:domain constructors take following arguments:lower, upper: lower upper bound numerical parameters (p_dbl p_int). need given get bounded parameter spaces valid tuning.levels: Allowed categorical values p_fct parameters. Required argument p_fct. See details parameter.trafo: transformation function, see .depends: dependencies, see .tags: information parameter, used example hyperband tuner.default: Value corresponding default behavior parameter given. used tuning search spaces.special_vals: Valid values besides normally accepted values parameter. used tuning search spaces.custom_check: Function checks whether value given p_uty valid. used tuning search spaces.lower, upper, levels parameters always first (second, upper) position respective constructors, preferred omit defining ParamSet, improved conciseness:","code":"\nlibrary(\"mlr3verse\")\n\nsearch_space = ps()\nprint(search_space)## <ParamSet>\n## Empty.\nsearch_space = ps(\n  cost = p_dbl(lower = 0.1, upper = 10),\n  kernel = p_fct(levels = c(\"polynomial\", \"radial\"))\n)\nprint(search_space)## <ParamSet>\n##        id    class lower upper nlevels        default value\n## 1:   cost ParamDbl   0.1    10     Inf <NoDefault[3]>      \n## 2: kernel ParamFct    NA    NA       2 <NoDefault[3]>\nsearch_space = ps(cost = p_dbl(0.1, 10), kernel = p_fct(c(\"polynomial\", \"radial\")))"},{"path":"optimization.html","id":"searchspace-trafo","chapter":"3 Model Optimization","heading":"3.2.2 Transformations (trafo)","text":"can use paradox function generate_design_grid look values evaluated grid search.\n(using rbindlist() result $transpose() list harder read. didn’t use $transpose(), hand, transformations investigate applied.)notice cost parameter taken linear scale.\nassume, however, difference cost 0.1 1 similar effect difference 1 10.\nTherefore makes sense tune logarithmic scale.\ndone using transformation (trafo).\nfunction applied parameter sampled tuner.\ncan tune cost logarithmic scale sampling linear scale [-1, 1] computing 10^x value.even possible attach another transformation ParamSet whole gets executed individual parameter’s transformations performed.\ngiven .extra_trafo argument function parameters x param_set takes list parameter values x returns modified list.\ntransformation can access parameter values evaluation modify interactions.\neven possible add remove parameters.\n(following bit silly example.)available types search space parameters limited: continuous, integer, discrete, logical scalars.\nmany machine learning algorithms, however, take parameters types, example vectors functions.\ncan defined search space ParamSet, often given ParamUty Learner’s ParamSet.\ntrying tune hyperparameters, necessary perform Transformation changes type parameter.example class.weights parameter SVM, takes named vector class weights one entry target class.\ntrafo tune class.weights tsk(\"spam\") dataset :(omitting rbindlist() example breaks vector valued return elements.)","code":"\nlibrary(\"data.table\")\nrbindlist(generate_design_grid(search_space, 3)$transpose())##     cost     kernel\n## 1:  0.10 polynomial\n## 2:  0.10     radial\n## 3:  5.05 polynomial\n## 4:  5.05     radial\n## 5: 10.00 polynomial\n## 6: 10.00     radial\nsearch_space = ps(\n  cost = p_dbl(-1, 1, trafo = function(x) 10^x),\n  kernel = p_fct(c(\"polynomial\", \"radial\"))\n)\nrbindlist(generate_design_grid(search_space, 3)$transpose())##    cost     kernel\n## 1:  0.1 polynomial\n## 2:  0.1     radial\n## 3:  1.0 polynomial\n## 4:  1.0     radial\n## 5: 10.0 polynomial\n## 6: 10.0     radial\nsearch_space = ps(\n  cost = p_dbl(-1, 1, trafo = function(x) 10^x),\n  kernel = p_fct(c(\"polynomial\", \"radial\")),\n  .extra_trafo = function(x, param_set) {\n    if (x$kernel == \"polynomial\") {\n      x$cost = x$cost * 2\n    }\n    x\n  }\n)\nrbindlist(generate_design_grid(search_space, 3)$transpose())##    cost     kernel\n## 1:  0.2 polynomial\n## 2:  0.1     radial\n## 3:  2.0 polynomial\n## 4:  1.0     radial\n## 5: 20.0 polynomial\n## 6: 10.0     radial\nsearch_space = ps(\n  class.weights = p_dbl(0.1, 0.9, trafo = function(x) c(spam = x, nonspam = 1 - x))\n)\ngenerate_design_grid(search_space, 3)$transpose()## [[1]]\n## [[1]]$class.weights\n##    spam nonspam \n##     0.1     0.9 \n## \n## \n## [[2]]\n## [[2]]$class.weights\n##    spam nonspam \n##     0.5     0.5 \n## \n## \n## [[3]]\n## [[3]]$class.weights\n##    spam nonspam \n##     0.9     0.1"},{"path":"optimization.html","id":"autolevel","chapter":"3 Model Optimization","heading":"3.2.3 Automatic Factor Level Transformation","text":"common use-case necessity specify list values tried (sampled ).\nmay case hyperparameter accepts function objects values certain list functions tried.\nmay choice special numeric values tried.\n, p_fct constructor’s level argument may value character vector, something else.\n, example, values 0.1, 3, 10 tried cost parameter, even random search, following search space achieve :equivalent following:may seem silly, makes sense considering factorial tuning parameters always character values:aware results “unordered” hyperparameter, however.\nTuning algorithms make use ordering information parameters, like genetic algorithms model based optimization, perform worse done.\nalgorithms, may make sense define p_dbl p_int fitting trafo.class.weights case can also implemented like , candidates class.weights vectors tried.\nNote levels argument p_fct must named easy way .character() create names:","code":"\nsearch_space = ps(\n  cost = p_fct(c(0.1, 3, 10)),\n  kernel = p_fct(c(\"polynomial\", \"radial\"))\n)\nrbindlist(generate_design_grid(search_space, 3)$transpose())##    cost     kernel\n## 1:  0.1 polynomial\n## 2:  0.1     radial\n## 3:  3.0 polynomial\n## 4:  3.0     radial\n## 5: 10.0 polynomial\n## 6: 10.0     radial\nsearch_space = ps(\n  cost = p_fct(c(\"0.1\", \"3\", \"10\"),\n    trafo = function(x) list(`0.1` = 0.1, `3` = 3, `10` = 10)[[x]]),\n  kernel = p_fct(c(\"polynomial\", \"radial\"))\n)\nrbindlist(generate_design_grid(search_space, 3)$transpose())##    cost     kernel\n## 1:  0.1 polynomial\n## 2:  0.1     radial\n## 3:  3.0 polynomial\n## 4:  3.0     radial\n## 5: 10.0 polynomial\n## 6: 10.0     radial\nsearch_space = ps(\n  cost = p_fct(c(0.1, 3, 10)),\n  kernel = p_fct(c(\"polynomial\", \"radial\"))\n)\ntypeof(search_space$params$cost$levels)## [1] \"character\"\nsearch_space = ps(\n  class.weights = p_fct(\n    list(\n      candidate_a = c(spam = 0.5, nonspam = 0.5),\n      candidate_b = c(spam = 0.3, nonspam = 0.7)\n    )\n  )\n)\ngenerate_design_grid(search_space)$transpose()## [[1]]\n## [[1]]$class.weights\n##    spam nonspam \n##     0.5     0.5 \n## \n## \n## [[2]]\n## [[2]]$class.weights\n##    spam nonspam \n##     0.3     0.7"},{"path":"optimization.html","id":"searchspace-depends","chapter":"3 Model Optimization","heading":"3.2.4 Parameter Dependencies (depends)","text":"parameters relevant another parameter certain value, one several values.\nSVM, example, degree parameter valid kernel \"polynomial\".\ncan specified using depends argument.\nexpression must involve parameters form <param> == <scalar>, <param> %% <vector>, multiple chained &&.\ntune degree parameter, one need following:","code":"\nsearch_space = ps(\n  cost = p_dbl(-1, 1, trafo = function(x) 10^x),\n  kernel = p_fct(c(\"polynomial\", \"radial\")),\n  degree = p_int(1, 3, depends = kernel == \"polynomial\")\n)\nrbindlist(generate_design_grid(search_space, 3)$transpose(), fill = TRUE)##     cost     kernel degree\n##  1:  0.1 polynomial      1\n##  2:  0.1 polynomial      2\n##  3:  0.1 polynomial      3\n##  4:  0.1     radial     NA\n##  5:  1.0 polynomial      1\n##  6:  1.0 polynomial      2\n##  7:  1.0 polynomial      3\n##  8:  1.0     radial     NA\n##  9: 10.0 polynomial      1\n## 10: 10.0 polynomial      2\n## 11: 10.0 polynomial      3\n## 12: 10.0     radial     NA"},{"path":"optimization.html","id":"creating-tuning-paramsets-from-other-paramsets","chapter":"3 Model Optimization","heading":"3.2.5 Creating Tuning ParamSets from other ParamSets","text":"define tuning ParamSet Learner already parameter set information may seem unnecessarily tedious, indeed way create tuning ParamSets Learner’s ParamSet, making use much information already available.done setting values Learner’s ParamSet -called TuneTokens, constructed to_tune call.\ncan done way hyperparameters set specific values.\ncan understood hyperparameters tagged later tuning.\nresulting ParamSet used tuning can retrieved using $search_space() method.possible omit lower , can inferred lower bound degree parameter .\nparameters, already bounded, possible give bounds , ranges already bounded.\nexample logical shrinking hyperparameter:to_tune can also constructed Domain object, .e. something constructed p_*** call.\nway possible tune continuous parameters discrete values, give trafos dependencies.\nOne , example, tune cost three given special values, introduce dependency shrinking .\nNotice short form to_tune(<levels>) short form to_tune(p_fct(<levels>)).\n(introducing dependency, need use degree value implicit trafo, name .character() respective value, \"val2\"!)search_space() picks dependencies fromt underlying ParamSet automatically.\nkernel tuned, degree automatically gets dependency , without us specify .\n(reset cost shrinking NULL sake clarity generated output.)even possible define whole ParamSets get tuned single parameter.\nmay especially useful vector hyperparameters searched along multiple dimensions.\nParamSet must, however, .extra_trafo returns list single element, corresponds single hyperparameter tuned.\nSuppose class.weights hyperparameter tuned along two dimensions:","code":"\nlearner = lrn(\"classif.svm\")\nlearner$param_set$values$kernel = \"polynomial\"  # for example\nlearner$param_set$values$degree = to_tune(lower = 1, upper = 3)\n\nprint(learner$param_set$search_space())## <ParamSet>\n##        id    class lower upper nlevels        default value\n## 1: degree ParamInt     1     3       3 <NoDefault[3]>\nrbindlist(generate_design_grid(learner$param_set$search_space(), 3)$transpose())##    degree\n## 1:      1\n## 2:      2\n## 3:      3\nlearner$param_set$values$shrinking = to_tune()\n\nprint(learner$param_set$search_space())## <ParamSet>\n##           id    class lower upper nlevels        default value\n## 1:    degree ParamInt     1     3       3 <NoDefault[3]>      \n## 2: shrinking ParamLgl    NA    NA       2           TRUE\nrbindlist(generate_design_grid(learner$param_set$search_space(), 3)$transpose())##    degree shrinking\n## 1:      1      TRUE\n## 2:      1     FALSE\n## 3:      2      TRUE\n## 4:      2     FALSE\n## 5:      3      TRUE\n## 6:      3     FALSE\nlearner$param_set$values$type = \"C-classification\"  # needs to be set because of a bug in paradox\nlearner$param_set$values$cost = to_tune(c(val1 = 0.3, val2 = 0.7))\nlearner$param_set$values$shrinking = to_tune(p_lgl(depends = cost == \"val2\"))\n\nprint(learner$param_set$search_space())## <ParamSet>\n##           id    class lower upper nlevels        default parents value\n## 1:      cost ParamFct    NA    NA       2 <NoDefault[3]>              \n## 2:    degree ParamInt     1     3       3 <NoDefault[3]>              \n## 3: shrinking ParamLgl    NA    NA       2 <NoDefault[3]>    cost      \n## Trafo is set.\nrbindlist(generate_design_grid(learner$param_set$search_space(), 3)$transpose(), fill = TRUE)##    degree cost shrinking\n## 1:      1  0.3        NA\n## 2:      1  0.7      TRUE\n## 3:      1  0.7     FALSE\n## 4:      2  0.3        NA\n## 5:      2  0.7      TRUE\n## 6:      2  0.7     FALSE\n## 7:      3  0.3        NA\n## 8:      3  0.7      TRUE\n## 9:      3  0.7     FALSE\nlearner$param_set$values$cost = NULL\nlearner$param_set$values$shrinking = NULL\nlearner$param_set$values$kernel = to_tune(c(\"polynomial\", \"radial\"))\n\nprint(learner$param_set$search_space())## <ParamSet>\n##        id    class lower upper nlevels        default parents value\n## 1: degree ParamInt     1     3       3 <NoDefault[3]>  kernel      \n## 2: kernel ParamFct    NA    NA       2 <NoDefault[3]>\nrbindlist(generate_design_grid(learner$param_set$search_space(), 3)$transpose(), fill = TRUE)##        kernel degree\n## 1: polynomial      1\n## 2: polynomial      2\n## 3: polynomial      3\n## 4:     radial     NA\nlearner$param_set$values$class.weights = to_tune(\n  ps(spam = p_dbl(0.1, 0.9), nonspam = p_dbl(0.1, 0.9),\n    .extra_trafo = function(x, param_set) list(c(spam = x$spam, nonspam = x$nonspam))\n  ))\nhead(generate_design_grid(learner$param_set$search_space(), 3)$transpose(), 3)## [[1]]\n## [[1]]$kernel\n## [1] \"polynomial\"\n## \n## [[1]]$degree\n## [1] 1\n## \n## [[1]]$class.weights\n##    spam nonspam \n##     0.1     0.1 \n## \n## \n## [[2]]\n## [[2]]$kernel\n## [1] \"polynomial\"\n## \n## [[2]]$degree\n## [1] 1\n## \n## [[2]]$class.weights\n##    spam nonspam \n##     0.1     0.5 \n## \n## \n## [[3]]\n## [[3]]$kernel\n## [1] \"polynomial\"\n## \n## [[3]]$degree\n## [1] 1\n## \n## [[3]]$class.weights\n##    spam nonspam \n##     0.1     0.9"},{"path":"optimization.html","id":"nested-resampling","chapter":"3 Model Optimization","heading":"3.3 Nested Resampling","text":"Evaluating machine learning model often requires additional layer resampling hyperparameters features selected.\nNested resampling separates model selection steps process estimating performance model.\ndata used model selection steps evaluation model , resulting performance estimate model might severely biased.\nOne reason repeated evaluation model test data leak information structure model, results -optimistic performance estimates.\nKeep mind nested resampling statistical procedure estimate predictive performance model trained full dataset.\nNested resampling procedure select optimal hyperparameters.\nresampling produces many hyperparameter configurations used construct final model (Simon 2007).\ngraphic illustrates nested resampling hyperparameter tuning 3-fold cross-validation outer 4-fold cross-validation inner loop.outer resampling loop, three pairs training/test sets.\nouter training sets parameter tuning done, thereby executing inner resampling loop.\nway, get one set selected hyperparameters outer training set.\nlearner fitted outer training set using corresponding selected hyperparameters.\nSubsequently, can evaluate performance learner outer test sets.\naggregated performance outer test sets unbiased performance estimate model.","code":""},{"path":"optimization.html","id":"nested-resamp-exec","chapter":"3 Model Optimization","heading":"3.3.1 Execution","text":"previous section examined optimization simple classification tree mlr_tasks_pima.\ncontinue example estimate predictive performance model nested resampling.use 4-fold cross-validation inner resampling loop.\nAutoTuner executes hyperparameter tuning stopped 5 evaluations.\nhyperparameter configurations proposed grid search.3-fold cross-validation used outer resampling loop.\nthree outer train sets hyperparameter tuning done receive three optimized hyperparameter configurations.\nexecute nested resampling, pass AutoTuner resample() function.\nset store_models = TRUE need AutoTuner models investigate inner tuning.can freely combine different inner outer resampling strategies.\nNested resampling restricted hyperparameter tuning.\ncan swap AutoTuner AutoFSelector estimate performance model fitted optimized feature subset.","code":"\nlibrary(\"mlr3verse\")\n\nlearner = lrn(\"classif.rpart\")\nresampling = rsmp(\"holdout\")\nmeasure = msr(\"classif.ce\")\nsearch_space = ps(cp = p_dbl(lower = 0.001, upper = 0.1))\nterminator = trm(\"evals\", n_evals = 5)\ntuner = tnr(\"grid_search\", resolution = 10)\n\nat = AutoTuner$new(learner, resampling, measure, terminator, tuner, search_space)\ntask = tsk(\"pima\")\nouter_resampling = rsmp(\"cv\", folds = 3)\n\nrr = resample(task, at, outer_resampling, store_models = TRUE)## INFO  [14:21:58.716] [bbotk] Starting to optimize 1 parameter(s) with '<OptimizerGridSearch>' and '<TerminatorEvals> [n_evals=5, k=0]' \n## INFO  [14:21:58.750] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:58.869] [bbotk] Result of batch 1: \n## INFO  [14:21:58.872] [bbotk]     cp classif.ce runtime_learners                                uhash \n## INFO  [14:21:58.872] [bbotk]  0.078     0.2222            0.016 13a32d6a-557c-42f7-91b0-e374e1a35a9e \n## INFO  [14:21:58.874] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:58.965] [bbotk] Result of batch 2: \n## INFO  [14:21:58.967] [bbotk]     cp classif.ce runtime_learners                                uhash \n## INFO  [14:21:58.967] [bbotk]  0.045     0.2222            0.009 df035515-620e-446b-af89-ffe383d6bea7 \n## INFO  [14:21:58.969] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:59.067] [bbotk] Result of batch 3: \n## INFO  [14:21:59.069] [bbotk]     cp classif.ce runtime_learners                                uhash \n## INFO  [14:21:59.069] [bbotk]  0.067     0.2222            0.008 537451b7-d14d-4df6-882f-f14dfcff50d7 \n## INFO  [14:21:59.070] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:59.167] [bbotk] Result of batch 4: \n## INFO  [14:21:59.168] [bbotk]   cp classif.ce runtime_learners                                uhash \n## INFO  [14:21:59.168] [bbotk]  0.1     0.2222            0.009 861bc132-6bab-4059-848c-f10d88958511 \n## INFO  [14:21:59.170] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:59.264] [bbotk] Result of batch 5: \n## INFO  [14:21:59.266] [bbotk]     cp classif.ce runtime_learners                                uhash \n## INFO  [14:21:59.266] [bbotk]  0.034     0.2222            0.009 1ed587be-73cd-423c-8f27-1e32c0cd23b4 \n## INFO  [14:21:59.271] [bbotk] Finished optimizing after 5 evaluation(s) \n## INFO  [14:21:59.272] [bbotk] Result: \n## INFO  [14:21:59.273] [bbotk]     cp learner_param_vals  x_domain classif.ce \n## INFO  [14:21:59.273] [bbotk]  0.078          <list[2]> <list[1]>     0.2222 \n## INFO  [14:21:59.327] [bbotk] Starting to optimize 1 parameter(s) with '<OptimizerGridSearch>' and '<TerminatorEvals> [n_evals=5, k=0]' \n## INFO  [14:21:59.330] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:59.419] [bbotk] Result of batch 1: \n## INFO  [14:21:59.421] [bbotk]   cp classif.ce runtime_learners                                uhash \n## INFO  [14:21:59.421] [bbotk]  0.1     0.2164            0.008 0ba63400-f9da-4d7d-bb1a-888abc563da6 \n## INFO  [14:21:59.423] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:59.512] [bbotk] Result of batch 2: \n## INFO  [14:21:59.514] [bbotk]     cp classif.ce runtime_learners                                uhash \n## INFO  [14:21:59.514] [bbotk]  0.001     0.2281            0.008 a8301cbd-5425-4165-ba51-979d8c000b57 \n## INFO  [14:21:59.516] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:59.682] [bbotk] Result of batch 3: \n## INFO  [14:21:59.684] [bbotk]     cp classif.ce runtime_learners                                uhash \n## INFO  [14:21:59.684] [bbotk]  0.034     0.2164            0.008 ebf8a9e0-ef92-4c64-aad4-df53382fe50c \n## INFO  [14:21:59.685] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:59.768] [bbotk] Result of batch 4: \n## INFO  [14:21:59.770] [bbotk]     cp classif.ce runtime_learners                                uhash \n## INFO  [14:21:59.770] [bbotk]  0.067     0.2164            0.007 33fe6d5f-dc72-4b64-a111-2064e650d9fd \n## INFO  [14:21:59.771] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:21:59.859] [bbotk] Result of batch 5: \n## INFO  [14:21:59.860] [bbotk]     cp classif.ce runtime_learners                                uhash \n## INFO  [14:21:59.860] [bbotk]  0.012     0.2281            0.008 ba0a1087-2e5f-4101-bfb5-6b963386d7e9 \n## INFO  [14:21:59.865] [bbotk] Finished optimizing after 5 evaluation(s) \n## INFO  [14:21:59.865] [bbotk] Result: \n## INFO  [14:21:59.866] [bbotk]   cp learner_param_vals  x_domain classif.ce \n## INFO  [14:21:59.866] [bbotk]  0.1          <list[2]> <list[1]>     0.2164 \n## INFO  [14:21:59.918] [bbotk] Starting to optimize 1 parameter(s) with '<OptimizerGridSearch>' and '<TerminatorEvals> [n_evals=5, k=0]' \n## INFO  [14:21:59.921] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:22:00.002] [bbotk] Result of batch 1: \n## INFO  [14:22:00.003] [bbotk]     cp classif.ce runtime_learners                                uhash \n## INFO  [14:22:00.003] [bbotk]  0.089     0.2398            0.009 c0465599-de27-4deb-95da-696fc130bb55 \n## INFO  [14:22:00.004] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:22:00.097] [bbotk] Result of batch 2: \n## INFO  [14:22:00.099] [bbotk]     cp classif.ce runtime_learners                                uhash \n## INFO  [14:22:00.099] [bbotk]  0.067     0.2398            0.009 c658cf05-3ef1-4777-8a81-8861e6a81741 \n## INFO  [14:22:00.100] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:22:00.184] [bbotk] Result of batch 3: \n## INFO  [14:22:00.185] [bbotk]   cp classif.ce runtime_learners                                uhash \n## INFO  [14:22:00.185] [bbotk]  0.1     0.2398            0.009 075a5757-79fe-49e0-bbbb-e81e828d3339 \n## INFO  [14:22:00.187] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:22:00.276] [bbotk] Result of batch 4: \n## INFO  [14:22:00.278] [bbotk]     cp classif.ce runtime_learners                                uhash \n## INFO  [14:22:00.278] [bbotk]  0.045     0.2398            0.008 927a988a-a41d-4b26-8e4e-3e97226722b1 \n## INFO  [14:22:00.279] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:22:00.365] [bbotk] Result of batch 5: \n## INFO  [14:22:00.367] [bbotk]     cp classif.ce runtime_learners                                uhash \n## INFO  [14:22:00.367] [bbotk]  0.001     0.2573            0.009 5c1e2a7a-c7b2-4bde-bac6-22f766ed71de \n## INFO  [14:22:00.371] [bbotk] Finished optimizing after 5 evaluation(s) \n## INFO  [14:22:00.372] [bbotk] Result: \n## INFO  [14:22:00.373] [bbotk]     cp learner_param_vals  x_domain classif.ce \n## INFO  [14:22:00.373] [bbotk]  0.089          <list[2]> <list[1]>     0.2398"},{"path":"optimization.html","id":"nested-resamp-eval","chapter":"3 Model Optimization","heading":"3.3.2 Evaluation","text":"created ResampleResult can now inspect executed resampling iterations closely.\nSee section Resampling detailed information ResampleResult objects.check inner tuning results stable hyperparameters.\nmeans selected hyperparameters vary much.\nmight observe unstable models example small data set low number resampling iterations might introduces much randomness.\nUsually, aim selection stable hyperparameters outer training sets.Next, want compare predictive performances estimated outer resampling inner resampling.\nSignificantly lower predictive performances outer resampling indicate models optimized hyperparameters overfit data.aggregated performance outer resampling iterations essentially unbiased performance model optimal hyperparameter found grid search.Note nested resampling computationally expensive.\nreason use relatively small number hyperparameter configurations low number resampling iterations example.\npractice, normally increase .\ncomputationally intensive might want look section Parallelization.","code":"\nextract_inner_tuning_results(rr)##    iteration    cp classif.ce learner_param_vals  x_domain task_id\n## 1:         1 0.078     0.2222          <list[2]> <list[1]>    pima\n## 2:         2 0.089     0.2398          <list[2]> <list[1]>    pima\n## 3:         3 0.100     0.2164          <list[2]> <list[1]>    pima\n##             learner_id resampling_id\n## 1: classif.rpart.tuned            cv\n## 2: classif.rpart.tuned            cv\n## 3: classif.rpart.tuned            cv\nrr$score()##                 task task_id         learner          learner_id\n## 1: <TaskClassif[47]>    pima <AutoTuner[40]> classif.rpart.tuned\n## 2: <TaskClassif[47]>    pima <AutoTuner[40]> classif.rpart.tuned\n## 3: <TaskClassif[47]>    pima <AutoTuner[40]> classif.rpart.tuned\n##            resampling resampling_id iteration              prediction\n## 1: <ResamplingCV[19]>            cv         1 <PredictionClassif[19]>\n## 2: <ResamplingCV[19]>            cv         2 <PredictionClassif[19]>\n## 3: <ResamplingCV[19]>            cv         3 <PredictionClassif[19]>\n##    classif.ce\n## 1:     0.2422\n## 2:     0.2617\n## 3:     0.2969\nrr$aggregate()## classif.ce \n##     0.2669"},{"path":"optimization.html","id":"nested-final-model","chapter":"3 Model Optimization","heading":"3.3.3 Final Model","text":"can use AutoTuner tune hyperparameters learner fit final model full data set.trained model can now used make predictions new data.\ncommon mistake report performance estimated resampling sets tuning performed ($tuning_result$classif.ce) model’s performance.\nInstead, report performance estimated nested resampling performance model.","code":"\nat$train(task)## INFO  [14:22:00.713] [bbotk] Starting to optimize 1 parameter(s) with '<OptimizerGridSearch>' and '<TerminatorEvals> [n_evals=5, k=0]' \n## INFO  [14:22:00.716] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:22:00.803] [bbotk] Result of batch 1: \n## INFO  [14:22:00.805] [bbotk]     cp classif.ce runtime_learners                                uhash \n## INFO  [14:22:00.805] [bbotk]  0.056     0.2812            0.008 a123ea38-5ad1-4a96-b065-5d5e754a7ec8 \n## INFO  [14:22:00.806] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:22:00.889] [bbotk] Result of batch 2: \n## INFO  [14:22:00.891] [bbotk]     cp classif.ce runtime_learners                                uhash \n## INFO  [14:22:00.891] [bbotk]  0.089     0.2812            0.009 31a8f982-2ff9-4c4f-b889-9fe2d7498b62 \n## INFO  [14:22:00.892] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:22:00.986] [bbotk] Result of batch 3: \n## INFO  [14:22:00.988] [bbotk]     cp classif.ce runtime_learners                                uhash \n## INFO  [14:22:00.988] [bbotk]  0.034     0.2852            0.016 2009121b-27a7-4e5d-b7d2-3173dc7b2998 \n## INFO  [14:22:00.989] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:22:01.078] [bbotk] Result of batch 4: \n## INFO  [14:22:01.080] [bbotk]     cp classif.ce runtime_learners                                uhash \n## INFO  [14:22:01.080] [bbotk]  0.023     0.3008            0.009 6e896ee0-8d32-429f-8d9c-17d038aa935e \n## INFO  [14:22:01.081] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:22:01.172] [bbotk] Result of batch 5: \n## INFO  [14:22:01.173] [bbotk]     cp classif.ce runtime_learners                                uhash \n## INFO  [14:22:01.173] [bbotk]  0.078     0.2812            0.015 f8b82e08-86ce-4621-ba71-f0d5af22911b \n## INFO  [14:22:01.178] [bbotk] Finished optimizing after 5 evaluation(s) \n## INFO  [14:22:01.178] [bbotk] Result: \n## INFO  [14:22:01.180] [bbotk]     cp learner_param_vals  x_domain classif.ce \n## INFO  [14:22:01.180] [bbotk]  0.056          <list[2]> <list[1]>     0.2812"},{"path":"optimization.html","id":"hyperband","chapter":"3 Model Optimization","heading":"3.4 Tuning with Hyperband","text":"Besides traditional tuning methods, ecosystem around mlr3 offers another procedure hyperparameter optimization called Hyperband implemented mlr3hyperband package.Hyperband budget-oriented procedure, weeding suboptimal performing configurations early partially sequential training process, increasing tuning efficiency consequence.\n, combination incremental resource allocation early stopping used: optimization progresses, computational resources increased promising configurations, less promising ones terminated early.give introductory analogy, imagine two horse trainers given eight untrained horses.\ntrainers want win upcoming race, given 32 units food.\nGiven horse can fed 8 units food (“maximum budget” per horse), enough food horses.\ncritical identify promising horses early, give enough food improve.\n, trainers need develop strategy split food best possible way.\nfirst trainer optimistic wants explore full capabilities horse, want pass judgment horse’s performance unless fully trained.\n, divides budget maximum amount can give horse (lets say eight, \\(32 / 8 = 4\\)) randomly picks four horses - budget simply enough fully train .\nfour horses trained full capabilities, rest set free.\nway, trainer confident choosing best four trained horses, might overlooked horse highest potential since focused half .\ntrainer creative develops different strategy.\nthinks, horse performing well beginning, also improve training.\nBased assumption, decides give one unit food horse observes develop.\ninitial food consumed, checks performance kicks slowest half training regime.\n, increases available food remaining, trains food consumed , kick worst half .\nrepeats one remaining horse gets rest food.\nmeans one horse fully trained, flip side, able start training eight horses.race day, horses put starting line.\ntrainer winning horse?\none, tried train maximum amount horses fullest?\none, made assumptions training progress horses?\ntraining phases may possibly look like visualized figure 3.1.\nFigure 3.1: Visulization training processes may look like. left plot corresponds non-selective trainer, right one selective trainer.\nHyperband works similar ways, also different others.\nembodied one trainers analogy, person, pay .\nHyperband consists several brackets, bracket corresponding trainer, care horses hyperparameter configurations machine learning algorithm.\nbudget terms food, terms hyperparameter learner scales way computational effort.\nexample number epochs train neural network, number iterations boosting.\nFurthermore, two brackets (trainers), several, placed unique spot fully explorative later training stages extremely selective, equal higher exploration early training stages.\nlevel selection aggressiveness handled user-defined parameter called \\(\\eta\\).\n, \\(1/\\eta\\) fraction remaining configurations bracket removes worst performing ones, \\(\\eta\\) also factor budget increased next stage.\ndifferent maximum budget per configuration makes sense different scenarios, user also set \\(R\\) parameter.\nparameters required Hyperband – full required budget across brackets indirectly given \n\\[ (\\lfloor \\log_{\\eta}{R} \\rfloor + 1)^2 * R\\] (Li et al. 2016).\ngive idea full bracket layout might look like specific \\(R\\) \\(\\eta\\), quick overview given following table.\nTable 3.1: Hyperband layout \\(\\eta = 2\\) \\(R = 8\\), consisting four brackets \\(n\\) amount active configurations.\ncourse, early termination based performance criterion may disadvantageous done aggressively certain scenarios.\nlearner jumping radically estimated performance training phase may get best configurations canceled early, simply improve quickly enough compared others.\nwords, often unclear beforehand high amount configurations \\(n\\), gets aggressively discarded early, better high budget \\(B\\) per configuration.\narising tradeoff, made, called “\\(n\\) versus \\(B/n\\) problem”.\ncreate balance selection based early training performance versus exploration training performances later training stages, \\(\\lfloor \\log_{\\eta}{R} \\rfloor + 1\\) brackets constructed associated set varying sized configurations.\nThus, brackets contain configurations, small initial budget.\n, lot discarded trained short amount time, corresponding selective trainer horse analogy.\nOthers constructed fewer configurations, discarding takes place significant amount budget consumed.\nlast bracket usually never discards anything, also starts configurations – equivalent trainer explorative later stages.\nformer corresponds high \\(n\\), latter high \\(B/n\\).\nEven though different brackets initialized different amount configurations different initial budget sizes, bracket assigned (approximately) budget \\((\\lfloor \\log_{\\eta}{R} \\rfloor + 1) * R\\).configurations start bracket initialized random, often uniform sampling.\nNote currently configurations trained completely beginning, online updates models stage stage happening.identify budget evaluating Hyperband, user specify explicitly hyperparameter learner influences budget extending single hyperparameter ParamSet argument (tags = \"budget\"), like following snippet:Thanks broad ecosystem mlr3verse learner require natural budget parameter.\ntypical case decision trees.\nusing subsampling preprocessing mlr3pipelines, can work around lacking budget parameter.can now plug new learner extended hyperparameter set TuningInstanceSingleCrit way usual.\nNaturally, Hyperband terminates brackets evaluated, Terminator tuning instance acts upper bound set low value one unsure long Hyperband take finish given settings.Now, initialize new instance mlr3hyperband::mlr_tuners_hyperband class start tuning .receive results sampled configuration, simply run following snippet.can access best found configuration instance object.familiar original paper, may wondered just used Hyperband parameter ranging 0.1 1.0 (Li et al. 2016).\nanswer , help internal rescaling budget parameter.\nmlr3hyperband automatically divides budget parameters boundaries lower bound, ending budget range starting 1, like case originally.\nwant overview bracket layout Hyperband created rescaling bracket worked, can print compact table see information.traditional way, Hyperband uses uniform sampling receive configuration sample start bracket.\nalso possible define custom Sampler hyperparameter., defined sampler given argument instance creation.\nAfterwards, usual tuning can proceed.Furthermore, extended original algorithm, make also possible use mlr3hyperband multi-objective optimization.\n, simply specify measures TuningInstanceMultiCrit run rest usual.Now result single best configuration estimated Pareto front.\nred points dominated another parameter configuration regarding fpr tpr performance measures.","code":"\nlibrary(\"mlr3verse\")\n\n# Hyperparameter subset of XGBoost\nsearch_space = ps(\n  nrounds = p_int(lower = 1, upper = 16, tags = \"budget\"),\n  booster = p_fct(levels = c(\"gbtree\", \"gblinear\", \"dart\"))\n)\nset.seed(123)\n\n# extend \"classif.rpart\" with \"subsampling\" as preprocessing step\nll = po(\"subsample\") %>>% lrn(\"classif.rpart\")\n\n# extend hyperparameters of \"classif.rpart\" with subsampling fraction as budget\nsearch_space = ps(\n  classif.rpart.cp = p_dbl(lower = 0.001, upper = 0.1),\n  classif.rpart.minsplit = p_int(lower = 1, upper = 10),\n  subsample.frac = p_dbl(lower = 0.1, upper = 1, tags = \"budget\")\n)\ninstance = TuningInstanceSingleCrit$new(\n  task = tsk(\"iris\"),\n  learner = ll,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"none\"), # hyperband terminates itself\n  search_space = search_space\n)\nlibrary(\"mlr3hyperband\")## Loading required package: mlr3tuning## Loading required package: paradox\ntuner = tnr(\"hyperband\", eta = 3)\n\n# reduce logging output\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\ntuner$optimize(instance)##    classif.rpart.cp classif.rpart.minsplit subsample.frac learner_param_vals\n## 1:          0.07348                      5         0.1111          <list[6]>\n##     x_domain classif.ce\n## 1: <list[3]>       0.02\nas.data.table(instance$archive)[, c(\n  \"subsample.frac\",\n  \"classif.rpart.cp\",\n  \"classif.rpart.minsplit\",\n  \"classif.ce\"\n), with = FALSE]##     subsample.frac classif.rpart.cp classif.rpart.minsplit classif.ce\n##  1:         0.1111          0.02533                      3       0.04\n##  2:         0.1111          0.07348                      5       0.02\n##  3:         0.1111          0.08490                      3       0.02\n##  4:         0.1111          0.05026                      6       0.02\n##  5:         0.1111          0.03940                      4       0.02\n##  6:         0.1111          0.02540                      7       0.42\n##  7:         0.1111          0.01200                      4       0.14\n##  8:         0.1111          0.03961                      4       0.02\n##  9:         0.1111          0.05762                      6       0.02\n## 10:         0.3333          0.07348                      5       0.06\n## 11:         0.3333          0.08490                      3       0.04\n## 12:         0.3333          0.05026                      6       0.06\n## 13:         1.0000          0.08490                      3       0.04\n## 14:         0.3333          0.08650                      6       0.02\n## 15:         0.3333          0.07491                      9       0.06\n## 16:         0.3333          0.06716                      6       0.04\n## 17:         0.3333          0.06218                      9       0.08\n## 18:         0.3333          0.03785                      4       0.06\n## 19:         1.0000          0.08650                      6       0.04\n## 20:         1.0000          0.02724                     10       0.04\n## 21:         1.0000          0.05689                      3       0.04\n## 22:         1.0000          0.09141                      4       0.04\n##     subsample.frac classif.rpart.cp classif.rpart.minsplit classif.ce\ninstance$result##    classif.rpart.cp classif.rpart.minsplit subsample.frac learner_param_vals\n## 1:          0.07348                      5         0.1111          <list[6]>\n##     x_domain classif.ce\n## 1: <list[3]>       0.02\ninstance$result_learner_param_vals## $subsample.frac\n## [1] 0.1111\n## \n## $subsample.stratify\n## [1] FALSE\n## \n## $subsample.replace\n## [1] FALSE\n## \n## $classif.rpart.xval\n## [1] 0\n## \n## $classif.rpart.cp\n## [1] 0.07348\n## \n## $classif.rpart.minsplit\n## [1] 5\ninstance$result_y## classif.ce \n##       0.02\nunique(as.data.table(instance$archive)[, .(bracket, bracket_stage, budget_scaled, budget_real, n_configs)])##    bracket bracket_stage budget_scaled budget_real n_configs\n## 1:       2             0         1.111      0.1111         9\n## 2:       2             1         3.333      0.3333         3\n## 3:       2             2        10.000      1.0000         1\n## 4:       1             0         3.333      0.3333         5\n## 5:       1             1        10.000      1.0000         1\n## 6:       0             0        10.000      1.0000         3\nsearch_space = ps(\n  nrounds = p_int(lower = 1, upper = 16, tags = \"budget\"),\n  eta = p_dbl(lower = 0, upper = 1),\n  booster = p_fct(levels = c(\"gbtree\", \"gblinear\", \"dart\"))\n)\n\ninstance = TuningInstanceSingleCrit$new(\n  task = tsk(\"iris\"),\n  learner = lrn(\"classif.xgboost\"),\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"none\"), # hyperband terminates itself\n  search_space = search_space\n)\n\n# beta distribution with alpha = 2 and beta = 5\n# categorical distribution with custom probabilities\nsampler = SamplerJointIndep$new(list(\n  Sampler1DRfun$new(search_space$params$eta, function(n) rbeta(n, 2, 5)),\n  Sampler1DCateg$new(search_space$params$booster, prob = c(0.2, 0.3, 0.5))\n))\ntuner = tnr(\"hyperband\", eta = 2, sampler = sampler)\nset.seed(123)\ntuner$optimize(instance)## [14:22:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:09] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:11] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:11] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:11] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:11] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:11] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:11] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:11] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:11] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:12] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:12] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:12] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:12] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:13] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:13] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:13] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:13] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:13] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:14] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:14] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:14] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:14] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:14] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:14] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:14] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:14] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:15] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:15] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:15] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:15] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:15] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:15] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:15] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:16] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:16] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:16] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:16] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:16] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:16] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:16] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:16] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:17] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:17] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:17] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:17] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:18] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:18] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:18] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:18] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:18] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:18] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:18] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:19] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:19] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:19] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:19] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:19] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.##    nrounds    eta booster learner_param_vals  x_domain classif.ce\n## 1:       1 0.2415    dart          <list[5]> <list[3]>       0.04\ninstance$result##    nrounds    eta booster learner_param_vals  x_domain classif.ce\n## 1:       1 0.2415    dart          <list[5]> <list[3]>       0.04\ninstance = TuningInstanceMultiCrit$new(\n  task = tsk(\"pima\"),\n  learner = lrn(\"classif.xgboost\"),\n  resampling = rsmp(\"holdout\"),\n  measures = msrs(c(\"classif.tpr\", \"classif.fpr\")),\n  terminator = trm(\"none\"), # hyperband terminates itself\n  search_space = search_space\n)\n\ntuner = tnr(\"hyperband\", eta = 4)\ntuner$optimize(instance)## [14:22:20] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:20] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:20] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:20] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:21] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:21] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:21] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:21] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:21] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:21] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:21] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:21] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:21] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:21] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:21] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:21] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:23] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:23] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:23] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:23] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:23] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:24] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:24] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:24] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:24] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:24] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:24] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:24] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:25] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:25] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n## [14:22:25] WARNING: amalgamation/../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.##     nrounds     eta  booster learner_param_vals  x_domain classif.tpr\n##  1:       1 0.34093 gblinear          <list[5]> <list[3]>      0.0000\n##  2:       1 0.50700 gblinear          <list[5]> <list[3]>      0.0000\n##  3:       1 0.77369 gblinear          <list[5]> <list[3]>      0.0000\n##  4:       1 0.55967 gblinear          <list[5]> <list[3]>      0.0000\n##  5:       1 0.46843 gblinear          <list[5]> <list[3]>      0.0000\n##  6:       1 0.27922 gblinear          <list[5]> <list[3]>      0.0000\n##  7:       1 0.79274 gblinear          <list[5]> <list[3]>      0.0000\n##  8:       1 0.58557 gblinear          <list[5]> <list[3]>      0.0000\n##  9:       1 0.39476 gblinear          <list[5]> <list[3]>      0.0000\n## 10:       1 0.58145 gblinear          <list[5]> <list[3]>      0.0000\n## 11:      16 0.01919   gbtree          <list[5]> <list[3]>      0.6374\n## 12:       4 0.55531     dart          <list[5]> <list[3]>      0.5824\n## 13:       4 0.65920   gbtree          <list[5]> <list[3]>      0.6044\n## 14:      16 0.13587     dart          <list[5]> <list[3]>      0.6264\n##     classif.fpr\n##  1:      0.0000\n##  2:      0.0000\n##  3:      0.0000\n##  4:      0.0000\n##  5:      0.0000\n##  6:      0.0000\n##  7:      0.0000\n##  8:      0.0000\n##  9:      0.0000\n## 10:      0.0000\n## 11:      0.2364\n## 12:      0.1576\n## 13:      0.1758\n## 14:      0.1818\ninstance$result##     nrounds     eta  booster learner_param_vals  x_domain classif.tpr\n##  1:       1 0.34093 gblinear          <list[5]> <list[3]>      0.0000\n##  2:       1 0.50700 gblinear          <list[5]> <list[3]>      0.0000\n##  3:       1 0.77369 gblinear          <list[5]> <list[3]>      0.0000\n##  4:       1 0.55967 gblinear          <list[5]> <list[3]>      0.0000\n##  5:       1 0.46843 gblinear          <list[5]> <list[3]>      0.0000\n##  6:       1 0.27922 gblinear          <list[5]> <list[3]>      0.0000\n##  7:       1 0.79274 gblinear          <list[5]> <list[3]>      0.0000\n##  8:       1 0.58557 gblinear          <list[5]> <list[3]>      0.0000\n##  9:       1 0.39476 gblinear          <list[5]> <list[3]>      0.0000\n## 10:       1 0.58145 gblinear          <list[5]> <list[3]>      0.0000\n## 11:      16 0.01919   gbtree          <list[5]> <list[3]>      0.6374\n## 12:       4 0.55531     dart          <list[5]> <list[3]>      0.5824\n## 13:       4 0.65920   gbtree          <list[5]> <list[3]>      0.6044\n## 14:      16 0.13587     dart          <list[5]> <list[3]>      0.6264\n##     classif.fpr\n##  1:      0.0000\n##  2:      0.0000\n##  3:      0.0000\n##  4:      0.0000\n##  5:      0.0000\n##  6:      0.0000\n##  7:      0.0000\n##  8:      0.0000\n##  9:      0.0000\n## 10:      0.0000\n## 11:      0.2364\n## 12:      0.1576\n## 13:      0.1758\n## 14:      0.1818\nplot(classif.tpr~classif.fpr, instance$archive$data)\npoints(classif.tpr~classif.fpr, instance$result, col = \"red\")"},{"path":"optimization.html","id":"fs","chapter":"3 Model Optimization","heading":"3.5 Feature Selection / Filtering","text":"Often, data sets include large number features.\ntechnique extracting subset relevant features called “feature selection”.objective feature selection fit sparse dependent model subset available data features suitable manner.\nFeature selection can enhance interpretability model, speed learning process improve learner performance.\nDifferent approaches exist identify relevant features.\nTwo different approaches emphasized literature:\none called Filtering approach often referred feature subset selection wrapper methods.differences (Guyon Elisseeff 2003; Chandrashekar Sahin 2014)?Filtering:\nexternal algorithm computes rank features (e.g. based correlation response).\n, features subsetted certain criteria, e.g. absolute number percentage number variables.\nselected features used fit model (optional hyperparameters selected tuning).\ncalculation usually cheaper “feature subset selection” terms computation time.\nfilters connected via package mlr3filters.Filtering:\nexternal algorithm computes rank features (e.g. based correlation response).\n, features subsetted certain criteria, e.g. absolute number percentage number variables.\nselected features used fit model (optional hyperparameters selected tuning).\ncalculation usually cheaper “feature subset selection” terms computation time.\nfilters connected via package mlr3filters.Wrapper Methods:\n, ranking features done.\nInstead, optimization algorithm selects subset features, evaluates set calculating resampled predictive performance, \nproposes new set features (terminates).\nsimple example sequential forward selection.\nmethod usually computationally intensive lot models fitted.\nAlso, strictly speaking, models need tuned performance estimated.\nrequire additional nested level CV setting.\nundertaken steps, final set selected features fitted (optional hyperparameters selected tuning).\nWrapper methods implemented mlr3fselect package.Wrapper Methods:\n, ranking features done.\nInstead, optimization algorithm selects subset features, evaluates set calculating resampled predictive performance, \nproposes new set features (terminates).\nsimple example sequential forward selection.\nmethod usually computationally intensive lot models fitted.\nAlso, strictly speaking, models need tuned performance estimated.\nrequire additional nested level CV setting.\nundertaken steps, final set selected features fitted (optional hyperparameters selected tuning).\nWrapper methods implemented mlr3fselect package.Embedded Methods:\nMany learners internally select subset features find helpful prediction.\nsubsets can usually queried, following example demonstrates:\n\nlibrary(\"mlr3verse\")\n\ntask = tsk(\"iris\")\nlearner = lrn(\"classif.rpart\")\n\n# ensure learner selects features\nstopifnot(\"selected_features\" %% learner$properties)\n\n# fit simple classification tree\nlearner = learner$train(task)\n\n# extract features used classification tree:\nlearner$selected_features()\n## [1] \"Petal.Length\" \"Petal.Width\"Embedded Methods:\nMany learners internally select subset features find helpful prediction.\nsubsets can usually queried, following example demonstrates:also ensemble filters built upon idea stacking single filter methods. yet implemented.","code":"\nlibrary(\"mlr3verse\")\n\ntask = tsk(\"iris\")\nlearner = lrn(\"classif.rpart\")\n\n# ensure that the learner selects features\nstopifnot(\"selected_features\" %in% learner$properties)\n\n# fit a simple classification tree\nlearner = learner$train(task)\n\n# extract all features used in the classification tree:\nlearner$selected_features()## [1] \"Petal.Length\" \"Petal.Width\""},{"path":"optimization.html","id":"fs-filter","chapter":"3 Model Optimization","heading":"3.5.1 Filters","text":"Filter methods assign importance value feature.\nBased values features can ranked.\nThereafter, able select feature subset.\nlist implemented filter methods Appendix.","code":""},{"path":"optimization.html","id":"fs-calc","chapter":"3 Model Optimization","heading":"3.5.2 Calculating filter values","text":"Currently, classification regression tasks supported.first step create new R object using class desired filter method.\nSimilar instances mlr3, registered dictionary (mlr_filters) associated shortcut function flt().\nobject class Filter .$calculate() method computes filter values ranks descending order.filters support changing specific hyperparameters.\nsimilar setting hyperparameters Learner using .$param_set$values:","code":"\nfilter = flt(\"jmim\")\n\ntask = tsk(\"iris\")\nfilter$calculate(task)\n\nas.data.table(filter)##         feature  score\n## 1:  Petal.Width 1.0000\n## 2: Sepal.Length 0.6667\n## 3: Petal.Length 0.3333\n## 4:  Sepal.Width 0.0000\nfilter_cor = flt(\"correlation\")\nfilter_cor$param_set## <ParamSet>\n##        id    class lower upper nlevels    default value\n## 1:    use ParamFct    NA    NA       5 everything      \n## 2: method ParamFct    NA    NA       3    pearson\n# change parameter 'method'\nfilter_cor$param_set$values = list(method = \"spearman\")\nfilter_cor$param_set## <ParamSet>\n##        id    class lower upper nlevels    default    value\n## 1:    use ParamFct    NA    NA       5 everything         \n## 2: method ParamFct    NA    NA       3    pearson spearman"},{"path":"optimization.html","id":"fs-var-imp-filters","chapter":"3 Model Optimization","heading":"3.5.3 Variable Importance Filters","text":"Learner property “importance” come integrated feature selection methods.can find list learners property Appendix.learners desired filter method needs set learner creation.\nexample, learner classif.ranger comes multiple integrated methods, c.f. help page ranger::ranger().\nuse method “impurity”, need set filter method construction.Now can use FilterImportance filter class algorithm-embedded methods:","code":"\nlrn = lrn(\"classif.ranger\", importance = \"impurity\")\ntask = tsk(\"iris\")\nfilter = flt(\"importance\", learner = lrn)\nfilter$calculate(task)\nhead(as.data.table(filter), 3)##         feature score\n## 1: Petal.Length 45.21\n## 2:  Petal.Width 42.65\n## 3: Sepal.Length  9.29"},{"path":"optimization.html","id":"fs-wrapper","chapter":"3 Model Optimization","heading":"3.5.4 Wrapper Methods","text":"Wrapper feature selection supported via mlr3fselect extension package.\nheart mlr3fselect R6 classes:FSelectInstanceSingleCrit, FSelectInstanceMultiCrit: two classes describe feature selection problem store results.FSelector: class base class implementations feature selection algorithms.","code":""},{"path":"optimization.html","id":"fs-wrapper-optimization","chapter":"3 Model Optimization","heading":"3.5.5 The FSelectInstance Classes","text":"following sub-section examines feature selection Pima data set used predict whether patient diabetes.use classification tree rpart.Next, need specify evaluate performance feature subsets.\n, need choose resampling strategy performance measure.Finally, one choose available budget feature selection.\ndone selecting one available Terminators:Terminate given time (TerminatorClockTime)Terminate given amount iterations (TerminatorEvals)Terminate specific performance reached (TerminatorPerfReached)Terminate feature selection improve (TerminatorStagnation)combination fashion (TerminatorCombo)short introduction, specify budget 20 evaluations put everything together FSelectInstanceSingleCrit:start feature selection, still need select algorithm defined via FSelector class","code":"\ntask = tsk(\"pima\")\nprint(task)## <TaskClassif:pima> (768 x 9)\n## * Target: diabetes\n## * Properties: twoclass\n## * Features (8):\n##   - dbl (8): age, glucose, insulin, mass, pedigree, pregnant, pressure,\n##     triceps\nlearner = lrn(\"classif.rpart\")\nhout = rsmp(\"holdout\")\nmeasure = msr(\"classif.ce\")\nevals20 = trm(\"evals\", n_evals = 20)\n\ninstance = FSelectInstanceSingleCrit$new(\n  task = task,\n  learner = learner,\n  resampling = hout,\n  measure = measure,\n  terminator = evals20\n)\ninstance## <FSelectInstanceSingleCrit>\n## * State:  Not optimized\n## * Objective: <ObjectiveFSelect:classif.rpart_on_pima>\n## * Search Space:\n## <ParamSet>\n##          id    class lower upper nlevels        default value\n## 1:      age ParamLgl    NA    NA       2 <NoDefault[3]>      \n## 2:  glucose ParamLgl    NA    NA       2 <NoDefault[3]>      \n## 3:  insulin ParamLgl    NA    NA       2 <NoDefault[3]>      \n## 4:     mass ParamLgl    NA    NA       2 <NoDefault[3]>      \n## 5: pedigree ParamLgl    NA    NA       2 <NoDefault[3]>      \n## 6: pregnant ParamLgl    NA    NA       2 <NoDefault[3]>      \n## 7: pressure ParamLgl    NA    NA       2 <NoDefault[3]>      \n## 8:  triceps ParamLgl    NA    NA       2 <NoDefault[3]>      \n## * Terminator: <TerminatorEvals>\n## * Terminated: FALSE\n## * Archive:\n## <ArchiveFSelect>\n## Null data.table (0 rows and 0 cols)"},{"path":"optimization.html","id":"the-fselector-class","chapter":"3 Model Optimization","heading":"3.5.6 The FSelector Class","text":"following algorithms currently implemented mlr3fselect:Random Search (FSelectorRandomSearch)Exhaustive Search (FSelectorExhaustiveSearch)Sequential Search (FSelectorSequential)Recursive Feature Elimination (FSelectorRFE)Design Points (FSelectorDesignPoints)example, use simple random search retrieve dictionary mlr_fselectors fs() function:","code":"\nfselector = fs(\"random_search\")"},{"path":"optimization.html","id":"wrapper-selection-triggering","chapter":"3 Model Optimization","heading":"3.5.7 Triggering the Tuning","text":"start feature selection, simply pass FSelectInstanceSingleCrit $optimize() method initialized FSelector. algorithm proceeds followsThe FSelector proposes least one feature subset may propose multiple subsets improve parallelization, can controlled via setting batch_size).feature subset, given Learner fitted Task using provided Resampling.\nevaluations stored archive FSelectInstanceSingleCrit.Terminator queried budget exhausted.\nbudget exhausted, restart 1) .Determine feature subset best observed performance.Store best feature subset result instance object.\nbest feature subset ($result_feature_set) corresponding measured performance ($result_y) can accessed instance.One can investigate resamplings undertaken, stored archive FSelectInstanceSingleCrit can accessed using .data.table():associated resampling iterations can accessed BenchmarkResult:uhash column links resampling iterations evaluated feature subsets stored instance$archive$data(). allows e.g. score included ResampleResults different measure.Now optimized feature subset can used subset task fit model observations.trained model can now used make prediction external data.\nNote predicting observations present task, avoided.\nmodel seen observations already feature selection therefore results statistically biased.\nHence, resulting performance measure -optimistic.\nInstead, get statistically unbiased performance estimates current task, nested resampling required.","code":"\n# reduce logging output\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\nfselector$optimize(instance)##     age glucose insulin mass pedigree pregnant pressure triceps\n## 1: TRUE    TRUE    TRUE TRUE     TRUE     TRUE     TRUE    TRUE\n##                                          features classif.ce\n## 1: age,glucose,insulin,mass,pedigree,pregnant,...      0.207\ninstance$result_feature_set## [1] \"age\"      \"glucose\"  \"insulin\"  \"mass\"     \"pedigree\" \"pregnant\" \"pressure\"\n## [8] \"triceps\"\ninstance$result_y## classif.ce \n##      0.207\nas.data.table(instance$archive)##       age glucose insulin  mass pedigree pregnant pressure triceps classif.ce\n##  1:  TRUE   FALSE   FALSE FALSE    FALSE    FALSE    FALSE   FALSE     0.3242\n##  2:  TRUE    TRUE    TRUE FALSE     TRUE     TRUE     TRUE    TRUE     0.2227\n##  3: FALSE   FALSE   FALSE FALSE    FALSE     TRUE    FALSE   FALSE     0.3477\n##  4: FALSE    TRUE    TRUE  TRUE     TRUE     TRUE    FALSE    TRUE     0.2734\n##  5: FALSE    TRUE   FALSE FALSE     TRUE    FALSE    FALSE   FALSE     0.2617\n##  6:  TRUE    TRUE    TRUE  TRUE    FALSE     TRUE     TRUE    TRUE     0.2188\n##  7:  TRUE   FALSE   FALSE FALSE    FALSE     TRUE    FALSE   FALSE     0.3203\n##  8:  TRUE   FALSE    TRUE FALSE     TRUE     TRUE    FALSE   FALSE     0.3125\n##  9: FALSE    TRUE   FALSE  TRUE    FALSE     TRUE     TRUE   FALSE     0.2500\n## 10:  TRUE   FALSE   FALSE FALSE    FALSE    FALSE     TRUE   FALSE     0.3672\n## 11: FALSE   FALSE   FALSE  TRUE    FALSE    FALSE    FALSE    TRUE     0.3945\n## 12:  TRUE   FALSE    TRUE  TRUE    FALSE     TRUE    FALSE    TRUE     0.3008\n## 13:  TRUE    TRUE   FALSE  TRUE    FALSE     TRUE    FALSE    TRUE     0.2344\n## 14: FALSE   FALSE   FALSE FALSE    FALSE     TRUE    FALSE   FALSE     0.3477\n## 15:  TRUE    TRUE    TRUE FALSE     TRUE     TRUE     TRUE   FALSE     0.2227\n## 16:  TRUE    TRUE    TRUE  TRUE     TRUE     TRUE     TRUE    TRUE     0.2070\n## 17: FALSE    TRUE   FALSE FALSE     TRUE     TRUE    FALSE   FALSE     0.2461\n## 18:  TRUE    TRUE    TRUE  TRUE     TRUE     TRUE     TRUE    TRUE     0.2070\n## 19:  TRUE   FALSE    TRUE  TRUE     TRUE    FALSE     TRUE    TRUE     0.3086\n## 20:  TRUE    TRUE   FALSE FALSE     TRUE     TRUE     TRUE    TRUE     0.2266\n##     runtime_learners           timestamp batch_nr      resample_result\n##  1:            0.070 2021-09-19 14:22:30        1 <ResampleResult[20]>\n##  2:            0.056 2021-09-19 14:22:30        2 <ResampleResult[20]>\n##  3:            0.053 2021-09-19 14:22:30        3 <ResampleResult[20]>\n##  4:            0.060 2021-09-19 14:22:31        4 <ResampleResult[20]>\n##  5:            0.062 2021-09-19 14:22:31        5 <ResampleResult[20]>\n##  6:            0.063 2021-09-19 14:22:31        6 <ResampleResult[20]>\n##  7:            0.054 2021-09-19 14:22:31        7 <ResampleResult[20]>\n##  8:            0.055 2021-09-19 14:22:31        8 <ResampleResult[20]>\n##  9:            0.062 2021-09-19 14:22:32        9 <ResampleResult[20]>\n## 10:            0.065 2021-09-19 14:22:32       10 <ResampleResult[20]>\n## 11:            0.067 2021-09-19 14:22:32       11 <ResampleResult[20]>\n## 12:            0.069 2021-09-19 14:22:32       12 <ResampleResult[20]>\n## 13:            0.061 2021-09-19 14:22:33       13 <ResampleResult[20]>\n## 14:            0.053 2021-09-19 14:22:33       14 <ResampleResult[20]>\n## 15:            0.057 2021-09-19 14:22:33       15 <ResampleResult[20]>\n## 16:            0.073 2021-09-19 14:22:33       16 <ResampleResult[20]>\n## 17:            0.058 2021-09-19 14:22:33       17 <ResampleResult[20]>\n## 18:            0.062 2021-09-19 14:22:34       18 <ResampleResult[20]>\n## 19:            0.071 2021-09-19 14:22:34       19 <ResampleResult[20]>\n## 20:            0.062 2021-09-19 14:22:34       20 <ResampleResult[20]>\ninstance$archive$benchmark_result$data## Warning: '.__BenchmarkResult__data' is deprecated.\n## Use 'as.data.table(benchmark_result)' instead.\n## See help(\"Deprecated\")## <ResultData>\n##   Public:\n##     as_data_table: function (view = NULL, reassemble_learners = TRUE, convert_predictions = TRUE, \n##     clone: function (deep = FALSE) \n##     combine: function (rdata) \n##     data: list\n##     initialize: function (data = NULL, store_backends = TRUE) \n##     iterations: function (view = NULL) \n##     learners: function (view = NULL, states = TRUE, reassemble = TRUE) \n##     logs: function (view = NULL, condition) \n##     prediction: function (view = NULL, predict_sets = \"test\") \n##     predictions: function (view = NULL, predict_sets = \"test\") \n##     resamplings: function (view = NULL) \n##     sweep: function () \n##     task_type: active binding\n##     tasks: function (view = NULL) \n##     uhashes: function (view = NULL) \n##   Private:\n##     deep_clone: function (name, value) \n##     get_view_index: function (view)\ntask$select(instance$result_feature_set)\nlearner$train(task)"},{"path":"optimization.html","id":"autofselect","chapter":"3 Model Optimization","heading":"3.5.8 Automating the Feature Selection","text":"AutoFSelector wraps learner augments automatic feature selection given task.\nAutoFSelector inherits Learner base class, can used like learner.\nAnalogously previous subsection, new classification tree learner created.\nclassification tree learner automatically starts feature selection given task using inner resampling (holdout).\ncreate terminator allows 10 evaluations, uses simple random search feature selection algorithm:can now use learner like learner, calling $train() $predict() method.\ntime however, pass benchmark() compare optimized feature subset complete feature set.\nway, AutoFSelector resampling feature selection training set respective split outer resampling.\nlearner undertakes predictions using test set outer resampling.\nyields unbiased performance measures, observations test set used feature selection fitting respective learner.\ncalled nested resampling.compare optimized feature subset complete feature set, can use benchmark():Note expect significant differences since evaluated small fraction possible feature subsets.","code":"\nlearner = lrn(\"classif.rpart\")\nterminator = trm(\"evals\", n_evals = 10)\nfselector = fs(\"random_search\")\n\nat = AutoFSelector$new(\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\"),\n  terminator = terminator,\n  fselector = fselector\n)\nat## <AutoFSelector:classif.rpart.fselector>\n## * Model: -\n## * Parameters: xval=0\n## * Packages: rpart\n## * Predict Type: response\n## * Feature types: logical, integer, numeric, factor, ordered\n## * Properties: importance, missings, multiclass, selected_features,\n##   twoclass, weights\ngrid = benchmark_grid(\n  task = tsk(\"pima\"),\n  learner = list(at, lrn(\"classif.rpart\")),\n  resampling = rsmp(\"cv\", folds = 3)\n)\n\nbmr = benchmark(grid, store_models = TRUE)\nbmr$aggregate(msrs(c(\"classif.ce\", \"time_train\")))##    nr      resample_result task_id              learner_id resampling_id iters\n## 1:  1 <ResampleResult[20]>    pima classif.rpart.fselector            cv     3\n## 2:  2 <ResampleResult[20]>    pima           classif.rpart            cv     3\n##    classif.ce time_train\n## 1:     0.2513          0\n## 2:     0.2552          0"},{"path":"pipelines.html","id":"pipelines","chapter":"4 Pipelines","heading":"4 Pipelines","text":"mlr3pipelines (Binder et al. 2021) dataflow programming toolkit.\nchapter focuses applicant’s side package.\n-depth technically oriented guide can found -depth look mlr3pipelines chapter.Machine learning workflows can written directed “Graphs”/“Pipelines” represent data flows preprocessing, model fitting, ensemble learning units expressive intuitive language.\noften use term “Graph” manual can interchangeably used “pipeline” “workflow”.can examine example graph:Single computational steps can represented -called PipeOps, can connected directed edges Graph.\nscope mlr3pipelines still growing.\nCurrently supported features :Data manipulation preprocessing operations, e.g. PCA, feature filtering, imputationTask subsampling speed outcome class imbalance handlingmlr3 Learner operations prediction stackingEnsemble methods aggregation predictionsAdditionally, implement several meta operators can used construct powerful pipelines:Simultaneous path branching (data going ways)Alternative path branching (data going one specific way, controlled hyperparameters)extensive introduction creating custom PipeOps (PO’s) can found technical introduction.Using methods mlr3tuning, even possible simultaneously optimize parameters multiple processing units.predecessor package mlrCPO package, works mlr 2.x.\npackages provide, varying degree, preprocessing functionality machine learning domain specific language, :caret package related recipes projectthe dplyr packageAn example Pipeline can constructed using mlr3pipelines depicted :","code":""},{"path":"pipelines.html","id":"pipe-pipeops","chapter":"4 Pipelines","heading":"4.1 The Building Blocks: PipeOps","text":"building blocks mlr3pipelines PipeOp-objects (PO).\ncan constructed directly using PipeOp<NAME>$new(), recommended way retrieve mlr_pipeops dictionary:Single POs can created using po(<name>):using syntactic sugarSome POs require additional arguments construction:short po(\"learner\", lrn(\"classif.rpart\")).Hyperparameters POs can set param_vals argument.\nset fraction features filter:short notation:figure shows exemplary PipeOp.\ntakes input, transforms .$train .$predict returns data:","code":"\nlibrary(\"mlr3pipelines\")\nas.data.table(mlr_pipeops)##                       key           packages                             tags\n##  1:                boxcox      bestNormalize                   data transform\n##  2:                branch                                                meta\n##  3:                 chunk                                                meta\n##  4:        classbalancing                      imbalanced data,data transform\n##  5:            classifavg              stats                         ensemble\n##  6:          classweights                      imbalanced data,data transform\n##  7:              colapply                                      data transform\n##  8:       collapsefactors                                      data transform\n##  9:              colroles                                      data transform\n## 10:                  copy                                                meta\n## 11:          datefeatures                                      data transform\n## 12:                encode              stats            encode,data transform\n## 13:          encodeimpact                               encode,data transform\n## 14:            encodelmer        lme4,nloptr            encode,data transform\n## 15:          featureunion                                            ensemble\n## 16:                filter                    feature selection,data transform\n## 17:            fixfactors                            robustify,data transform\n## 18:               histbin           graphics                   data transform\n## 19:                   ica            fastICA                   data transform\n## 20:        imputeconstant                                            missings\n## 21:            imputehist           graphics                         missings\n## 22:         imputelearner                                            missings\n## 23:            imputemean                                            missings\n## 24:          imputemedian              stats                         missings\n## 25:            imputemode                                            missings\n## 26:             imputeoor                                            missings\n## 27:          imputesample                                            missings\n## 28:             kernelpca            kernlab                   data transform\n## 29:               learner                                             learner\n## 30:            learner_cv                     learner,ensemble,data transform\n## 31:               missind                             missings,data transform\n## 32:           modelmatrix              stats                   data transform\n## 33:     multiplicityexply                                        multiplicity\n## 34:     multiplicityimply                                        multiplicity\n## 35:                mutate                                      data transform\n## 36:                   nmf           MASS,NMF                   data transform\n## 37:                   nop                                                meta\n## 38:              ovrsplit                       target transform,multiplicity\n## 39:              ovrunite                               multiplicity,ensemble\n## 40:                   pca                                      data transform\n## 41:                 proxy                                                meta\n## 42:           quantilebin              stats                   data transform\n## 43:      randomprojection                                      data transform\n## 44:        randomresponse                                            abstract\n## 45:               regravg                                            ensemble\n## 46:       removeconstants                            robustify,data transform\n## 47:         renamecolumns                                      data transform\n## 48:             replicate                                        multiplicity\n## 49:                 scale                                      data transform\n## 50:           scalemaxabs                                      data transform\n## 51:            scalerange                                      data transform\n## 52:                select                    feature selection,data transform\n## 53:                 smote        smotefamily   imbalanced data,data transform\n## 54:           spatialsign                                      data transform\n## 55:             subsample                                      data transform\n## 56:          targetinvert                                            abstract\n## 57:          targetmutate                                    target transform\n## 58: targettrafoscalerange                                    target transform\n## 59:        textvectorizer quanteda,stopwords                   data transform\n## 60:             threshold                                    target transform\n## 61:         tunethreshold              bbotk                 target transform\n## 62:              unbranch                                                meta\n## 63:                vtreat             vtreat   encode,missings,data transform\n## 64:            yeojohnson      bestNormalize                   data transform\n##                       key           packages                             tags\n##                                            feature_types input.num output.num\n##  1:                                      numeric,integer         1          1\n##  2:                                                   NA         1         NA\n##  3:                                                   NA         1         NA\n##  4: logical,integer,numeric,character,factor,ordered,...         1          1\n##  5:                                                   NA        NA          1\n##  6: logical,integer,numeric,character,factor,ordered,...         1          1\n##  7: logical,integer,numeric,character,factor,ordered,...         1          1\n##  8:                                       factor,ordered         1          1\n##  9: logical,integer,numeric,character,factor,ordered,...         1          1\n## 10:                                                   NA         1         NA\n## 11:                                              POSIXct         1          1\n## 12:                                       factor,ordered         1          1\n## 13:                                       factor,ordered         1          1\n## 14:                                       factor,ordered         1          1\n## 15:                                                   NA        NA          1\n## 16: logical,integer,numeric,character,factor,ordered,...         1          1\n## 17:                                       factor,ordered         1          1\n## 18:                                      numeric,integer         1          1\n## 19:                                      numeric,integer         1          1\n## 20: logical,integer,numeric,character,factor,ordered,...         1          1\n## 21:                                      integer,numeric         1          1\n## 22:                               logical,factor,ordered         1          1\n## 23:                                      numeric,integer         1          1\n## 24:                                      numeric,integer         1          1\n## 25:               factor,integer,logical,numeric,ordered         1          1\n## 26:             character,factor,integer,numeric,ordered         1          1\n## 27:               factor,integer,logical,numeric,ordered         1          1\n## 28:                                      numeric,integer         1          1\n## 29:                                                   NA         1          1\n## 30: logical,integer,numeric,character,factor,ordered,...         1          1\n## 31: logical,integer,numeric,character,factor,ordered,...         1          1\n## 32: logical,integer,numeric,character,factor,ordered,...         1          1\n## 33:                                                   NA         1         NA\n## 34:                                                   NA        NA          1\n## 35: logical,integer,numeric,character,factor,ordered,...         1          1\n## 36:                                      numeric,integer         1          1\n## 37:                                                   NA         1          1\n## 38:                                                   NA         1          1\n## 39:                                                   NA         1          1\n## 40:                                      numeric,integer         1          1\n## 41:                                                   NA        NA          1\n## 42:                                      numeric,integer         1          1\n## 43:                                      numeric,integer         1          1\n## 44:                                                   NA         1          1\n## 45:                                                   NA        NA          1\n## 46: logical,integer,numeric,character,factor,ordered,...         1          1\n## 47: logical,integer,numeric,character,factor,ordered,...         1          1\n## 48:                                                   NA         1          1\n## 49:                                      numeric,integer         1          1\n## 50:                                      numeric,integer         1          1\n## 51:                                      numeric,integer         1          1\n## 52: logical,integer,numeric,character,factor,ordered,...         1          1\n## 53: logical,integer,numeric,character,factor,ordered,...         1          1\n## 54:                                      numeric,integer         1          1\n## 55: logical,integer,numeric,character,factor,ordered,...         1          1\n## 56:                                                   NA         2          1\n## 57:                                                   NA         1          2\n## 58:                                                   NA         1          2\n## 59:                                            character         1          1\n## 60:                                                   NA         1          1\n## 61:                                                   NA         1          1\n## 62:                                                   NA        NA          1\n## 63: logical,integer,numeric,character,factor,ordered,...         1          1\n## 64:                                      numeric,integer         1          1\n##                                            feature_types input.num output.num\n##     input.type.train  input.type.predict output.type.train output.type.predict\n##  1:             Task                Task              Task                Task\n##  2:                *                   *                 *                   *\n##  3:             Task                Task              Task                Task\n##  4:      TaskClassif         TaskClassif       TaskClassif         TaskClassif\n##  5:             NULL   PredictionClassif              NULL   PredictionClassif\n##  6:      TaskClassif         TaskClassif       TaskClassif         TaskClassif\n##  7:             Task                Task              Task                Task\n##  8:             Task                Task              Task                Task\n##  9:             Task                Task              Task                Task\n## 10:                *                   *                 *                   *\n## 11:             Task                Task              Task                Task\n## 12:             Task                Task              Task                Task\n## 13:             Task                Task              Task                Task\n## 14:             Task                Task              Task                Task\n## 15:             Task                Task              Task                Task\n## 16:             Task                Task              Task                Task\n## 17:             Task                Task              Task                Task\n## 18:             Task                Task              Task                Task\n## 19:             Task                Task              Task                Task\n## 20:             Task                Task              Task                Task\n## 21:             Task                Task              Task                Task\n## 22:             Task                Task              Task                Task\n## 23:             Task                Task              Task                Task\n## 24:             Task                Task              Task                Task\n## 25:             Task                Task              Task                Task\n## 26:             Task                Task              Task                Task\n## 27:             Task                Task              Task                Task\n## 28:             Task                Task              Task                Task\n## 29:      TaskClassif         TaskClassif              NULL   PredictionClassif\n## 30:      TaskClassif         TaskClassif       TaskClassif         TaskClassif\n## 31:             Task                Task              Task                Task\n## 32:             Task                Task              Task                Task\n## 33:              [*]                 [*]                 *                   *\n## 34:                *                   *               [*]                 [*]\n## 35:             Task                Task              Task                Task\n## 36:             Task                Task              Task                Task\n## 37:                *                   *                 *                   *\n## 38:      TaskClassif         TaskClassif     [TaskClassif]       [TaskClassif]\n## 39:           [NULL] [PredictionClassif]              NULL   PredictionClassif\n## 40:             Task                Task              Task                Task\n## 41:                *                   *                 *                   *\n## 42:             Task                Task              Task                Task\n## 43:             Task                Task              Task                Task\n## 44:             NULL          Prediction              NULL          Prediction\n## 45:             NULL      PredictionRegr              NULL      PredictionRegr\n## 46:             Task                Task              Task                Task\n## 47:             Task                Task              Task                Task\n## 48:                *                   *               [*]                 [*]\n## 49:             Task                Task              Task                Task\n## 50:             Task                Task              Task                Task\n## 51:             Task                Task              Task                Task\n## 52:             Task                Task              Task                Task\n## 53:             Task                Task              Task                Task\n## 54:             Task                Task              Task                Task\n## 55:             Task                Task              Task                Task\n## 56:        NULL,NULL function,Prediction              NULL          Prediction\n## 57:             Task                Task         NULL,Task       function,Task\n## 58:         TaskRegr            TaskRegr     NULL,TaskRegr   function,TaskRegr\n## 59:             Task                Task              Task                Task\n## 60:             NULL   PredictionClassif              NULL   PredictionClassif\n## 61:             Task                Task              NULL          Prediction\n## 62:                *                   *                 *                   *\n## 63:             Task                Task              Task                Task\n## 64:             Task                Task              Task                Task\n##     input.type.train  input.type.predict output.type.train output.type.predict\npca = po(\"pca\")\npca = po(\"pca\")\nlearner = po(\"learner\")\n\n# Error in as_learner(learner) : argument \"learner\" is missing, with no default argument \"learner\" is missing, with no default\nlearner = po(\"learner\", lrn(\"classif.rpart\"))\nfilter = po(\"filter\",\n  filter = mlr3filters::flt(\"variance\"),\n  param_vals = list(filter.frac = 0.5))\npo(\"filter\", mlr3filters::flt(\"variance\"), filter.frac = 0.5)"},{"path":"pipelines.html","id":"pipe-operator","chapter":"4 Pipelines","heading":"4.2 The Pipeline Operator: %>>%","text":"possible create intricate Graphs edges going place (long loops introduced).\nIrrespective, usually clear direction flow “layers” Graph.\ntherefore convenient build Graph layers.\ncan done using %>>% (“double-arrow”) operator.\ntakes either PipeOp Graph sides connects outputs left-hand side one inputs right-hand side.\nnumber inputs therefore must match number outputs.","code":"\nlibrary(\"magrittr\")\n\ngr = po(\"scale\") %>>% po(\"pca\")\ngr$plot(html = FALSE)"},{"path":"pipelines.html","id":"pipe-nodes-edges-graphs","chapter":"4 Pipelines","heading":"4.3 Nodes, Edges and Graphs","text":"POs combined Graphs.\nmanual way (= hard way) construct Graph create empty graph first.\none fills empty graph POs, connects edges POs.\nConceptually, may look like :POs identified $id.\nNote operations modify object -place return object .\nTherefore, multiple modifications can chained.example use pca PO defined new PO named “mutate”.\nlatter creates new feature existing variables.\nAdditionally, use filter PO .much quicker way use %>>% operator chain POs Graph s.\nresult can achieved following:Now Graph can inspected using $plot() function:Chaining multiple POs kindIf multiple POs kind chained, necessary change id avoid name clashes.\ncan done either accessing $id slot construction:","code":"\nmutate = po(\"mutate\")\n\nfilter = po(\"filter\",\n  filter = mlr3filters::flt(\"variance\"),\n  param_vals = list(filter.frac = 0.5))\ngraph = Graph$new()$\n  add_pipeop(mutate)$\n  add_pipeop(filter)$\n  add_edge(\"mutate\", \"variance\")  # add connection mutate -> filter\ngraph = mutate %>>% filter\ngraph$plot()\ngraph$add_pipeop(po(\"pca\"))\ngraph$add_pipeop(po(\"pca\", id = \"pca2\"))"},{"path":"pipelines.html","id":"pipe-modeling","chapter":"4 Pipelines","heading":"4.4 Modeling","text":"main purpose Graph build combined preprocessing model fitting pipelines can used mlr3 Learner.Conceptually, process may summarized follows:following chain two preprocessing tasks:mutate (creation new feature)filter (filtering dataset)Subsequently one can chain PO learner train predict modified dataset.defined main pipeline stored Graph.\nNow can train predict pipeline:Rather calling $train() $predict() manually, can put pipeline Graph GraphLearner object.\nGraphLearner encapsulates whole pipeline (including preprocessing steps) can put resample() benchmark() .\nfamiliar old mlr package, equivalent make*Wrapper() functions.\npipeline encapsulated (Graph ) must always produce Prediction $predict() call, probably contain least one PipeOpLearner .learner can used model fitting, resampling, benchmarking, tuning:","code":"\nmutate = po(\"mutate\")\nfilter = po(\"filter\",\n  filter = mlr3filters::flt(\"variance\"),\n  param_vals = list(filter.frac = 0.5))\n\ngraph = mutate %>>%\n  filter %>>%\n  po(\"learner\",\n    learner = lrn(\"classif.rpart\"))\ntask = tsk(\"iris\")\ngraph$train(task)## $classif.rpart.output\n## NULL\ngraph$predict(task)## $classif.rpart.output\n## <PredictionClassif> for 150 observations:\n##     row_ids     truth  response\n##           1    setosa    setosa\n##           2    setosa    setosa\n##           3    setosa    setosa\n## ---                            \n##         148 virginica virginica\n##         149 virginica virginica\n##         150 virginica virginica\nglrn = as_learner(graph)\ncv3 = rsmp(\"cv\", folds = 3)\nresample(task, glrn, cv3)## <ResampleResult> of 3 iterations\n## * Task: iris\n## * Learner: mutate.variance.classif.rpart\n## * Warnings: 0 in 0 iterations\n## * Errors: 0 in 0 iterations"},{"path":"pipelines.html","id":"pipe-hyperpars","chapter":"4 Pipelines","heading":"4.4.1 Setting Hyperparameters","text":"Individual POs offer hyperparameters contain $param_set slots can read written $param_set$values (via paradox package).\nparameters get passed Graph, finally GraphLearner .\nmakes possible easily change behavior Graph / GraphLearner try different settings manually, also perform tuning using mlr3tuning package.","code":"\nglrn$param_set$values$variance.filter.frac = 0.25\ncv3 = rsmp(\"cv\", folds = 3)\nresample(task, glrn, cv3)## <ResampleResult> of 3 iterations\n## * Task: iris\n## * Learner: mutate.variance.classif.rpart\n## * Warnings: 0 in 0 iterations\n## * Errors: 0 in 0 iterations"},{"path":"pipelines.html","id":"pipe-tuning","chapter":"4 Pipelines","heading":"4.4.2 Tuning","text":"unfamiliar tuning mlr3, recommend take look section tuning first.\ndefine ParamSet “rpart” learner “variance” filter optimized tuning process.defined PerformanceEvaluator, random search 10 iterations created.\ninner resampling, simply using holdout (single split train/test) keep runtimes reasonable.tuning result can found respective result slots.","code":"\nlibrary(\"paradox\")\nps = ps(\n  classif.rpart.cp = p_dbl(lower = 0, upper = 0.05),\n  variance.filter.frac = p_dbl(lower = 0.25, upper = 1)\n)\nlibrary(\"mlr3tuning\")\ninstance = TuningInstanceSingleCrit$new(\n  task = task,\n  learner = glrn,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\"),\n  search_space = ps,\n  terminator = trm(\"evals\", n_evals = 20)\n)\ntuner = tnr(\"random_search\")\ntuner$optimize(instance)\ninstance$result_learner_param_vals\ninstance$result_y"},{"path":"pipelines.html","id":"pipe-nonlinear","chapter":"4 Pipelines","heading":"4.5 Non-Linear Graphs","text":"Graphs seen far linear structure.\nPOs may multiple input output channels.\nchannels make possible create non-linear Graphs alternative paths taken data.Possible types :Branching:\nSplitting node several paths, e.g. useful comparing multiple feature-selection methods (pca, filters).\none path executed.Copying:\nSplitting node several paths, paths executed (sequentially).\nParallel execution yet supported.Stacking:\nSingle graphs stacked onto , .e. output one Graph input another.\nmachine learning means prediction one Graph used input another Graph","code":""},{"path":"pipelines.html","id":"pipe-model-ensembles-branching-copying","chapter":"4 Pipelines","heading":"4.5.1 Branching & Copying","text":"PipeOpBranch PipeOpUnbranch POs make possible specify multiple alternative paths.\none path actually executed, others ignored.\nactive path determined hyperparameter.\nconcept makes possible tune alternative preprocessing paths (learner models).conceptual visualization branching:PipeOp(Un)Branch initialized either number branches, character-vector indicating names branches.\nnames given, “branch-choosing” hyperparameter becomes readable.\nfollowing, set three options:nothing (“nop”)Applying PCAScaling dataIt important “unbranch” “branching”, outputs merged one result objects.following first create branched graph show happens “unbranching” applied:Without “unbranching” one creates following graph:Now “unbranching”, obtain following results:can achieved using shorter notation:","code":"\ngraph = po(\"branch\", c(\"nop\", \"pca\", \"scale\")) %>>%\n  gunion(list(\n    po(\"nop\", id = \"null1\"),\n    po(\"pca\"),\n    po(\"scale\")\n  ))\ngraph$plot(html = FALSE)\n(graph %>>% po(\"unbranch\", c(\"nop\", \"pca\", \"scale\")))$plot(html = FALSE)\n# List of pipeops\nopts = list(po(\"nop\", \"no_op\"), po(\"pca\"), po(\"scale\"))\n# List of po ids\nopt_ids = mlr3misc::map_chr(opts, `[[`, \"id\")\npo(\"branch\", options = opt_ids) %>>%\n  gunion(opts) %>>%\n  po(\"unbranch\", options = opt_ids)## Graph with 5 PipeOps:\n##        ID         State        sccssors       prdcssors\n##    branch <<UNTRAINED>> no_op,pca,scale                \n##     no_op <<UNTRAINED>>        unbranch          branch\n##       pca <<UNTRAINED>>        unbranch          branch\n##     scale <<UNTRAINED>>        unbranch          branch\n##  unbranch <<UNTRAINED>>                 no_op,pca,scale"},{"path":"pipelines.html","id":"pipe-model-ensembles","chapter":"4 Pipelines","heading":"4.5.2 Model Ensembles","text":"can leverage different operations presented connect POs.\nallows us form powerful graphs.go details, split task train test indices.","code":"\ntask = tsk(\"iris\")\ntrain.idx = sample(seq_len(task$nrow), 120)\ntest.idx = setdiff(seq_len(task$nrow), train.idx)"},{"path":"pipelines.html","id":"pipe-model-ensembles-bagging","chapter":"4 Pipelines","heading":"4.5.2.1 Bagging","text":"first examine Bagging introduced (Breiman 1996).\nbasic idea create multiple predictors aggregate single, powerful predictor.“… multiple versions formed\nmaking bootstrap replicates learning set\nusing new learning sets” (Breiman 1996)Bagging aggregates set predictors averaging (regression) majority vote (classification).\nidea behind bagging , set weak, different predictors can combined order arrive single, better predictor.can achieve downsampling data training learner, repeating e.g. 10 times performing majority vote predictions.\nGraphically, may summarized follows:First, create simple pipeline, uses PipeOpSubsample PipeOpLearner trained:can now copy operation 10 times using pipeline_greplicate.\npipeline_greplicate allows us parallelize many copies operation creating Graph containing n copies input Graph.\ncan also create using syntactic sugar via ppl():Afterwards need aggregate 10 pipelines form single model:Now can plot see happens:pipeline can used conjunction GraphLearner order Bagging used like Learner:conjunction different Backends, can powerful tool.\ncases data fully fit memory, one can obtain fraction data learner DataBackend aggregate predictions learners.","code":"\nsingle_pred = po(\"subsample\", frac = 0.7) %>>%\n  po(\"learner\", lrn(\"classif.rpart\"))\npred_set = ppl(\"greplicate\", single_pred, 10L)\nbagging = pred_set %>>%\n  po(\"classifavg\", innum = 10)\nbagging$plot(html = FALSE)\nbaglrn = as_learner(bagging)\nbaglrn$train(task, train.idx)\nbaglrn$predict(task, test.idx)## <PredictionClassif> for 30 observations:\n##     row_ids     truth  response prob.setosa prob.versicolor prob.virginica\n##           4    setosa    setosa           1               0              0\n##          12    setosa    setosa           1               0              0\n##          13    setosa    setosa           1               0              0\n## ---                                                                       \n##         138 virginica virginica           0               0              1\n##         144 virginica virginica           0               0              1\n##         145 virginica virginica           0               0              1"},{"path":"pipelines.html","id":"pipe-model-ensembles-stacking","chapter":"4 Pipelines","heading":"4.5.2.2 Stacking","text":"Stacking (Wolpert 1992) another technique can improve model performance.\nbasic idea behind stacking use predictions one model features subsequent model possibly improve performance.conceptual illustration stacking:example can train decision tree use predictions model conjunction original features order train additional model top.limit overfitting, additionally predict original predictions learner.\nInstead, predict --bag predictions.\n, can use PipeOpLearnerCV .PipeOpLearnerCV performs nested cross-validation training data, fitting model fold.\nmodels used predict --fold data.\nresult, obtain predictions every data point input data.first create “level 0” learner, used extract lower level prediction.\nAdditionally, clone() learner object obtain copy learner.\nSubsequently, one sets custom id PipeOp .use PipeOpNOP combination gunion, order send unchanged Task next level.\ncombined predictions decision tree learner.Afterwards, want concatenate predictions PipeOpLearnerCV original Task using PipeOpFeatureUnion :Now can train another learner top combined features:vignette, showed simple use-case stacking.\nmany real-world applications, stacking done multiple levels multiple representations dataset.\nlower level, different preprocessing methods can defined conjunction several learners.\nhigher level, can combine predictions order form powerful model.","code":"\nlrn = lrn(\"classif.rpart\")\nlrn_0 = po(\"learner_cv\", lrn$clone())\nlrn_0$id = \"rpart_cv\"\nlevel_0 = gunion(list(lrn_0, po(\"nop\")))\ncombined = level_0 %>>% po(\"featureunion\", 2)\nstack = combined %>>% po(\"learner\", lrn$clone())\nstack$plot(html = FALSE)\nstacklrn = as_learner(stack)\nstacklrn$train(task, train.idx)\nstacklrn$predict(task, test.idx)## <PredictionClassif> for 30 observations:\n##     row_ids     truth  response\n##           4    setosa    setosa\n##          12    setosa    setosa\n##          13    setosa    setosa\n## ---                            \n##         138 virginica virginica\n##         144 virginica virginica\n##         145 virginica virginica"},{"path":"pipelines.html","id":"multilevel-stacking","chapter":"4 Pipelines","heading":"4.5.2.3 Multilevel Stacking","text":"order showcase power mlr3pipelines, show complicated stacking example.case, train glmnet 2 different rpart models (transform inputs using PipeOpPCA ) task “level 0” concatenate original features (via gunion).\nresult passed “level 1”, copy concatenated features 3 times put task rpart glmnet model.\nAdditionally, keep version “level 0” output (via PipeOpNOP) pass “level 2”.\n“level 2” simply concatenate “level 1” outputs train final decision tree.following examples, use <lrn>$param_set$values$<param_name> = <param_value> set hyperparameters\ndifferent learner.can call .$train .$predict:","code":"\nlibrary(\"magrittr\")\nlibrary(\"mlr3learners\") # for classif.glmnet\n\nrprt = lrn(\"classif.rpart\", predict_type = \"prob\")\nglmn = lrn(\"classif.glmnet\", predict_type = \"prob\")\n\n#  Create Learner CV Operators\nlrn_0 = po(\"learner_cv\", rprt, id = \"rpart_cv_1\")\nlrn_0$param_set$values$maxdepth = 5L\nlrn_1 = po(\"pca\", id = \"pca1\") %>>% po(\"learner_cv\", rprt, id = \"rpart_cv_2\")\nlrn_1$param_set$values$rpart_cv_2.maxdepth = 1L\nlrn_2 = po(\"pca\", id = \"pca2\") %>>% po(\"learner_cv\", glmn)\n\n# Union them with a PipeOpNULL to keep original features\nlevel_0 = gunion(list(lrn_0, lrn_1, lrn_2, po(\"nop\", id = \"NOP1\")))\n\n# Cbind the output 3 times, train 2 learners but also keep level\n# 0 predictions\nlevel_1 = level_0 %>>%\n  po(\"featureunion\", 4) %>>%\n  po(\"copy\", 3) %>>%\n  gunion(list(\n    po(\"learner_cv\", rprt, id = \"rpart_cv_l1\"),\n    po(\"learner_cv\", glmn, id = \"glmnt_cv_l1\"),\n    po(\"nop\", id = \"NOP_l1\")\n  ))\n\n# Cbind predictions, train a final learner\nlevel_2 = level_1 %>>%\n  po(\"featureunion\", 3, id = \"u2\") %>>%\n  po(\"learner\", rprt, id = \"rpart_l2\")\n\n# Plot the resulting graph\nlevel_2$plot(html = FALSE)\ntask = tsk(\"iris\")\nlrn = as_learner(level_2)\nlrn$\n  train(task, train.idx)$\n  predict(task, test.idx)$\n  score()## classif.ce \n##    0.03333"},{"path":"pipelines.html","id":"pipe-special-ops","chapter":"4 Pipelines","heading":"4.6 Special Operators","text":"section introduces special operators, might useful numerous applications.","code":""},{"path":"pipelines.html","id":"imputation-pipeopimpute","chapter":"4 Pipelines","heading":"4.6.1 Imputation: PipeOpImpute","text":"often occurring setting imputation missing data.\nImputation methods range relatively simple imputation using either mean, median histograms way involved methods including using machine learning algorithms order predict missing values.following PipeOps, PipeOpImpute:Impute numeric values histogramAdds new level factorsAdd column marking whether value given feature missing (numeric )use po(\"featureunion\") cbind missing indicator features.learner can thus equipped automatic imputation missing values adding imputation Pipeop.","code":"\npom = po(\"missind\")\npon = po(\"imputehist\", id = \"imputer_num\", affect_columns = is.numeric)\npof = po(\"imputeoor\", id = \"imputer_fct\", affect_columns = is.factor)\nimputer = pom %>>% pon %>>% pof\npolrn = po(\"learner\", lrn(\"classif.rpart\"))\nlrn = as_learner(imputer %>>% polrn)"},{"path":"pipelines.html","id":"feature-engineering-pipeopmutate","chapter":"4 Pipelines","heading":"4.6.2 Feature Engineering: PipeOpMutate","text":"New features can added computed task using PipeOpMutate .\noperator evaluates one multiple expressions provided alist.\nexample, compute new features top iris task.\nadd data illustrated :outside data required, can make use env parameter.\nMoreover, provide environment, expressions evaluated (env defaults .GlobalEnv).","code":"\npom = po(\"mutate\")\n\n# Define a set of mutations\nmutations = list(\n  Sepal.Sum = ~ Sepal.Length + Sepal.Width,\n  Petal.Sum = ~ Petal.Length + Petal.Width,\n  Sepal.Petal.Ratio = ~ (Sepal.Length / Petal.Length)\n)\npom$param_set$values$mutation = mutations"},{"path":"pipelines.html","id":"training-on-data-subsets-pipeopchunk","chapter":"4 Pipelines","heading":"4.6.3 Training on data subsets: PipeOpChunk","text":"cases, data big fit machine’s memory, often-used technique split data several parts.\nSubsequently, parts trained part data.undertaking steps, aggregate models.\nexample, split data 4 parts using PipeOpChunk .\nAdditionally, create 4 PipeOpLearner POS, trained split data.Afterwards can use PipeOpClassifAvg aggregate predictions 4 different models new one.can now connect different operators visualize full graph:","code":"\nchks = po(\"chunk\", 4)\nlrns = ppl(\"greplicate\", po(\"learner\", lrn(\"classif.rpart\")), 4)\nmjv = po(\"classifavg\", 4)\npipeline = chks %>>% lrns %>>% mjv\npipeline$plot(html = FALSE)\ntask = tsk(\"iris\")\ntrain.idx = sample(seq_len(task$nrow), 120)\ntest.idx = setdiff(seq_len(task$nrow), train.idx)\n\npipelrn = as_learner(pipeline)\npipelrn$train(task, train.idx)$\n  predict(task, train.idx)$\n  score()## classif.ce \n##    0.06667"},{"path":"pipelines.html","id":"feature-selection-pipeopfilter-and-pipeopselect","chapter":"4 Pipelines","heading":"4.6.4 Feature Selection: PipeOpFilter and PipeOpSelect","text":"package mlr3filters contains many different mlr3filters::Filters can used select features subsequent learners.\noften required data large amount features.PipeOp filters PipeOpFilter:many features keep can set using filter_nfeat, filter_frac filter_cutoff.Filters can selected / de-selected name using PipeOpSelect.","code":"\npo(\"filter\", mlr3filters::flt(\"information_gain\"))## PipeOp: <information_gain> (not trained)\n## values: <list()>\n## Input channels <name [train type, predict type]>:\n##   input [Task,Task]\n## Output channels <name [train type, predict type]>:\n##   output [Task,Task]"},{"path":"pipelines.html","id":"in-depth-pipelines","chapter":"4 Pipelines","heading":"4.7 In-depth look into mlr3pipelines","text":"vignette -depth introduction mlr3pipelines, dataflow programming toolkit machine learning R using mlr3.\ngo basic concepts give examples show simplicity well power versatility using mlr3pipelines.","code":""},{"path":"pipelines.html","id":"whats-the-point","chapter":"4 Pipelines","heading":"4.7.1 What’s the Point","text":"Machine learning toolkits often try abstract away processes happening inside machine learning algorithms.\nmakes easy user switch one algorithm another without worry happening inside , kind data able operate etc.\nbenefit using mlr3, example, one can create Learner, Task, Resampling etc. use typical machine learning operations.\ntrivial exchange individual components therefore use, example, different Learner experiment comparison.However, modularity breaks soon learning algorithm encompasses just model fitting, like data preprocessing, ensembles meta models.\nmlr3pipelines takes modularity one step mlr3: makes possible build individual steps within Learner building blocks called PipeOps.","code":"\ntask = as_task_classif(iris, target = \"Species\")\nlrn = lrn(\"classif.rpart\")\nrsmp = rsmp(\"holdout\")\nresample(task, lrn, rsmp)## <ResampleResult> of 1 iterations\n## * Task: iris\n## * Learner: classif.rpart\n## * Warnings: 0 in 0 iterations\n## * Errors: 0 in 0 iterations"},{"path":"pipelines.html","id":"pipeop-pipeline-operators","chapter":"4 Pipelines","heading":"4.7.2 PipeOp: Pipeline Operators","text":"basic unit functionality within mlr3pipelines PipeOp, short “pipeline operator”, represents trans-formative operation input (example training dataset) leading output.\ncan therefore seen generalized notion function, certain twist: PipeOps behave differently “training phase” “prediction phase”.\ntraining phase typically generate certain model data saved internal state.\nprediction phase operate input data depending trained model.example behavior principal component analysis operation (“PipeOpPCA”):\ntraining, transform incoming data rotating way leads uncorrelated features ordered contribution total variance.\nalso save rotation matrix used new data.\nmakes possible perform “prediction” single rows new data, row’s scores principal components (components training data!) computed.shows important primitives incorporated PipeOp:\n* $train(), taking list input arguments, turning list outputs, meanwhile saving state $state\n* $predict(), taking list input arguments, turning list outputs, making use saved $state\n* $state, “model” trained $train() utilized $predict().Schematically can represent PipeOp like :","code":"\npo = po(\"pca\")\npo$train(list(task))[[1]]$data()##        Species    PC1      PC2      PC3       PC4\n##   1:    setosa -2.684  0.31940 -0.02791 -0.002262\n##   2:    setosa -2.714 -0.17700 -0.21046 -0.099027\n##   3:    setosa -2.889 -0.14495  0.01790 -0.019968\n##   4:    setosa -2.745 -0.31830  0.03156  0.075576\n##   5:    setosa -2.729  0.32675  0.09008  0.061259\n##  ---                                             \n## 146: virginica  1.944  0.18753  0.17783 -0.426196\n## 147: virginica  1.527 -0.37532 -0.12190 -0.254367\n## 148: virginica  1.764  0.07886  0.13048 -0.137001\n## 149: virginica  1.901  0.11663  0.72325 -0.044595\n## 150: virginica  1.390 -0.28266  0.36291  0.155039\nsingle_line_task = task$clone()$filter(1)\npo$predict(list(single_line_task))[[1]]$data()##    Species    PC1    PC2      PC3       PC4\n## 1:  setosa -2.684 0.3194 -0.02791 -0.002262\npo$state## Standard deviations (1, .., p=4):\n## [1] 2.0563 0.4926 0.2797 0.1544\n## \n## Rotation (n x k) = (4 x 4):\n##                   PC1      PC2      PC3     PC4\n## Petal.Length  0.85667 -0.17337  0.07624  0.4798\n## Petal.Width   0.35829 -0.07548  0.54583 -0.7537\n## Sepal.Length  0.36139  0.65659 -0.58203 -0.3155\n## Sepal.Width  -0.08452  0.73016  0.59791  0.3197"},{"path":"pipelines.html","id":"why-the-state","chapter":"4 Pipelines","heading":"4.7.2.1 Why the $state","text":"important take moment notice importance $state variable $train() / $predict() dichotomy PipeOp.\nmany preprocessing methods, example scaling parameters imputation, theory just applied training data prediction / validation data separately, applied task resampling performed.\n, however, fallacious:preprocessing instance prediction data depend remaining prediction dataset.\nprediction single instance new data give result prediction performed whole dataset.preprocessing performed task resampling done, information test set can leak training set.\nResampling evaluate generalization performance entire machine learning method, therefore behavior entire method must depend content training split resampling.","code":""},{"path":"pipelines.html","id":"where-to-get-pipeops","chapter":"4 Pipelines","heading":"4.7.2.2 Where to Get PipeOps","text":"PipeOp instance “R6” class, many provided mlr3pipelines package .\ncan constructed explicitly (“PipeOpPCA$new()”) retrieved mlr_pipeops dictionary: po(\"pca\").\nentire list available PipeOps, meta-information, can retrieved using .data.table():retrieving PipeOps mlr_pipeops dictionary, also possible give additional constructor arguments, id parameter values.","code":"\nas.data.table(mlr_pipeops)[, c(\"key\", \"input.num\", \"output.num\")]##                       key input.num output.num\n##  1:                boxcox         1          1\n##  2:                branch         1         NA\n##  3:                 chunk         1         NA\n##  4:        classbalancing         1          1\n##  5:            classifavg        NA          1\n##  6:          classweights         1          1\n##  7:              colapply         1          1\n##  8:       collapsefactors         1          1\n##  9:              colroles         1          1\n## 10:                  copy         1         NA\n## 11:          datefeatures         1          1\n## 12:                encode         1          1\n## 13:          encodeimpact         1          1\n## 14:            encodelmer         1          1\n## 15:          featureunion        NA          1\n## 16:                filter         1          1\n## 17:            fixfactors         1          1\n## 18:               histbin         1          1\n## 19:                   ica         1          1\n## 20:        imputeconstant         1          1\n## 21:            imputehist         1          1\n## 22:         imputelearner         1          1\n## 23:            imputemean         1          1\n## 24:          imputemedian         1          1\n## 25:            imputemode         1          1\n## 26:             imputeoor         1          1\n## 27:          imputesample         1          1\n## 28:             kernelpca         1          1\n## 29:               learner         1          1\n## 30:            learner_cv         1          1\n## 31:               missind         1          1\n## 32:           modelmatrix         1          1\n## 33:     multiplicityexply         1         NA\n## 34:     multiplicityimply        NA          1\n## 35:                mutate         1          1\n## 36:                   nmf         1          1\n## 37:                   nop         1          1\n## 38:              ovrsplit         1          1\n## 39:              ovrunite         1          1\n## 40:                   pca         1          1\n## 41:                 proxy        NA          1\n## 42:           quantilebin         1          1\n## 43:      randomprojection         1          1\n## 44:        randomresponse         1          1\n## 45:               regravg        NA          1\n## 46:       removeconstants         1          1\n## 47:         renamecolumns         1          1\n## 48:             replicate         1          1\n## 49:                 scale         1          1\n## 50:           scalemaxabs         1          1\n## 51:            scalerange         1          1\n## 52:                select         1          1\n## 53:                 smote         1          1\n## 54:           spatialsign         1          1\n## 55:             subsample         1          1\n## 56:          targetinvert         2          1\n## 57:          targetmutate         1          2\n## 58: targettrafoscalerange         1          2\n## 59:        textvectorizer         1          1\n## 60:             threshold         1          1\n## 61:         tunethreshold         1          1\n## 62:              unbranch        NA          1\n## 63:                vtreat         1          1\n## 64:            yeojohnson         1          1\n##                       key input.num output.num\npo(\"pca\", rank. = 3)## PipeOp: <pca> (not trained)\n## values: <rank.=3>\n## Input channels <name [train type, predict type]>:\n##   input [Task,Task]\n## Output channels <name [train type, predict type]>:\n##   output [Task,Task]"},{"path":"pipelines.html","id":"pipeop-channels","chapter":"4 Pipelines","heading":"4.7.3 PipeOp Channels","text":"","code":""},{"path":"pipelines.html","id":"input-channels","chapter":"4 Pipelines","heading":"4.7.3.1 Input Channels","text":"Just like functions, PipeOps can take multiple inputs.\nmultiple inputs always given elements input list.\nexample, PipeOpFeatureUnion combines multiple tasks different features “cbind()s” together, creating one combined task.\ntwo halves iris task given, example, recreates original task:PipeOpFeatureUnion effectively takes two input arguments , can say two input channels.\ninput channel also carries information type input acceptable.\ninput channels pofu object constructed , example, accept Task training prediction.\ninformation can queried $input slot:PipeOps may channels take different types different phases.\nbackuplearner PipeOp, example, takes NULL Task training, Prediction Task prediction:","code":"\niris_first_half = task$clone()$select(c(\"Petal.Length\", \"Petal.Width\"))\niris_second_half = task$clone()$select(c(\"Sepal.Length\", \"Sepal.Width\"))\n\npofu = po(\"featureunion\", innum = 2)\n\npofu$train(list(iris_first_half, iris_second_half))[[1]]$data()##        Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n##   1:    setosa          1.4         0.2          5.1         3.5\n##   2:    setosa          1.4         0.2          4.9         3.0\n##   3:    setosa          1.3         0.2          4.7         3.2\n##   4:    setosa          1.5         0.2          4.6         3.1\n##   5:    setosa          1.4         0.2          5.0         3.6\n##  ---                                                            \n## 146: virginica          5.2         2.3          6.7         3.0\n## 147: virginica          5.0         1.9          6.3         2.5\n## 148: virginica          5.2         2.0          6.5         3.0\n## 149: virginica          5.4         2.3          6.2         3.4\n## 150: virginica          5.1         1.8          5.9         3.0\npofu$input##      name train predict\n## 1: input1  Task    Task\n## 2: input2  Task    Task\n## TODO this is an important case to handle here, do not delete unless there is a better example.\n## po(\"backuplearner\")$input"},{"path":"pipelines.html","id":"output-channels","chapter":"4 Pipelines","heading":"4.7.3.2 Output Channels","text":"Unlike typical notion function, PipeOps can also multiple output channels.\n$train() $predict() always return list, certain PipeOps may return lists one element.\nSimilar input channels, information number type outputs given PipeOp available $output slot.\nchunk PipeOp, example, chunks given Task subsets consequently returns multiple Task objects, training prediction.\nnumber output channels must given construction outnum argument.Note number output channels training prediction .\nschema PipeOp two output channels:","code":"\npo(\"chunk\", outnum = 3)$output##       name train predict\n## 1: output1  Task    Task\n## 2: output2  Task    Task\n## 3: output3  Task    Task"},{"path":"pipelines.html","id":"channel-configuration","chapter":"4 Pipelines","heading":"4.7.3.3 Channel Configuration","text":"PipeOps one input channel (take list single element), one;\nmany cases, number input output channels determined construction, e.g. innum / outnum arguments.\ninput.num output.num columns mlr_pipeops-table show default number channels, NA number depends construction argument.default printer PipeOp gives information channel names types:","code":"\n## po(\"backuplearner\")"},{"path":"pipelines.html","id":"graph-networks-of-pipeops","chapter":"4 Pipelines","heading":"4.7.4 Graph: Networks of PipeOps","text":"","code":""},{"path":"pipelines.html","id":"basics-1","chapter":"4 Pipelines","heading":"4.7.4.1 Basics","text":"advantage tedious way declaring input output channels handling /output lists?\nPipeOp known number input output channels always produce accept data known type, possible network together Graphs.\nGraph collection PipeOps “edges” mandate data flowing along .\nEdges always pass PipeOp channels, possible explicitly prescribe position input output list edge refers , makes possible make different components PipeOp’s output flow multiple different PipeOps, well PipeOp gather input multiple PipeOps.schema simple graph PipeOps:Graph empty first created, PipeOps can added using $add_pipeop() method.\n$add_edge() method used create connections .\nprinter Graph gives information layout, intuitive way visualizing using $plot() function.Graph $train() $predict() method accept data propagate data network PipeOps.\nreturn value corresponds output PipeOp output channels connected PipeOps.collection PipeOps inside Graph can accessed $pipeops slot.\nset edges Graph can inspected $edges slot.\npossible modify individual PipeOps edges Graph slots, recommended error checking performed may put Graph unsupported state.","code":"\ngr = Graph$new()\ngr$add_pipeop(po(\"scale\"))\ngr$add_pipeop(po(\"subsample\", frac = 0.1))\ngr$add_edge(\"scale\", \"subsample\")\nprint(gr)## Graph with 2 PipeOps:\n##         ID         State  sccssors prdcssors\n##      scale <<UNTRAINED>> subsample          \n##  subsample <<UNTRAINED>>               scale\ngr$plot(html = FALSE)\ngr$train(task)[[1]]$data()##        Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n##  1:     setosa      -1.3358  -1.3110521      -1.0184      1.2450\n##  2:     setosa      -1.1658  -1.0486668      -0.5354      1.9333\n##  3:     setosa      -1.3924  -1.0486668      -0.5354      1.9333\n##  4:     setosa      -1.2225  -1.0486668      -1.0184      0.7862\n##  5:     setosa      -1.3924  -1.3110521      -1.7430     -0.1315\n##  6: versicolor       0.4203   0.5256453       0.1892      0.7862\n##  7: versicolor       0.3637   0.0008746      -0.4146     -1.0493\n##  8: versicolor       0.4770   0.2632600       0.3100     -0.1315\n##  9:  virginica       1.2134   1.1816087       1.5176     -0.1315\n## 10:  virginica       1.1567   1.3128014       0.7930     -0.1315\n## 11:  virginica       1.6099   1.1816087       2.1214     -0.1315\n## 12:  virginica       1.4400   0.7880307       1.7591     -0.3610\n## 13:  virginica       1.1567   0.7880307       1.0345     -1.2787\n## 14:  virginica       0.8735   1.4439941       0.6722      0.3273\n## 15:  virginica       1.1001   1.4439941       1.2761      0.3273\ngr$predict(single_line_task)[[1]]$data()##    Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n## 1:  setosa       -1.336      -1.311      -0.8977       1.016"},{"path":"pipelines.html","id":"networks","chapter":"4 Pipelines","heading":"4.7.4.2 Networks","text":"example showed linear preprocessing pipeline, fact possible build true “graphs” operations, long loops introduced1.\nPipeOps multiple output channels can feed data multiple different subsequent PipeOps, PipeOps multiple input channels can take results different PipeOps.\nPipeOp one input / output channel, Graph’s $add_edge() method needs additional argument indicates channel connect .\nargument can given form integer, name channel.following constructs Graph copies input gives one copy “scale” “pca” PipeOp.\nresulting columns operation put next “featureunion”.","code":"\ngr = Graph$new()$\n  add_pipeop(po(\"copy\", outnum = 2))$\n  add_pipeop(po(\"scale\"))$\n  add_pipeop(po(\"pca\"))$\n  add_pipeop(po(\"featureunion\", innum = 2))\n\ngr$\n  add_edge(\"copy\", \"scale\", src_channel = 1)$        # designating channel by index\n  add_edge(\"copy\", \"pca\", src_channel = \"output2\")$  # designating channel by name\n  add_edge(\"scale\", \"featureunion\", dst_channel = 1)$\n  add_edge(\"pca\", \"featureunion\", dst_channel = 2)\n\ngr$plot(html = FALSE)\ngr$train(iris_first_half)[[1]]$data()##        Species Petal.Length Petal.Width    PC1       PC2\n##   1:    setosa      -1.3358     -1.3111 -2.561 -0.006922\n##   2:    setosa      -1.3358     -1.3111 -2.561 -0.006922\n##   3:    setosa      -1.3924     -1.3111 -2.653  0.031850\n##   4:    setosa      -1.2791     -1.3111 -2.469 -0.045694\n##   5:    setosa      -1.3358     -1.3111 -2.561 -0.006922\n##  ---                                                    \n## 146: virginica       0.8169      1.4440  1.756  0.455479\n## 147: virginica       0.7036      0.9192  1.417  0.164312\n## 148: virginica       0.8169      1.0504  1.640  0.178946\n## 149: virginica       0.9302      1.4440  1.940  0.377936\n## 150: virginica       0.7602      0.7880  1.470  0.033362"},{"path":"pipelines.html","id":"syntactic-sugar","chapter":"4 Pipelines","heading":"4.7.4.3 Syntactic Sugar","text":"Although possible create intricate Graphs edges going place (long loops introduced), usually clear direction flow “layers” Graph.\ntherefore convenient build Graph layers, can done using %>>% (“double-arrow”) operator.\ntakes either PipeOp Graph sides connects outputs left-hand side one inputs right-hand side–number inputs therefore must match number outputs.\nTogether gunion() operation, takes PipeOps Graphs arranges next akin (disjoint) graph union, network can easily constructed follows:","code":"\ngr = po(\"copy\", outnum = 2) %>>%\n  gunion(list(po(\"scale\"), po(\"pca\"))) %>>%\n  po(\"featureunion\", innum = 2)\n\ngr$plot(html = FALSE)"},{"path":"pipelines.html","id":"pipeop-ids-and-id-name-clashes","chapter":"4 Pipelines","heading":"4.7.4.4 PipeOp IDs and ID Name Clashes","text":"PipeOps within graph addressed $id-slot.\ntherefore necessary PipeOps within Graph unique $id.\n$id can set construction, directly changed PipeOp inserted Graph.\npoint, $set_names()-method can used change PipeOp ids.","code":"\npo1 = po(\"scale\")\npo2 = po(\"scale\")\npo1 %>>% po2  ## name clash## Error in gunion(list(g1, g2)): Assertion on 'ids of pipe operators' failed: Must have unique names, but element 2 is duplicated.\npo2$id = \"scale2\"\ngr = po1 %>>% po2\ngr## Graph with 2 PipeOps:\n##      ID         State sccssors prdcssors\n##   scale <<UNTRAINED>>   scale2          \n##  scale2 <<UNTRAINED>>              scale\n## Alternative ways of getting new ids:\npo(\"scale\", id = \"scale2\")## PipeOp: <scale2> (not trained)\n## values: <robust=FALSE>\n## Input channels <name [train type, predict type]>:\n##   input [Task,Task]\n## Output channels <name [train type, predict type]>:\n##   output [Task,Task]\npo(\"scale\", id = \"scale2\")## PipeOp: <scale2> (not trained)\n## values: <robust=FALSE>\n## Input channels <name [train type, predict type]>:\n##   input [Task,Task]\n## Output channels <name [train type, predict type]>:\n##   output [Task,Task]\n## sometimes names of PipeOps within a Graph need to be changed\ngr2 = po(\"scale\") %>>% po(\"pca\")\ngr %>>% gr2## Error in gunion(list(g1, g2)): Assertion on 'ids of pipe operators' failed: Must have unique names, but element 3 is duplicated.\ngr2$set_names(\"scale\", \"scale3\")\ngr %>>% gr2## Graph with 4 PipeOps:\n##      ID         State sccssors prdcssors\n##   scale <<UNTRAINED>>   scale2          \n##  scale2 <<UNTRAINED>>   scale3     scale\n##  scale3 <<UNTRAINED>>      pca    scale2\n##     pca <<UNTRAINED>>             scale3"},{"path":"pipelines.html","id":"learners-in-graphs-graphs-in-learners","chapter":"4 Pipelines","heading":"4.7.5 Learners in Graphs, Graphs in Learners","text":"true power mlr3pipelines derives fact can integrated seamlessly mlr3.\nTwo components mainly responsible :PipeOpLearner, PipeOp encapsulates mlr3 Learner creates PredictionData object $predict() phaseGraphLearner, mlr3 Learner can used place mlr3 Learner, prediction using Graph given itNote dual : One takes Learner produces PipeOp (extension Graph); takes Graph produces Learner.","code":""},{"path":"pipelines.html","id":"pipeoplearner","chapter":"4 Pipelines","heading":"4.7.5.1 PipeOpLearner","text":"PipeOpLearner constructed using mlr3 Learner use create PredictionData $predict() phase.\noutput $train() NULL.\ncan used preprocessing pipeline, even possible perform operations PredictionData, example averaging multiple predictions using “PipeOpBackupLearner” operator impute predictions given model failed create.following simple Graph performs training prediction data performing principal component analysis.","code":"\ngr = po(\"pca\") %>>% po(\"learner\",\n  lrn(\"classif.rpart\"))\ngr$train(task)## $classif.rpart.output\n## NULL\ngr$predict(task)## $classif.rpart.output\n## <PredictionClassif> for 150 observations:\n##     row_ids     truth  response\n##           1    setosa    setosa\n##           2    setosa    setosa\n##           3    setosa    setosa\n## ---                            \n##         148 virginica virginica\n##         149 virginica virginica\n##         150 virginica virginica"},{"path":"pipelines.html","id":"graphlearner","chapter":"4 Pipelines","heading":"4.7.5.2 GraphLearner","text":"Although Graph $train() $predict() functions, can used directly places mlr3 Learners can used like resampling benchmarks.\n, needs wrapped GraphLearner object, thin wrapper enables functionality.\nresulting Learner extremely versatile, every part can modified, replaced, parameterized optimized .\nResampling graph can done way resampling Learner performed introductory example.","code":"\nlrngrph = as_learner(gr)\nresample(task, lrngrph, rsmp)## <ResampleResult> of 1 iterations\n## * Task: iris\n## * Learner: pca.classif.rpart\n## * Warnings: 0 in 0 iterations\n## * Errors: 0 in 0 iterations"},{"path":"pipelines.html","id":"hyperparameters","chapter":"4 Pipelines","heading":"4.7.6 Hyperparameters","text":"mlr3pipelines relies paradox package provide parameters can modify PipeOp’s behavior.\nparadox parameters provide information parameters can changed, well types ranges.\nprovide unified interface benchmarks parameter optimization (“tuning”).\ndeep dive paradox, see tuning chapter -depth paradox chapter.ParamSet, representing space possible parameter configurations PipeOp, can inspected accessing $param_set slot PipeOp Graph.set retrieve parameter, $param_set$values slot can accessed.\nAlternatively, param_vals value can given construction.PipeOp can bring individual parameters collected together Graph’s $param_set.\nPipeOp’s parameter names prefixed $id prevent parameter name clashes.PipeOpLearner GraphLearner preserve parameters objects encapsulate.","code":"\nop_pca = po(\"pca\")\nop_pca$param_set## <ParamSet:pca>\n##                id    class lower upper nlevels       default value\n## 1:         center ParamLgl    NA    NA       2          TRUE      \n## 2:         scale. ParamLgl    NA    NA       2         FALSE      \n## 3:          rank. ParamInt     1   Inf     Inf                    \n## 4: affect_columns ParamUty    NA    NA     Inf <Selector[1]>\nop_pca$param_set$values$center = FALSE\nop_pca$param_set$values## $center\n## [1] FALSE\nop_pca = po(\"pca\", center = TRUE)\nop_pca$param_set$values## $center\n## [1] TRUE\ngr = op_pca %>>% po(\"scale\")\ngr$param_set## <ParamSetCollection>\n##                      id    class lower upper nlevels        default value\n## 1:           pca.center ParamLgl    NA    NA       2           TRUE  TRUE\n## 2:           pca.scale. ParamLgl    NA    NA       2          FALSE      \n## 3:            pca.rank. ParamInt     1   Inf     Inf                     \n## 4:   pca.affect_columns ParamUty    NA    NA     Inf  <Selector[1]>      \n## 5:         scale.center ParamLgl    NA    NA       2           TRUE      \n## 6:          scale.scale ParamLgl    NA    NA       2           TRUE      \n## 7:         scale.robust ParamLgl    NA    NA       2 <NoDefault[3]> FALSE\n## 8: scale.affect_columns ParamUty    NA    NA     Inf  <Selector[1]>\ngr$param_set$values## $pca.center\n## [1] TRUE\n## \n## $scale.robust\n## [1] FALSE\nop_rpart = po(\"learner\", lrn(\"classif.rpart\"))\nop_rpart$param_set## <ParamSet:classif.rpart>\n##                 id    class lower upper nlevels        default value\n##  1:             cp ParamDbl     0     1     Inf           0.01      \n##  2:     keep_model ParamLgl    NA    NA       2          FALSE      \n##  3:     maxcompete ParamInt     0   Inf     Inf              4      \n##  4:       maxdepth ParamInt     1    30      30             30      \n##  5:   maxsurrogate ParamInt     0   Inf     Inf              5      \n##  6:      minbucket ParamInt     1   Inf     Inf <NoDefault[3]>      \n##  7:       minsplit ParamInt     1   Inf     Inf             20      \n##  8: surrogatestyle ParamInt     0     1       2              0      \n##  9:   usesurrogate ParamInt     0     2       3              2      \n## 10:           xval ParamInt     0   Inf     Inf             10     0\nglrn = as_learner(gr %>>% op_rpart)\nglrn$param_set## <ParamSetCollection>\n##                               id    class lower upper nlevels        default\n##  1:                   pca.center ParamLgl    NA    NA       2           TRUE\n##  2:                   pca.scale. ParamLgl    NA    NA       2          FALSE\n##  3:                    pca.rank. ParamInt     1   Inf     Inf               \n##  4:           pca.affect_columns ParamUty    NA    NA     Inf  <Selector[1]>\n##  5:                 scale.center ParamLgl    NA    NA       2           TRUE\n##  6:                  scale.scale ParamLgl    NA    NA       2           TRUE\n##  7:                 scale.robust ParamLgl    NA    NA       2 <NoDefault[3]>\n##  8:         scale.affect_columns ParamUty    NA    NA     Inf  <Selector[1]>\n##  9:             classif.rpart.cp ParamDbl     0     1     Inf           0.01\n## 10:     classif.rpart.keep_model ParamLgl    NA    NA       2          FALSE\n## 11:     classif.rpart.maxcompete ParamInt     0   Inf     Inf              4\n## 12:       classif.rpart.maxdepth ParamInt     1    30      30             30\n## 13:   classif.rpart.maxsurrogate ParamInt     0   Inf     Inf              5\n## 14:      classif.rpart.minbucket ParamInt     1   Inf     Inf <NoDefault[3]>\n## 15:       classif.rpart.minsplit ParamInt     1   Inf     Inf             20\n## 16: classif.rpart.surrogatestyle ParamInt     0     1       2              0\n## 17:   classif.rpart.usesurrogate ParamInt     0     2       3              2\n## 18:           classif.rpart.xval ParamInt     0   Inf     Inf             10\n##     value\n##  1:  TRUE\n##  2:      \n##  3:      \n##  4:      \n##  5:      \n##  6:      \n##  7: FALSE\n##  8:      \n##  9:      \n## 10:      \n## 11:      \n## 12:      \n## 13:      \n## 14:      \n## 15:      \n## 16:      \n## 17:      \n## 18:     0"},{"path":"technical.html","id":"technical","chapter":"5 Technical","heading":"5 Technical","text":"chapter provides overview technical details mlr3 framework.ParallelizationAt first, details Parallelization usage future given.\nParallelization refers process running multiple jobs simultaneously.\nprocess employed minimize necessary computing power.\nAlgorithms consist sequential (non-parallelizable) parallelizable parts.\nTherefore, parallelization always alter performance positive substantial manner.\nSummed , sub-chapter illustrates use parallelization mlr3.Database BackendsThe section Database Backends describes work database backends mlr3 supports.\nDatabase backends can helpful large data processing fit memory stored natively database (e.g. SQLite).\nSpecifically working large data sets, undertaking numerous tasks simultaneously, can advantageous interface --memory data.\nsection provides illustration implement Database Backends using NYC flight data.ParametersIn section Parameters instructions given :define parameter sets learnersundertake parameter samplingapply parameter transformationsFor illustrative purposes, sub-chapter uses paradox package, successor ParamHelpers.Logging VerbosityThe sub-chapter Logging Verbosity shows change important settings related logging.\nmlr3 use lgr package.","code":""},{"path":"technical.html","id":"parallelization","chapter":"5 Technical","heading":"5.1 Parallelization","text":"Parallelization refers process running multiple jobs parallel, simultaneously.\nprocess allows significant savings computing power.\ndistinguish implicit parallelism explicit parallelism.","code":""},{"path":"technical.html","id":"implicit-parallelization","chapter":"5 Technical","heading":"5.1.1 Implicit Parallelization","text":"talk implicit parallelization context call external code (.e., code foreign CRAN packages) runs parallel.\nMany machine learning algorithms can parallelize model fit using threading, e.g. ranger\nxgboost.\nUnfortunately, threading conflicts certain parallel backends used explicit parallelization, causing system overutilized best case causing hangs segfaults worst case.\nreason, introduced convention implicit parallelization turned defaults, can enabled via hyperparameter tagged label \"threads\".enable parallelization learner, simply can call helper function set_threads()):also works filters mlr3filters lists objects, even objects support threading :","code":"\nlibrary(\"mlr3verse\")\n\nlearner = lrn(\"classif.ranger\")\nlearner$param_set$ids(tags = \"threads\")## [1] \"num.threads\"\n# set to use 4 CPUs\nset_threads(learner, n = 4)## <LearnerClassifRanger:classif.ranger>\n## * Model: -\n## * Parameters: num.threads=4\n## * Packages: ranger\n## * Predict Type: response\n## * Feature types: logical, integer, numeric, character, factor, ordered\n## * Properties: importance, multiclass, oob_error, twoclass, weights\n# auto-detect cores on the local machine\nset_threads(learner)## <LearnerClassifRanger:classif.ranger>\n## * Model: -\n## * Parameters: num.threads=2\n## * Packages: ranger\n## * Predict Type: response\n## * Feature types: logical, integer, numeric, character, factor, ordered\n## * Properties: importance, multiclass, oob_error, twoclass, weights\n# retrieve 2 filters\n# * variance filter with no support for threading\n# * mrmr filter with threading support\nfilters = flts(c(\"variance\", \"mrmr\"))\n\n# set threads for all filters which support it\nset_threads(filters, n = 4)## [[1]]\n## <FilterVariance:variance>\n## Task Types: classif, regr\n## Task Properties: -\n## Packages: stats\n## Feature types: integer, numeric\n## \n## [[2]]\n## <FilterMRMR:mrmr>\n## Task Types: classif, regr\n## Task Properties: -\n## Packages: praznik\n## Feature types: integer, numeric, factor, ordered\n# variance filter is unchanged\nfilters[[1]]$param_set## <ParamSet>\n##       id    class lower upper nlevels default value\n## 1: na.rm ParamLgl    NA    NA       2    TRUE\n# mrmr now works in parallel with 4 cores\nfilters[[2]]$param_set## <ParamSet>\n##         id    class lower upper nlevels default value\n## 1: threads ParamInt     0   Inf     Inf       0     4"},{"path":"technical.html","id":"explicit-parallelization","chapter":"5 Technical","heading":"5.1.2 Explicit Parallelization","text":"talk explicit parallelization mlr3 starts parallelization .\nabstraction implemented future used support broad range parallel backends.\ntwo use cases mlr3 calls future: resample() benchmark().\nresampling, resampling iterations can executed parallelization.\nholds benchmarking, additionally combinations provided design also independent.\nloops performed future using parallel backend configured future::plan().\nExtension packages like mlr3tuning internally call benchmark() tuning thus work parallel, .section, use spam task simple classification tree showcase explicit parallelization.\nexample, future::multisession parallel backend selected work systems.default, CPUs machine used unless specify argument workers future::plan().systems see decrease reported elapsed time, practice expect runtime fall linearly number cores increases (Amdahl’s law).\nDepending parallel backend, technical overhead starting workers, communicating objects, sending back results shutting workers can quite large.\nTherefore, advised enable parallelization resamplings iteration runs least seconds.transitioning mlr, might used selecting different parallelization levels, e.g. resampling, benchmarking tuning.\nmlr3 longer required (except nested resampling, briefly described following section).\nkind events rolled level.\nTherefore, need decide whether want parallelize tuning resampling.Just lean back let machine work :-)","code":"\n# select the multisession backend\nfuture::plan(\"multisession\")\n\ntask = tsk(\"spam\")\nlearner = lrn(\"classif.rpart\")\nresampling = rsmp(\"subsampling\")\n\ntime = Sys.time()\nresample(task, learner, resampling)\nSys.time() - time"},{"path":"technical.html","id":"nested-resampling-parallelization","chapter":"5 Technical","heading":"5.1.3 Nested Resampling Parallelization","text":"Nested resampling results two nested resampling loops.\ncan choose different parallelization backends inner outer resampling loop, respectively.\njust pass list future backends:nesting real parallelization backends often unintended causes unnecessary overhead, useful distributed computing setups.\ncan achieved future forcing fixed number workers loop:example run 8 cores (= 2 * 4) local machine.\nvignette future package gives insight nested parallelization.","code":"\n# Runs the outer loop in parallel and the inner loop sequentially\nfuture::plan(list(\"multisession\", \"sequential\"))\n# Runs the outer loop sequentially and the inner loop in parallel\nfuture::plan(list(\"sequential\", \"multisession\"))\n# Runs both loops in parallel\nfuture::plan(list(future::tweak(\"multisession\", workers = 2),\n                  future::tweak(\"multisession\", workers = 4)))"},{"path":"technical.html","id":"error-handling","chapter":"5 Technical","heading":"5.2 Error Handling","text":"demonstrate properly deal misbehaving learners, mlr3 ships learner classif.debug:learner comes special hyperparameters let us controlwhat conditions signaled (message, warning, error, segfault) probabilityduring stage conditions signaled (train predict)ratio predictions NA (predict_missing)learner’s default settings, learner nothing special: learner learns random label creates constant predictions.now set hyperparameter let debug learner signal error train step.\ndefault,mlr3 catch conditions warnings errors raised third-party code like learners:regular learner, now start debugging traceback() (create MRE file bug report).However, machine learning algorithms raising errors uncommon algorithms typically process possible data.\nThus, need mechanism tocapture signaled conditions messages, warnings errors can analyze post-hoc, anda statistically sound way proceed calculation able aggregate partial results.two mechanisms explained following subsections.","code":"\ntask = tsk(\"iris\")\nlearner = lrn(\"classif.debug\")\nprint(learner)## <LearnerClassifDebug:classif.debug>\n## * Model: -\n## * Parameters: list()\n## * Packages: -\n## * Predict Type: response\n## * Feature types: logical, integer, numeric, character, factor, ordered\n## * Properties: missings, multiclass, twoclass\nlearner$param_set## <ParamSet>\n##                       id    class lower upper nlevels        default value\n##  1:        error_predict ParamDbl     0     1     Inf              0      \n##  2:          error_train ParamDbl     0     1     Inf              0      \n##  3:      message_predict ParamDbl     0     1     Inf              0      \n##  4:        message_train ParamDbl     0     1     Inf              0      \n##  5:      predict_missing ParamDbl     0     1     Inf              0      \n##  6: predict_missing_type ParamFct    NA    NA       2             na      \n##  7:           save_tasks ParamLgl    NA    NA       2          FALSE      \n##  8:     segfault_predict ParamDbl     0     1     Inf              0      \n##  9:       segfault_train ParamDbl     0     1     Inf              0      \n## 10:              threads ParamInt     1   Inf     Inf <NoDefault[3]>      \n## 11:      warning_predict ParamDbl     0     1     Inf              0      \n## 12:        warning_train ParamDbl     0     1     Inf              0      \n## 13:                    x ParamDbl     0     1     Inf <NoDefault[3]>\ntask = tsk(\"iris\")\nlearner$train(task)$predict(task)$confusion##             truth\n## response     setosa versicolor virginica\n##   setosa          0          0         0\n##   versicolor      0          0         0\n##   virginica      50         50        50\nlearner$param_set$values = list(error_train = 1)\nlearner$train(tsk(\"iris\"))## Error in .__LearnerClassifDebug__.train(self = self, private = private, : Error from classif.debug->train()"},{"path":"technical.html","id":"encapsulation","chapter":"5 Technical","heading":"5.2.1 Encapsulation","text":"encapsulation, exceptions stop program flow output logged learner (instead printed console).\nLearner field encapsulate control train predict steps executed.\nOne way encapsulate execution provided package evaluate (see encapsulate() details):training learner, one can access recorded log via fields log, warnings errors:Another method encapsulation implemented callr package.\ncallr spawns new R process execute respective step, thus even guards current session segfaults.\ndownside, starting new processes comes computational overhead.Without model, possible get predictions though:handle missing predictions graceful way resample() benchmark(), fallback learners introduced next.","code":"\ntask = tsk(\"iris\")\nlearner = lrn(\"classif.debug\")\nlearner$param_set$values = list(warning_train = 1, error_train = 1)\nlearner$encapsulate = c(train = \"evaluate\", predict = \"evaluate\")\n\nlearner$train(task)\nlearner$log##    stage   class                                 msg\n## 1: train warning Warning from classif.debug->train()\n## 2: train   error   Error from classif.debug->train()\nlearner$warnings## [1] \"Warning from classif.debug->train()\"\nlearner$errors## [1] \"Error from classif.debug->train()\"\nlearner$encapsulate = c(train = \"callr\", predict = \"callr\")\nlearner$param_set$values = list(segfault_train = 1)\nlearner$train(task = task)\nlearner$errors## [1] \"callr process exited with status -11\"\nlearner$predict(task)## Error: Cannot predict, Learner 'classif.debug' has not been trained yet"},{"path":"technical.html","id":"fallback-learners","chapter":"5 Technical","heading":"5.2.2 Fallback learners","text":"Fallback learners purpose allow scoring results cases Learner misbehaving sense.\ntypical examples include:learner fails fit model training, e.g., convergence criterion met learner ran memory.learner fails predict observations.\ntypical case e.g. new factor levels test data.first handle common case learner completely breaks fitting model predicting new data.\nlearner fails either two steps, rely second learner generate predictions: fallback learner.next example, addition debug learner, attach simple featureless learner debug learner.\nwhenever debug learner fails (every time given parametrization) encapsulation enabled, mlr3 falls back predictions featureless learner internally:Note log contains captured error (also included print output), although don’t model, can still get predictions:fallback learner limited use stepwise train-predict procedure, invaluable larger benchmark studies resampling iterations failing.\n, need replace missing scores number order aggregate resampling iterations.\nimputing number equivalent guessing labels often seems right amount penalization.following snippet compare previously created debug learner simple classification tree.\nre-parametrize debug learner fail roughly 30% resampling iterations training step:investigate errors, can extract ResampleResult:similar yet different problem emerges learner predicts subset observations test set (predicts NA others).\nHandling predictions statistically sound way straight-forward common source -optimism reporting results.\nImagine goal benchmark two algorithms using 10-fold cross validation binary classification task:Algorithm ordinary logistic regression.Algorithm B also ordinary logistic regression, twist:\nlogistic regression rather certain predicted label (> 90% probability), returns label missing value otherwise.comparing performance two algorithms, obviously fair average predictions algorithm average “easy--predict” observations algorithm B.\n, algorithm B easily outperform algorithm , factored can generate predictions many observations.\nhand, also feasible exclude observations test set benchmark study least one algorithm failed predict label.\nInstead, proceed imputing missing predictions something naive, e.g., predicting majority class featureless learner.\nmajority class may depend resampling split (opt arbitrary baseline learner), best just train second learner resampling split.Long story short, fallback learner involved, missing predictions base learner automatically replaced predictions fallback learner.\nillustrated following example:Summed , combining encapsulation fallback learners, possible benchmark even quite unreliable instable learning algorithms convenient way.","code":"\ntask = tsk(\"iris\")\nlearner = lrn(\"classif.debug\")\nlearner$param_set$values = list(error_train = 1)\nlearner$encapsulate = c(train = \"evaluate\")\nlearner$fallback = lrn(\"classif.featureless\")\nlearner$train(task)\nlearner## <LearnerClassifDebug:classif.debug>\n## * Model: -\n## * Parameters: error_train=1\n## * Packages: -\n## * Predict Type: response\n## * Feature types: logical, integer, numeric, character, factor, ordered\n## * Properties: missings, multiclass, twoclass\n## * Errors: Error from classif.debug->train()\nlearner$model## NULL\nprediction = learner$predict(task)\nprediction$score()## classif.ce \n##     0.6667\nlearner$param_set$values = list(error_train = 0.3)\n\nbmr = benchmark(benchmark_grid(tsk(\"iris\"), list(learner, lrn(\"classif.rpart\")), rsmp(\"cv\")))\naggr = bmr$aggregate(conditions = TRUE)\naggr##    nr      resample_result task_id    learner_id resampling_id iters warnings\n## 1:  1 <ResampleResult[20]>    iris classif.debug            cv    10        0\n## 2:  2 <ResampleResult[20]>    iris classif.rpart            cv    10        0\n##    errors classif.ce\n## 1:      4     0.7267\n## 2:      0     0.0600\nrr = aggr[learner_id == \"classif.debug\"]$resample_result[[1L]]\nrr$errors##    iteration                               msg\n## 1:         1 Error from classif.debug->train()\n## 2:         2 Error from classif.debug->train()\n## 3:         3 Error from classif.debug->train()\n## 4:         9 Error from classif.debug->train()\ntask = tsk(\"iris\")\nlearner = lrn(\"classif.debug\")\n\n# this hyperparameter sets the ratio of missing predictions\nlearner$param_set$values = list(predict_missing = 0.5)\n\n# without fallback\np = learner$train(task)$predict(task)\ntable(p$response, useNA = \"always\")## \n##     setosa versicolor  virginica       <NA> \n##          0         75          0         75\n# with fallback\nlearner$fallback = lrn(\"classif.featureless\")\np = learner$train(task)$predict(task)\ntable(p$response, useNA = \"always\")## \n##     setosa versicolor  virginica       <NA> \n##          0        150          0          0"},{"path":"technical.html","id":"backends","chapter":"5 Technical","heading":"5.3 Database Backends","text":"mlr3, Tasks store data abstract data format, DataBackend.\ndefault backend uses data.table via DataBackendDataTable -memory data base.larger data, working many tasks parallel, can advantageous interface --memory data.\nuse excellent R package dbplyr extends dplyr work many popular data bases like MariaDB, PostgreSQL SQLite.","code":""},{"path":"technical.html","id":"use-case-nyc-flights","chapter":"5 Technical","heading":"5.3.1 Use Case: NYC Flights","text":"generate halfway realistic scenario, use NYC flights data set package nycflights13:","code":"\n# load data\nrequireNamespace(\"DBI\")## Loading required namespace: DBI\nrequireNamespace(\"RSQLite\")## Loading required namespace: RSQLite\nrequireNamespace(\"nycflights13\")## Loading required namespace: nycflights13\ndata(\"flights\", package = \"nycflights13\")\nstr(flights)## tibble [336,776 × 19] (S3: tbl_df/tbl/data.frame)\n##  $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n##  $ month         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ day           : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n##  $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n##  $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n##  $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n##  $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n##  $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n##  $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n##  $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n##  $ tailnum       : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n##  $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n##  $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n##  $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n##  $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n##  $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n##  $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n##  $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ...\n# add column of unique row ids\nflights$row_id = 1:nrow(flights)\n\n# create sqlite database in temporary file\npath = tempfile(\"flights\", fileext = \".sqlite\")\ncon = DBI::dbConnect(RSQLite::SQLite(), path)\ntbl = DBI::dbWriteTable(con, \"flights\", as.data.frame(flights))\nDBI::dbDisconnect(con)\n\n# remove in-memory data\nrm(flights)"},{"path":"technical.html","id":"preprocessing-with-dplyr","chapter":"5 Technical","heading":"5.3.2 Preprocessing with dplyr","text":"SQLite database path, now re-establish connection switch dplyr/dbplyr essential preprocessing.First, select subset columns work :Additionally, remove observations arrival delay (arr_delay) missing value:keep runtime reasonable toy example, filter data use every second row:factor levels feature carrier merged infrequent carriers replaced level “”:","code":"\n# establish connection\ncon = DBI::dbConnect(RSQLite::SQLite(), path)\n\n# select the \"flights\" table, enter dplyr\nlibrary(\"dplyr\")## \n## Attaching package: 'dplyr'## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\nlibrary(\"dbplyr\")## \n## Attaching package: 'dbplyr'## The following objects are masked from 'package:dplyr':\n## \n##     ident, sql\ntbl = tbl(con, \"flights\")\nkeep = c(\"row_id\", \"year\", \"month\", \"day\", \"hour\", \"minute\", \"dep_time\",\n  \"arr_time\", \"carrier\", \"flight\", \"air_time\", \"distance\", \"arr_delay\")\ntbl = select(tbl, keep)\ntbl = filter(tbl, !is.na(arr_delay))\ntbl = filter(tbl, row_id %% 2 == 0)\ntbl = mutate(tbl, carrier = case_when(\n    carrier %in% c(\"OO\", \"HA\", \"YV\", \"F9\", \"AS\", \"FL\", \"VX\", \"WN\") ~ \"other\",\n    TRUE ~ carrier)\n)"},{"path":"technical.html","id":"databackenddplyr","chapter":"5 Technical","heading":"5.3.3 DataBackendDplyr","text":"processed table now used create mlr3db::DataBackendDplyr mlr3db:can now use interface DataBackend query basic information data:Note DataBackendDplyr know rows columns filtered dplyr , just operates view provided.","code":"\nlibrary(\"mlr3db\")\nb = as_data_backend(tbl, primary_key = \"row_id\")\nb$nrow## [1] 163707\nb$ncol## [1] 13\nb$head()##    row_id year month day hour minute dep_time arr_time carrier flight air_time\n## 1:      2 2013     1   1    5     29      533      850      UA   1714      227\n## 2:      4 2013     1   1    5     45      544     1004      B6    725      183\n## 3:      6 2013     1   1    5     58      554      740      UA   1696      150\n## 4:      8 2013     1   1    6      0      557      709      EV   5708       53\n## 5:     10 2013     1   1    6      0      558      753      AA    301      138\n## 6:     12 2013     1   1    6      0      558      853      B6     71      158\n##    distance arr_delay\n## 1:     1416        20\n## 2:     1576       -18\n## 3:      719        12\n## 4:      229       -14\n## 5:      733         8\n## 6:     1005        -3"},{"path":"technical.html","id":"model-fitting","chapter":"5 Technical","heading":"5.3.4 Model fitting","text":"create following mlr3 objects:regression task, based previously created mlr3db::DataBackendDplyr.regression learner (regr.rpart).resampling strategy: 3 times repeated subsampling using 2% observations training (“subsampling”)Measures “mse”, “time_train” “time_predict”pass objects resample() perform simple resampling three iterations.\niteration, required subset data queried SQLite data base passed rpart::rpart():","code":"\ntask = as_task_regr(b, id = \"flights_sqlite\", target = \"arr_delay\")\nlearner = lrn(\"regr.rpart\")\nmeasures = mlr_measures$mget(c(\"regr.mse\", \"time_train\", \"time_predict\"))\nresampling = rsmp(\"subsampling\")\nresampling$param_set$values = list(repeats = 3, ratio = 0.02)\nrr = resample(task, learner, resampling)\nprint(rr)## <ResampleResult> of 3 iterations\n## * Task: flights_sqlite\n## * Learner: regr.rpart\n## * Warnings: 0 in 0 iterations\n## * Errors: 0 in 0 iterations\nrr$aggregate(measures)##     regr.mse   time_train time_predict \n##         1183            0            0"},{"path":"technical.html","id":"cleanup","chapter":"5 Technical","heading":"5.3.5 Cleanup","text":"Finally, remove tbl object close connection.","code":"\nrm(tbl)\nDBI::dbDisconnect(con)"},{"path":"technical.html","id":"paradox","chapter":"5 Technical","heading":"5.4 Parameters (using paradox)","text":"paradox package offers language description parameter spaces, well tools useful operations parameter spaces.\nparameter space often useful describing:set sensible input values R functionThe set possible values slots configuration object can takeThe search space optimization processThe tools provided paradox therefore relate :Parameter checking: Verifying set parameters satisfies conditions parameter spaceParameter sampling: Generating parameter values lie parameter space systematic exploration program behavior depending parametersparadox , nature, auxiliary package derives usefulness packages make use .\nheavily utilized mlr-org packages mlr3, mlr3pipelines, mlr3tuning.","code":""},{"path":"technical.html","id":"reference-based-objects","chapter":"5 Technical","heading":"5.4.1 Reference Based Objects","text":"paradox spiritual successor ParamHelpers package written scratch using R6 class system.\nimportant consequence objects created paradox “reference-based”, unlike objects R.\nchange made ParamSet object, example adding parameter using $add() function, variables point ParamSet contain changed object.\ncreate independent copy ParamSet, $clone() method needs used:","code":"\nlibrary(\"paradox\")\n\nps = ParamSet$new()\nps2 = ps\nps3 = ps$clone(deep = TRUE)\nprint(ps) # the same for ps2 and ps3## <ParamSet>\n## Empty.\nps$add(ParamLgl$new(\"a\"))\nprint(ps)  # ps was changed## <ParamSet>\n##    id    class lower upper nlevels        default value\n## 1:  a ParamLgl    NA    NA       2 <NoDefault[3]>\nprint(ps2) # contains the same reference as ps## <ParamSet>\n##    id    class lower upper nlevels        default value\n## 1:  a ParamLgl    NA    NA       2 <NoDefault[3]>\nprint(ps3) # is a \"clone\" of the old (empty) ps## <ParamSet>\n## Empty."},{"path":"technical.html","id":"defining-a-parameter-space","chapter":"5 Technical","heading":"5.4.2 Defining a Parameter Space","text":"","code":""},{"path":"technical.html","id":"single-parameters","chapter":"5 Technical","heading":"5.4.2.1 Single Parameters","text":"basic building block describing parameter spaces Param class.\nrepresents single parameter, usually can take single atomic value.\nConsider, example, trying configure rpart package’s rpart.control object.\nvarious components (minsplit, cp, …) take single value, represented different instance Param object.Param class various sub-classes represent different value types:ParamInt: Integer numbersParamDbl: Real numbersParamFct: String values set possible values, similar R factorsParamLgl: Truth values (TRUE / FALSE), logicals RParamUty: Parameter can take valueA particular instance parameter created calling attached $new() function:Every parameter must :id - name parameter within parameter setdefault - default valuespecial_vals - list values accepted even conform typetags - Tags can used organize parametersThe numeric (Int Dbl) parameters furthermore allow specification lower upper bound.\nMeanwhile, Fct parameter must given vector levels define possible states parameter can take.\nUty parameter can also custom_check function must return TRUE value acceptable may return character(1) error description otherwise.\nexample defines parE parameter accepts functions.values given constructor accessible object inspection using $.\nAlthough values can changed parameter construction, can bad idea avoided possible.Instead, new parameter constructed.\nBesides possible values can given constructor, also $class, $nlevels, $is_bounded, $has_default, $storage_type, $is_number $is_categ slots give information parameter.list slots can found ?Param.also possible get information Param data.table calling .data.table.","code":"\nlibrary(\"paradox\")\nparA = ParamLgl$new(id = \"A\")\nparB = ParamInt$new(id = \"B\", lower = 0, upper = 10, tags = c(\"tag1\", \"tag2\"))\nparC = ParamDbl$new(id = \"C\", lower = 0, upper = 4, special_vals = list(NULL))\nparD = ParamFct$new(id = \"D\", levels = c(\"x\", \"y\", \"z\"), default = \"y\")\nparE = ParamUty$new(id = \"E\", custom_check = function(x) checkmate::checkFunction(x))\nparB$lower## [1] 0\nparA$levels## [1]  TRUE FALSE\nparE$class## [1] \"ParamUty\"\nas.data.table(parA)##    id    class lower upper      levels nlevels is_bounded special_vals        default storage_type tags\n## 1:  A ParamLgl    NA    NA  TRUE,FALSE       2       TRUE    <list[0]> <NoDefault[3]>      logical"},{"path":"technical.html","id":"type-range-checking","chapter":"5 Technical","heading":"5.4.2.1.1 Type / Range Checking","text":"Param object offers possibility check whether value satisfies condition, .e. right type, also falls within range allowed values, using $test(), $check(), $assert() functions.\ntest() used within conditional checks returns TRUE FALSE, check() returns error description value conform parameter (thus plays well checkmate::assert() function).\nassert() throw error whenever value fit.Instead testing single parameters, often convenient check whole set parameters using ParamSet.","code":"\nparA$test(FALSE)## [1] TRUE\nparA$test(\"FALSE\")## [1] FALSE\nparA$check(\"FALSE\")## [1] \"Must be of type 'logical flag', not 'character'\""},{"path":"technical.html","id":"parameter-sets","chapter":"5 Technical","heading":"5.4.2.2 Parameter Sets","text":"ordered collection parameters handled ParamSet2.\ninitialized using $new() function optionally takes list Params argument.\nParameters can also added constructed ParamSet using $add() function.\neven possible add whole ParamSets ParamSets.individual parameters can accessed $params slot.\nalso possible get information parameters vectorized fashion using mostly slots individual Params (.e. $class, $levels etc.), see ?ParamSet details.possible reduce ParamSets using $subset method.\naware modifies ParamSet -place, “clone” must created first original ParamSet modified.Just Params, much useful, possible get ParamSet data.table using .data.table().\nmakes easy subset parameters certain conditions aggregate information , using variety methods provided data.table.","code":"\nps = ParamSet$new(list(parA, parB))\nps$add(parC)\nps$add(ParamSet$new(list(parD, parE)))\nprint(ps)## <ParamSet>\n##    id    class lower upper nlevels        default value\n## 1:  A ParamLgl    NA    NA       2 <NoDefault[3]>      \n## 2:  B ParamInt     0    10      11 <NoDefault[3]>      \n## 3:  C ParamDbl     0     4     Inf <NoDefault[3]>      \n## 4:  D ParamFct    NA    NA       3              y      \n## 5:  E ParamUty    NA    NA     Inf <NoDefault[3]>\npsSmall = ps$clone()\npsSmall$subset(c(\"A\", \"B\", \"C\"))\nprint(psSmall)## <ParamSet>\n##    id    class lower upper nlevels        default value\n## 1:  A ParamLgl    NA    NA       2 <NoDefault[3]>      \n## 2:  B ParamInt     0    10      11 <NoDefault[3]>      \n## 3:  C ParamDbl     0     4     Inf <NoDefault[3]>\nas.data.table(ps)##    id    class lower upper      levels nlevels is_bounded special_vals        default storage_type      tags\n## 1:  A ParamLgl    NA    NA  TRUE,FALSE       2       TRUE    <list[0]> <NoDefault[3]>      logical          \n## 2:  B ParamInt     0    10                  11       TRUE    <list[0]> <NoDefault[3]>      integer tag1,tag2\n## 3:  C ParamDbl     0     4                 Inf       TRUE    <list[1]> <NoDefault[3]>      numeric          \n## 4:  D ParamFct    NA    NA       x,y,z       3       TRUE    <list[0]>              y    character          \n## 5:  E ParamUty    NA    NA                 Inf      FALSE    <list[0]> <NoDefault[3]>         list"},{"path":"technical.html","id":"type-range-checking-1","chapter":"5 Technical","heading":"5.4.2.2.1 Type / Range Checking","text":"Similar individual Params, ParamSet provides $test(), $check() $assert() functions allow type range checking parameters.\nargument must named list values checked respective parameters.\npossible check subset parameters.","code":"\nps$check(list(A = TRUE, B = 0, E = identity))## [1] TRUE\nps$check(list(A = 1))## [1] \"A: Must be of type 'logical flag', not 'double'\"\nps$check(list(Z = 1))## [1] \"Parameter 'Z' not available. Did you mean 'A' / 'B' / 'C'?\""},{"path":"technical.html","id":"values-in-a-paramset","chapter":"5 Technical","heading":"5.4.2.2.2 Values in a ParamSet","text":"Although ParamSet fundamentally represents value space, also slot $values can contain point within space.\nuseful many things define parameter space need similar operations (like parameter checking) can simplified.\n$values slot contains named list always checked parameter constraints.\ntrying set parameter values, e.g. mlr3 Learners, $values slot $param_set needs used.parameter constraints automatically checked:","code":"\nps$values = list(A = TRUE, B = 0)\nps$values$B = 1\nprint(ps$values)## $A\n## [1] TRUE\n## \n## $B\n## [1] 1\nps$values$B = 100## Error in self$assert(xs): Assertion on 'xs' failed: B: Element 1 is not <= 10."},{"path":"technical.html","id":"dependencies","chapter":"5 Technical","heading":"5.4.2.2.3 Dependencies","text":"often case certain parameters irrelevant given depending values parameters.\nexample parameter switches certain algorithm feature (example regularization) , combined another parameter controls behavior feature (e.g. regularization parameter).\nsecond parameter said depend first parameter value TRUE.dependency can added using $add_dep method, takes ids “depender” “dependee” parameters well Condition object.\nCondition object represents check performed “dependee”.\nCurrently can created using CondEqual$new() CondAnyOf$new().\nMultiple dependencies can added, parameters depend others can depended , long cyclic dependencies introduced.consequence dependencies twofold:\none, $check(), $test() $assert() tests accept presence parameter dependency met.\nFurthermore, sampling creating grid designs ParamSet, dependencies respected (see Parameter Sampling, particular Hierarchical Sampler).following example makes parameter D depend parameter FALSE, parameter B depend parameter D one \"x\" \"y\".\nintroduces implicit dependency B FALSE well, D take value TRUE.Internally, dependencies represented data.table, can accessed listed $deps slot.\ndata.table can even mutated, e.g. remove dependencies.\nsanity checks done $deps slot changed way.\nTherefore advised cautious.","code":"\nps$add_dep(\"D\", \"A\", CondEqual$new(FALSE))\nps$add_dep(\"B\", \"D\", CondAnyOf$new(c(\"x\", \"y\")))\nps$check(list(A = FALSE, D = \"x\", B = 1))          # OK: all dependencies met## [1] TRUE\nps$check(list(A = FALSE, D = \"z\", B = 1))          # B's dependency is not met## [1] \"The parameter 'B' can only be set if the following condition is met 'D ∈ {x, y}'. Instead the current parameter value is: D=z\"\nps$check(list(A = FALSE, B = 1))                   # B's dependency is not met## [1] \"The parameter 'B' can only be set if the following condition is met 'D ∈ {x, y}'. Instead the parameter value for 'D' is not set at all. Try setting 'D' to a value that satisfies the condition\"\nps$check(list(A = FALSE, D = \"z\"))                 # OK: B is absent## [1] TRUE\nps$check(list(A = TRUE))                           # OK: neither B nor D present## [1] TRUE\nps$check(list(A = TRUE, D = \"x\", B = 1))           # D's dependency is not met## [1] \"The parameter 'D' can only be set if the following condition is met 'A = FALSE'. Instead the current parameter value is: A=TRUE\"\nps$check(list(A = TRUE, B = 1))                    # B's dependency is not met## [1] \"The parameter 'B' can only be set if the following condition is met 'D ∈ {x, y}'. Instead the parameter value for 'D' is not set at all. Try setting 'D' to a value that satisfies the condition\"\nps$deps##    id on           cond\n## 1:  D  A <CondEqual[9]>\n## 2:  B  D <CondAnyOf[9]>"},{"path":"technical.html","id":"vector-parameters","chapter":"5 Technical","heading":"5.4.2.3 Vector Parameters","text":"Unlike old ParamHelpers package, vectorial parameters paradox.\nInstead, now possible create multiple copies single parameter using $rep function.\ncreates ParamSet consisting multiple copies parameter, can (optionally) added another ParamSet.also possible use ParamUty accept vectorial parameters, also works parameters variable length.\nParamSet containing ParamUty can used parameter checking, sampling.\nsample values method needs vectorial parameter, advised use parameter transformation function creates vector atomic values.Assembling vector repeated parameters aided parameter’s $tags: Parameters generated $rep() command automatically get tagged belonging group repeated parameters.","code":"\nps2d = ParamDbl$new(\"x\", lower = 0, upper = 1)$rep(2)\nprint(ps2d)## <ParamSet>\n##         id    class lower upper nlevels        default value\n## 1: x_rep_1 ParamDbl     0     1     Inf <NoDefault[3]>      \n## 2: x_rep_2 ParamDbl     0     1     Inf <NoDefault[3]>\nps$add(ps2d)\nprint(ps)## <ParamSet>\n##         id    class lower upper nlevels        default parents value\n## 1:       A ParamLgl    NA    NA       2 <NoDefault[3]>          TRUE\n## 2:       B ParamInt     0    10      11 <NoDefault[3]>       D     1\n## 3:       C ParamDbl     0     4     Inf <NoDefault[3]>              \n## 4:       D ParamFct    NA    NA       3              y       A      \n## 5:       E ParamUty    NA    NA     Inf <NoDefault[3]>              \n## 6: x_rep_1 ParamDbl     0     1     Inf <NoDefault[3]>              \n## 7: x_rep_2 ParamDbl     0     1     Inf <NoDefault[3]>\nps$tags## $A\n## character(0)\n## \n## $B\n## [1] \"tag1\" \"tag2\"\n## \n## $C\n## character(0)\n## \n## $D\n## character(0)\n## \n## $E\n## character(0)\n## \n## $x_rep_1\n## [1] \"x_rep\"\n## \n## $x_rep_2\n## [1] \"x_rep\""},{"path":"technical.html","id":"parameter-sampling","chapter":"5 Technical","heading":"5.4.3 Parameter Sampling","text":"often useful list possible parameter values can systematically iterated , example find parameter values algorithm performs particularly well (tuning).\nparadox offers variety functions allow creating evenly-spaced parameter values “grid” design well random sampling.\nlatter case, possible influence sampling distribution less fine detail.point always keep mind sampling numerical factorial parameters bounded can sampled , .e. ParamUty.\nFurthermore, samplers ParamInt ParamDbl must finite lower upper bounds.","code":""},{"path":"technical.html","id":"parameter-designs","chapter":"5 Technical","heading":"5.4.3.1 Parameter Designs","text":"Functions sample parameter space fundamentally return object Design class.\nobjects contain sampled data data.table $data slot, also offer conversion list parameter-values using $transpose() function.","code":""},{"path":"technical.html","id":"grid-design","chapter":"5 Technical","heading":"5.4.3.2 Grid Design","text":"generate_design_grid() function used create grid designs contain combinations parameter values: possible values ParamLgl ParamFct, values given resolution ParamInt ParamDbl.\nresolution can given numeric parameters, specific named parameters param_resolutions parameter.","code":"\ndesign = generate_design_grid(psSmall, 2)\nprint(design)## <Design> with 8 rows:\n##        A  B C\n## 1:  TRUE  0 0\n## 2:  TRUE  0 4\n## 3:  TRUE 10 0\n## 4:  TRUE 10 4\n## 5: FALSE  0 0\n## 6: FALSE  0 4\n## 7: FALSE 10 0\n## 8: FALSE 10 4\ngenerate_design_grid(psSmall, param_resolutions = c(B = 1, C = 2))## <Design> with 4 rows:\n##    B C     A\n## 1: 0 0  TRUE\n## 2: 0 0 FALSE\n## 3: 0 4  TRUE\n## 4: 0 4 FALSE"},{"path":"technical.html","id":"random-sampling","chapter":"5 Technical","heading":"5.4.3.3 Random Sampling","text":"paradox offers different methods random sampling, vary degree can configured.\neasiest way get uniformly random sample parameters generate_design_random.\nalso possible create “latin hypercube” sampled parameter values using generate_design_lhs, utilizes lhs package.\nLHS-sampling creates low-discrepancy sampled values cover parameter space evenly purely random values.","code":"\npvrand = generate_design_random(ps2d, 500)\npvlhs = generate_design_lhs(ps2d, 500)"},{"path":"technical.html","id":"generalized-sampling-the-sampler-class","chapter":"5 Technical","heading":"5.4.3.4 Generalized Sampling: The Sampler Class","text":"may sometimes desirable configure parameter sampling detail.\nparadox uses Sampler abstract base class sampling, many different sub-classes can parameterized combined control sampling process.\neven possible create sub-classes Sampler class (subclasses) even possibilities.Every Sampler object sample() function, takes one argument, number instances sample, returns Design object.","code":""},{"path":"technical.html","id":"d-samplers","chapter":"5 Technical","heading":"5.4.3.4.1 1D-Samplers","text":"variety samplers sample values single parameter.\nSampler1DUnif (uniform sampling), Sampler1DCateg (sampling categorical parameters), Sampler1DNormal (normally distributed sampling, truncated parameter bounds), Sampler1DRfun (arbitrary 1D sampling, given random-function).\ninitialized single Param, can used sample values.","code":"\nsampA = Sampler1DCateg$new(parA)\nsampA$sample(5)## <Design> with 5 rows:\n##        A\n## 1:  TRUE\n## 2:  TRUE\n## 3:  TRUE\n## 4: FALSE\n## 5:  TRUE"},{"path":"technical.html","id":"hierarchical-sampler","chapter":"5 Technical","heading":"5.4.3.4.2 Hierarchical Sampler","text":"SamplerHierarchical sampler auxiliary sampler combines many 1D-Samplers get combined distribution.\nname “hierarchical” implies able respect parameter dependencies.\nsuggests parameters get sampled dependencies met.following example shows works: Int parameter B depends Lgl parameter TRUE.\nsampled TRUE half cases, case B takes value 0 10.\ncases FALSE, B set NA.","code":"\npsSmall$add_dep(\"B\", \"A\", CondEqual$new(TRUE))\nsampH = SamplerHierarchical$new(psSmall,\n  list(Sampler1DCateg$new(parA),\n    Sampler1DUnif$new(parB),\n    Sampler1DUnif$new(parC))\n)\nsampled = sampH$sample(1000)\ntable(sampled$data[, c(\"A\", \"B\")], useNA = \"ifany\")##        B\n## A         0   1   2   3   4   5   6   7   8   9  10 <NA>\n##   FALSE   0   0   0   0   0   0   0   0   0   0   0  484\n##   TRUE   47  57  46  40  47  56  49  49  41  44  40    0"},{"path":"technical.html","id":"joint-sampler","chapter":"5 Technical","heading":"5.4.3.4.3 Joint Sampler","text":"Another way combining samplers SamplerJointIndep.\nSamplerJointIndep also makes possible combine Samplers 1D.\nHowever, SamplerJointIndep currently can handle ParamSets dependencies.","code":"\nsampJ = SamplerJointIndep$new(\n  list(Sampler1DUnif$new(ParamDbl$new(\"x\", 0, 1)),\n    Sampler1DUnif$new(ParamDbl$new(\"y\", 0, 1)))\n)\nsampJ$sample(5)## <Design> with 5 rows:\n##         x      y\n## 1: 0.3478 0.4639\n## 2: 0.2304 0.2626\n## 3: 0.2111 0.4474\n## 4: 0.3319 0.3231\n## 5: 0.2220 0.5235"},{"path":"technical.html","id":"samplerunif","chapter":"5 Technical","heading":"5.4.3.4.4 SamplerUnif","text":"Sampler used generate_design_random SamplerUnif sampler, corresponds HierarchicalSampler Sampler1DUnif parameters.","code":""},{"path":"technical.html","id":"parameter-transformation","chapter":"5 Technical","heading":"5.4.4 Parameter Transformation","text":"different Samplers allow wide specification parameter distributions, cases simplest way getting desired distribution sample parameters simple distribution (uniform distribution) transform .\ncan done assigning function $trafo slot ParamSet.\n$trafo function called two parameters:list parameter values transformed xThe ParamSet param_setThe $trafo function must return list transformed parameter values.transformation performed calling $transpose function Design object returned Sampler trafo ParamSet TRUE (default).\nfollowing, example, creates parameter exponentially distributed:Compare $transpose() without transformation:","code":"\npsexp = ParamSet$new(list(ParamDbl$new(\"par\", 0, 1)))\npsexp$trafo = function(x, param_set) {\n  x$par = -log(x$par)\n  x\n}\ndesign = generate_design_random(psexp, 2)\nprint(design)## <Design> with 2 rows:\n##       par\n## 1: 0.8855\n## 2: 0.4997\ndesign$transpose()  # trafo is TRUE## [[1]]\n## [[1]]$par\n## [1] 0.1216\n## \n## \n## [[2]]\n## [[2]]$par\n## [1] 0.6937\ndesign$transpose(trafo = FALSE)## [[1]]\n## [[1]]$par\n## [1] 0.8855\n## \n## \n## [[2]]\n## [[2]]$par\n## [1] 0.4997"},{"path":"technical.html","id":"transformation-between-types","chapter":"5 Technical","heading":"5.4.4.1 Transformation between Types","text":"Usually design created one ParamSet used configure objects ParamSet defines values take.\nParamSets can used random sampling, however, restricted ways:\nmust finite bounds, may contain “untyped” (ParamUty) parameters.\n$trafo provides glue situations.\nrelatively little constraint trafo function’s return value, possible return values different bounds even types original ParamSet.\neven possible remove parameters add new ones.Suppose, example, certain method requires function parameter.\nLet’s say function summarizes data certain way.\nuser can pass functions like median() mean(), also pass quantiles something completely different.\nmethod probably use following ParamSet:one wanted sample method, using one four functions, way :Note Design contains column “fun” character column.\nget single value function, $transpose function used.can now check fits requirements set methodPS, fun fact function:Imagine now different kind parametrization function desired:\nuser wants give function selects certain quantile, quantile set parameter.\ncase $transpose function generate function different way.\ninterpretability, parameter called “quantile” transformation, “fun” parameter generated fly.Design now contains column “quantile” used $transpose function create fun parameter.\nalso check fits requirement set methodPS, function.","code":"\nmethodPS = ParamSet$new(\n  list(\n    ParamUty$new(\"fun\",\n      custom_check = function(x) checkmate::checkFunction(x, nargs = 1))\n  )\n)\nprint(methodPS)## <ParamSet>\n##     id    class lower upper nlevels        default value\n## 1: fun ParamUty    NA    NA     Inf <NoDefault[3]>\nsamplingPS = ParamSet$new(\n  list(\n    ParamFct$new(\"fun\", c(\"mean\", \"median\", \"min\", \"max\"))\n  )\n)\n\nsamplingPS$trafo = function(x, param_set) {\n  # x$fun is a `character(1)`,\n  # in particular one of 'mean', 'median', 'min', 'max'.\n  # We want to turn it into a function!\n  x$fun = get(x$fun, mode = \"function\")\n  x\n}\ndesign = generate_design_random(samplingPS, 2)\nprint(design)## <Design> with 2 rows:\n##       fun\n## 1: median\n## 2:    min\nxvals = design$transpose()\nprint(xvals[[1]])## $fun\n## function (x, na.rm = FALSE, ...) \n## UseMethod(\"median\")\n## <bytecode: 0x556950793c50>\n## <environment: namespace:stats>\nmethodPS$check(xvals[[1]])## [1] \"fun: Must have exactly 1 formal arguments, but has 2\"\nxvals[[1]]$fun(1:10)## [1] 5.5\nsamplingPS2 = ParamSet$new(\n  list(\n    ParamDbl$new(\"quantile\", 0, 1)\n  )\n)\n\nsamplingPS2$trafo = function(x, param_set) {\n  # x$quantile is a `numeric(1)` between 0 and 1.\n  # We want to turn it into a function!\n  list(fun = function(input) quantile(input, x$quantile))\n}\ndesign = generate_design_random(samplingPS2, 2)\nprint(design)## <Design> with 2 rows:\n##    quantile\n## 1:   0.8254\n## 2:   0.9866\nxvals = design$transpose()\nprint(xvals[[1]])## $fun\n## function(input) quantile(input, x$quantile)\n## <environment: 0x55694fcb6058>\nmethodPS$check(xvals[[1]])## [1] TRUE\nxvals[[1]]$fun(1:10)## 82.53615% \n##     8.428"},{"path":"technical.html","id":"logging","chapter":"5 Technical","heading":"5.5 Logging","text":"use lgr package logging progress output.","code":""},{"path":"technical.html","id":"changing-mlr3-logging-levels","chapter":"5 Technical","heading":"5.5.1 Changing mlr3 logging levels","text":"change setting mlr3 current session, need retrieve logger (R6 object) lgr, change threshold like :default log level \"info\".\navailable levels can listed follows:increase verbosity, set log level higher value, e.g. \"debug\" :reduce verbosity, reduce log level warn:lgr comes global option called \"lgr.default_threshold\" can set via options() make choice permanent across sessions.Also note optimization packages mlr3tuning mlr3fselect use logger base package bbotk.\ndisable output mlr3, keep output mlr3tuning, reduce verbosity logger mlr3\noptionally change logger bbotk desired level.","code":"\nrequireNamespace(\"lgr\")\n\nlogger = lgr::get_logger(\"mlr3\")\nlogger$set_threshold(\"<level>\")\ngetOption(\"lgr.log_levels\")## fatal error  warn  info debug trace \n##   100   200   300   400   500   600\nlgr::get_logger(\"mlr3\")$set_threshold(\"debug\")\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"info\")"},{"path":"technical.html","id":"redirecting-output","chapter":"5 Technical","heading":"5.5.2 Redirecting output","text":"Redirecting output already extensively covered documentation vignette lgr.\njust short example adds additional appender log events temporary file JSON format:","code":"\ntf = tempfile(\"mlr3log_\", fileext = \".json\")\n\n# get the logger as R6 object\nlogger = lgr::get_logger(\"mlr\")\n\n# add Json appender\nlogger$add_appender(lgr::AppenderJson$new(tf), name = \"json\")\n\n# signal a warning\nlogger$warn(\"this is a warning from mlr3\")## WARN  [14:23:54.563] this is a warning from mlr3\n# print the contents of the file\ncat(readLines(tf))## {\"level\":300,\"timestamp\":\"2021-09-19 14:23:54\",\"logger\":\"mlr\",\"caller\":\"eval\",\"msg\":\"this is a warning from mlr3\"}\n# remove the appender again\nlogger$remove_appender(\"json\")"},{"path":"technical.html","id":"immediate-log-feedback","chapter":"5 Technical","heading":"5.5.3 Immediate Log Feedback","text":"mlr3 uses future package encapsulation make evaluations fast, stable, reproducible.\nHowever, may lead logs delayed, order, , case errors, present .\nnecessary immediate access log messages, example investigate problems, one may therefore choose disable future encapsulation.\ncan done enabling debug mode using options(mlr.debug = TRUE); $encapsulate slot learners also set \"none\" (default) \"evaluate\", \"callr\".\ndone investigate problems, however, production use, (1) disables parallelization, (2) leads different RNG behavior therefore results reproducible debug mode set.","code":""},{"path":"extending.html","id":"extending","chapter":"6 Extending","heading":"6 Extending","text":"chapter gives instructions extend mlr3 extension packages custom objects.approach always :determine base class want inherit ,extend class custom functionality,test implementation(optionally) add new object respective Dictionary.chapter Create new learner illustrates steps needed create custom learner mlr3.","code":""},{"path":"extending.html","id":"extending-learners","chapter":"6 Extending","heading":"6.1 Adding new Learners","text":", show create custom mlr3learner step--step using mlr3extralearners::create_learner.strongly recommended first open learner request issue discuss learner want implement plan creating pull request mlr-org. allows us discuss purpose necessity learner start put real work !section gives insights mlr3learner constructed troubleshoot issues.\nSee Learner FAQ subsection help.Summary steps adding new learnerCheck learner already exist .Fork, clone load mlr3extralearners.Run mlr3extralearners::create_learner.Add learner param_set.Manually add .train .predict private methods learner.applicable add importance oob_error public methods learner.applicable add references learner.Check unit tests paramtests pass (automatically created).Run cleaning functionsOpen pull request new learner template.(copy/paste code shown section. Use create_learner start.)","code":""},{"path":"extending.html","id":"setup","chapter":"6 Extending","heading":"6.1.1 Setting-up mlr3extralearners","text":"order use mlr3extralearners::create_learner function must local copy mlr3extralearners repository must specify correct path package. , follow steps:Fork repositoryClone local copy forked repository.one :Open new R session, call library(\"mlr3extralearners\") (install haven’t already), run mlr3extralearners::create_learner pkg argument set path (folder location) package directory.Open new R session, set working directory newly cloned repository, run devtools::load_all, run mlr3extralearners::create_learner, leaving pkg = \".\".newly cloned repository, open R project, automatically set working directory, run devtools::load_all, run mlr3extralearners::create_learner, leaving pkg = \".\".recommend last option. also important familiar three devtools commands:devtools::document - Generates roxygen documentation new learner.devtools::load_all - Loads functions mlr3extralearners locally, including hidden helper functions.devtools::check - Checks package still passes tests locally.","code":""},{"path":"extending.html","id":"create-learner","chapter":"6 Extending","heading":"6.1.2 Calling create_learner","text":"learner classif.rpart used running example throughout section.full documentation function arguments mlr3extralearners::create_learner, example following:pkg = \".\" - Set package root current directory (assumes mlr3extralearners already set working directory)classname = \"Rpart\" - Set R6 class name LearnerClassifRpart (classif )algorithm = \"decision tree\" - Create title “Classification Decision Tree Learner”, “Classification” determined automatically type “Learner” added learners.type = \"classif\" - Setting learner classification learner, automatically filling title, class name, id (“classif.rpart”) task type.key = \"rpart\" - Used type create unique ID learner, classif.rpart.package = \"rpart\" - Setting package learner implemented, fills things like training function (along caller) man field.caller = \"rpart\" - tells .train function, description function called run algorithm, package automatically fills rpart::rpart.feature_types = c(\"logical\", \"integer\", \"numeric\", \"factor\", \"ordered\") - Sets type features can handled learner. See meta information.predict_types = c(\"response\", \"prob\"), - Sets possible prediction types response (deterministic) prob (probabilistic). See meta information.properties = c(\"importance\", \"missings\", \"multiclass\", \"selected_features\", \"twoclass\", \"weights\") - Sets properties handled learner, including \"importance\" public method called importance created must manually filled. See meta information.references = TRUE - Tells template add “references” tag must filled manually.gh_name = \"RaphaelS1\" - Fills “author” tag GitHub handle, required identifies maintainer learner.sections demonstrate happens function run files created.","code":"\nlibrary(\"mlr3extralearners\")\ncreate_learner(\n  pkg = \".\",\n  classname = \"Rpart\",\n  algorithm = \"decision tree\",\n  type = \"classif\",\n  key = \"rpart\",\n  package = \"rpart\",\n  caller = \"rpart\",\n  feature_types = c(\"logical\", \"integer\", \"numeric\", \"factor\", \"ordered\"),\n  predict_types = c(\"response\", \"prob\"),\n  properties = c(\"importance\", \"missings\", \"multiclass\", \"selected_features\", \"twoclass\", \"weights\"),\n  references = TRUE,\n  gh_name = \"RaphaelS1\"\n)"},{"path":"extending.html","id":"learner_package_type_key.r","chapter":"6 Extending","heading":"6.1.3 learner_package_type_key.R","text":"first script complete running create_learner file form learner_package_type_key.R, case actually learner_rpart_classif_rpart.key. name must changed triggering automated tests rely strict naming scheme. example, resulting script looks like :Now following (top bottom):Fill references “references” delete tag starts “FIXME”Replace <param_set> parameter setOptionally change default values parameters <param_vals>included “importance” properties add function public method importanceFill private .train method, takes (filtered) Task returns model.Fill private .predict method, operates model self$model (stored $train()) (differently subsetted) Task return named list predictions.","code":"#' @title Classification Decision Tree Learner\n#' @author RaphaelS1\n#' @name mlr_learners_classif.rpart\n#'\n#' @template class_learner\n#' @templateVar id classif.rpart\n#' @templateVar caller rpart\n#'\n#' @references\n#' <FIXME - DELETE THIS AND LINE ABOVE IF OMITTED>\n#'\n#' @template seealso_learner\n#' @template example\n#' @export\nLearnerClassifRpart = R6Class(\"LearnerClassifRpart\",\n  inherit = LearnerClassif,\n\n  public = list(\n    #' @description\n    #' Creates a new instance of this [R6][R6::R6Class] class.\n    initialize = function() {\n      # FIXME - MANUALLY ADD PARAM_SET BELOW AND THEN DELETE THIS LINE\n      ps = <param_set>\n\n      # FIXME - MANUALLY UPDATE PARAM VALUES BELOW IF APPLICABLE THEN DELETE THIS LINE.\n      # OTHERWISE DELETE THIS AND LINE BELOW.\n      ps$values = list(<param_vals>)\n\n      super$initialize(\n        id = \"classif.rpart\",\n        packages = \"rpart\",\n        feature_types = c(\"logical\", \"integer\", \"numeric\", \"factor\", \"ordered\"),\n        predict_types = c(\"response\", \"prob\"),\n        param_set = ps,\n        properties = c(\"importance\", \"missings\", \"multiclass\", \"selected_features\", \"twoclass\", \"weights\"),\n        man = \"mlr3extralearners::mlr_learners_classif.rpart\"\n      )\n    },\n\n    # FIXME - ADD IMPORTANCE METHOD HERE AND DELETE THIS LINE.\n    # <See LearnerRegrRandomForest for an example>\n    #' @description\n    #' The importance scores are extracted from the slot <FIXME>.\n    #' @return Named `numeric()`.\n    importance = function() { }\n\n  ),\n\n  private = list(\n\n    .train = function(task) {\n      pars = self$param_set$get_values(tags = \"train\")\n\n      # set column names to ensure consistency in fit and predict\n      self$state$feature_names = task$feature_names\n\n      # FIXME - <Create objects for the train call\n      # <At least \"data\" and \"formula\" are required>\n      formula = task$formula()\n      data = task$data()\n\n      # FIXME - <here is space for some custom adjustments before proceeding to the\n      # train call. Check other learners for what can be done here>\n\n      # use the mlr3misc::invoke function (it's similar to do.call())\n      mlr3misc::invoke(rpart::rpart,\n                       formula = formula,\n                       data = data,\n                       .args = pars)\n    },\n\n    .predict = function(task) {\n      # get parameters with tag \"predict\"\n      pars = self$param_set$get_values(tags = \"predict\")\n      # get newdata\n      newdata = task$data(cols = task$feature_names)\n\n      pred = mlr3misc::invoke(predict, self$model, newdata = newdata,\n                              type = type, .args = pars)\n\n      # FIXME - ADD PREDICTIONS TO LIST BELOW\n      list(...)\n    }\n  )\n)\n\n.extralrns_dict$add(\"classif.rpart\", LearnerClassifRpart)"},{"path":"extending.html","id":"learner-meta-information","chapter":"6 Extending","heading":"6.1.4 Meta-information","text":"constructor (initialize()) constructor super class (e.g. LearnerClassif) called meta information learner constructed.\nincludes:id: ID new learner. Usually consists <type>.<algorithm>, example: \"classif.rpart\".packages: upstream package name implemented learner.param_set: set hyperparameters descriptions provided paradox::ParamSet.\nhyperparameter appropriate class needs chosen. using paradox::ps shortcut, short constructor form p_*** can used:\nparadox::ParamLgl / paradox::p_lgl scalar logical hyperparameters.\nparadox::ParamInt / paradox::p_int scalar integer hyperparameters.\nparadox::ParamDbl / paradox::p_dbl scalar numeric hyperparameters.\nparadox::ParamFct / paradox::p_fct scalar factor hyperparameters (includes characters).\nparadox::ParamUty / paradox::p_uty everything else (e.g. vector paramters list parameters).\nparadox::ParamLgl / paradox::p_lgl scalar logical hyperparameters.paradox::ParamInt / paradox::p_int scalar integer hyperparameters.paradox::ParamDbl / paradox::p_dbl scalar numeric hyperparameters.paradox::ParamFct / paradox::p_fct scalar factor hyperparameters (includes characters).paradox::ParamUty / paradox::p_uty everything else (e.g. vector paramters list parameters).predict_types: Set predict types learner able handle.\ndiffer depending type learner. See mlr_reflections$learner_predict_types full list feature types supported mlr3.\nLearnerClassif\nresponse: predicts class label observation test set.\nprob: Also predicts posterior probability class observation test set.\n\nLearnerRegr\nresponse: predicts numeric response observation test set.\nse: Also predicts standard error value response observation test set.\n\nLearnerClassif\nresponse: predicts class label observation test set.\nprob: Also predicts posterior probability class observation test set.\nresponse: predicts class label observation test set.prob: Also predicts posterior probability class observation test set.LearnerRegr\nresponse: predicts numeric response observation test set.\nse: Also predicts standard error value response observation test set.\nresponse: predicts numeric response observation test set.se: Also predicts standard error value response observation test set.feature_types: Set feature types learner able handle.\nSee mlr_reflections$task_feature_types feature types supported mlr3.properties: Set properties learner. See mlr_reflections$learner_properties full list feature types supported mlr3. Possible properties include:\n\"twoclass\": learner works binary classification problems.\n\"multiclass\": learner works multi-class classification problems.\n\"missings\": learner can natively handle missing values.\n\"weights\": learner can work tasks observation weights / case weights.\n\"parallel\": learner supports internal parallelization way.\nCurrently used, experimental property.\n\"importance\": learner supports extracting importance values features.\nproperty set, must also implement public method importance() retrieve importance values model.\n\"selected_features\": learner supports extracting features used.\nproperty set, must also implement public method selected_features() retrieve set used features model.\n\"twoclass\": learner works binary classification problems.\"multiclass\": learner works multi-class classification problems.\"missings\": learner can natively handle missing values.\"weights\": learner can work tasks observation weights / case weights.\"parallel\": learner supports internal parallelization way.\nCurrently used, experimental property.\"importance\": learner supports extracting importance values features.\nproperty set, must also implement public method importance() retrieve importance values model.\"selected_features\": learner supports extracting features used.\nproperty set, must also implement public method selected_features() retrieve set used features model.man: roxygen identifier learner.\nused within $help() method super class open help page learner.","code":""},{"path":"extending.html","id":"param-set","chapter":"6 Extending","heading":"6.1.5 ParamSet","text":"param_set set hyperparameters used model training predicting, given paradox::ParamSet. set consists list hyperparameters, specific class hyperparameter type (see ).classif.rpart following replace <param_set> :Within mlr3 packages suggest stick lengthly definition consistency, however <param_set> can written shorter, using paradox::ps shortcut:read though learner documentation find full list available parameters. Just looking example:\"cp\" numeric, feasible range [0,1] defaults 0.01.\nparameter used \"train\".\"xval\" integer lower bound 0, default 0 parameter used \"train\".\"keep_model\" logical default FALSE used \"train\".rare cases may want change default parameter values. can passing list <param_vals> template script . can see done \"classif.rpart\" default xval changed 0. Note default ParamSet recorded changed default (0), original (10). strongly recommended change defaults absolutely required, case add following learner documentation:","code":"\nps = ParamSet$new(list(\n  ParamInt$new(id = \"minsplit\", default = 20L, lower = 1L, tags = \"train\"),\n  ParamInt$new(id = \"minbucket\", lower = 1L, tags = \"train\"),\n  ParamDbl$new(id = \"cp\", default = 0.01, lower = 0, upper = 1, tags = \"train\"),\n  ParamInt$new(id = \"maxcompete\", default = 4L, lower = 0L, tags = \"train\"),\n  ParamInt$new(id = \"maxsurrogate\", default = 5L, lower = 0L, tags = \"train\"),\n  ParamInt$new(id = \"maxdepth\", default = 30L, lower = 1L, upper = 30L, tags = \"train\"),\n  ParamInt$new(id = \"usesurrogate\", default = 2L, lower = 0L, upper = 2L, tags = \"train\"),\n  ParamInt$new(id = \"surrogatestyle\", default = 0L, lower = 0L, upper = 1L, tags = \"train\"),\n  ParamInt$new(id = \"xval\", default = 0L, lower = 0L, tags = \"train\"),\n  ParamLgl$new(id = \"keep_model\", default = FALSE, tags = \"train\")\n))\nps$values = list(xval = 0L)\nps = ps(\n  minsplit = p_int(lower = 1L, default = 20L, tags = \"train\"),\n  minbucket = p_int(lower = 1L, tags = \"train\"),\n  cp = p_dbl(lower = 0, upper = 1, default = 0.01, tags = \"train\"),\n  maxcompete = p_int(lower = 0L, default = 4L, tags = \"train\"),\n  maxsurrogate = p_int(lower = 0L, default = 5L, tags = \"train\"),\n  maxdepth = p_int(lower = 1L, upper = 30L, default = 30L, tags = \"train\"),\n  usesurrogate = p_int(lower = 0L, upper = 2L, default = 2L, tags = \"train\"),\n  surrogatestyle = p_int(lower = 0L, upper = 1L, default = 0L, tags = \"train\"),\n  xval = p_int(lower = 0L, default = 0L, tags = \"train\"),\n  keep_model = p_lgl(default = FALSE, tags = \"train\")\n)\n#' @section Custom mlr3 defaults:\n#' - `<parameter>`:\n#'   - Actual default: <value>\n#'   - Adjusted default: <value>\n#'   - Reason for change: <text>"},{"path":"extending.html","id":"learner-train","chapter":"6 Extending","heading":"6.1.6 Train function","text":"Let’s talk .train() method.\ntrain function takes Task input must return model.Let’s say want translate following call rpart::rpart() code can used inside .train() method.First, write something works completely without mlr3:need pass formula notation Species ~ ., data hyperparameters.\nget hyperparameters, call self$param_set$get_values() query parameters using \"train\".dataset extracted Task.Last, call upstream function rpart::rpart() data pass hyperparameters via argument .args using mlr3misc::invoke() function.\nlatter simply optimized version .call() use within mlr3 ecosystem.","code":"\ndata = iris\nmodel = rpart::rpart(Species ~ ., data = iris, xval = 0)\n.train = function(task) {\n  pars = self$param_set$get_values(tags = \"train\")\n  formula = task$formula()\n  data = task$data()\n  mlr3misc::invoke(rpart::rpart,\n                   formula = formula,\n                   data = data,\n                   .args = pars)\n}"},{"path":"extending.html","id":"learner-predict","chapter":"6 Extending","heading":"6.1.7 Predict function","text":"internal predict method .predict() also operates Task well fitted model created train() call previously stored self$model.return value Prediction object.\nproceed analogously previous section.\nstart version without mlr3 objects continue replace objects reached desired interface:rpart::predict.rpart() function predicts class labels argument type set \"class\", class probabilities set \"prob\".Next, transition data task construct list return type requested user, stored $predict_type slot learner class. Note task automatically passed prediction object, need return predictions! Make sure list names identical task predict types.final .predict() method , omit pars line parameters \"predict\" tag keep consistent:Note rely column order data returned task$data() order columns may different order columns $.train. newdata line ensures ordering calling saved order set $.train, don’t delete either lines!","code":"\n# inputs:\ntask = tsk(\"iris\")\nself = list(model = rpart::rpart(task$formula(), data = task$data()))\n\ndata = iris\nresponse = predict(self$model, newdata = data, type = \"class\")\nprob = predict(self$model, newdata = data, type = \"prob\")\n.predict = function(task) {\n  pars = self$param_set$get_values(tags = \"predict\")\n  # get newdata and ensure same ordering in train and predict\n  newdata = task$data(cols = self$state$feature_names)\n  if (self$predict_type == \"response\") {\n    response = mlr3misc::invoke(predict,\n                            self$model,\n                            newdata = newdata,\n                            type = \"class\",\n                            .args = pars)\n\n    return(list(response = response))\n  } else {\n    prob = mlr3misc::invoke(predict,\n                            self$model,\n                            newdata = newdata,\n                            type = \"prob\",\n                            .args = pars)\n    return(list(prob = prob))\n  }\n}"},{"path":"extending.html","id":"learner-control","chapter":"6 Extending","heading":"6.1.8 Control objects/functions of learners","text":"learners rely “control” object/function glmnet::glmnet.control().\nAccounting depends underlying package works:package forwards control parameters via ... makes possible just pass control parameters additional parameters directly train call, need distinguish \"train\" \"control\" parameters.\ncan tagged “train” ParamSet just handed shown previously.control parameters need passed via separate argument, parameters also tagged accordingly ParamSet.\nAfterwards can queried via tag passed separately mlr3misc::invoke().\nSee example .","code":"control_pars = mlr3misc::(<package>::<function>,\n   self$param_set$get_values(tags = \"control\"))\n\ntrain_pars = self$param_set$get_values(tags = \"train\"))\n\nmlr3misc::invoke([...], .args = train_pars, control = control_pars)"},{"path":"extending.html","id":"learner-test","chapter":"6 Extending","heading":"6.1.9 Testing the learner","text":"learner created, ready start testing works, three types tests: manual, unit parameter.","code":""},{"path":"extending.html","id":"learner-test-manual","chapter":"6 Extending","heading":"6.1.9.1 Train and Predict","text":"bare-bone check can just try run simple train() call locally.runs without erroring, ’s good start!","code":"\ntask = tsk(\"iris\") # assuming a Classif learner\nlrn = lrn(\"classif.rpart\")\nlrn$train(task)\np = lrn$predict(task)\np$confusion"},{"path":"extending.html","id":"learner-test-unit","chapter":"6 Extending","heading":"6.1.9.2 Autotest","text":"ensure learner able handle kinds different properties feature types, written “autotest” checks learner different combinations .“autotest” setup generated automatically create_learner open running function, name form test_package_type_key.R, case actually test_rpart_classif_rpart.key. name must changed triggering automated tests rely strict naming scheme. example create following script, changes required pass (assuming learner correctly created):learners required parameters, needed set values required parameters construction learner can run first place.can also exclude specific test arrangements within “autotest” via argument exclude run_autotest() function.\nCurrently run_autotest() function lives inst/testthat mlr_plkg(\"mlr3\") still lacks documentation.\nchange near future.finally run test suite, call devtools::test() hit CTRL + Shift + T using RStudio.","code":"\ninstall_learners(\"classif.rpart\")\n\ntest_that(\"autotest\", {\n  learner = LearnerClassifRpart$new()\n  expect_learner(learner)\n  result = run_autotest(learner)\n  expect_true(result, info = result$error)\n})"},{"path":"extending.html","id":"learner-test-parameter","chapter":"6 Extending","heading":"6.1.9.3 Checking Parameters","text":"learners high number parameters easy miss creation new learner.\naddition, maintainer upstream package changes something respect arguments algorithm, learner danger break.\nAlso, new arguments added upstream manually checking new additions time tedious.Therefore written “Parameter Check” runs every learner asynchronously R CMD Check package . “Parameter Check” compares parameters mlr3 ParamSet arguments available upstream function called $train() $predict(). file automatically created opened create_learner, named like test_paramtest_package_type_key.R, example test_paramtest_rpart_classif_rpart.R.test comes exclude argument used exclude explain certain arguments upstream function within ParamSet mlr3learner. likely required learners common arguments like x, target data handled mlr3 interface therefore included within ParamSet.However, might parameters need excluded, example:Type dependent parameters, .e. parameters apply classification regression learners.Parameters actually deprecated upstream package therefore included mlr3 ParamSet.excluded parameters comment justifying exclusion.example, final paramtest script looks like:","code":"\nlibrary(\"mlr3extralearners\")\ninstall_learners(\"classif.rpart\")\n\ntest_that(\"classif.rpart train\", {\n  learner = lrn(\"classif.rpart\")\n  fun = rpart::rpart\n  exclude = c(\n    \"formula\",# handled internally\n    \"model\", # handled internally\n    \"data\", # handled internally\n    \"weights\", # handled by task\n    \"subset\", # handled by task\n    \"na.action\", # handled internally\n    \"method\", # handled internally\n    \"x\", # handled internally\n    \"y\", # handled internally\n    \"parms\", # handled internally\n    \"control\", # handled internally\n    \"cost\" # handled internally\n  )\n\n  ParamTest = run_paramtest(learner, fun, exclude)\n  expect_true(ParamTest, info = paste0(\n    \"Missing parameters:\",\n    paste0(\"- '\", ParamTest$missing, \"'\", collapse = \"\n\")))\n})\n\ntest_that(\"classif.rpart predict\", {\n  learner = lrn(\"classif.rpart\")\n  fun = rpart:::predict.rpart\n    exclude = c(\n      \"object\", # handled internally\n      \"newdata\", # handled internally\n      \"type\", # handled internally\n      \"na.action\" # handled internally\n    )\n\n  ParamTest = run_paramtest(learner, fun, exclude)\n  expect_true(ParamTest, info = paste0(\n    \"Missing parameters:\",\n    paste0(\"- '\", ParamTest$missing, \"'\", collapse = \"\n\")))\n})"},{"path":"extending.html","id":"cleaning","chapter":"6 Extending","heading":"6.1.10 Package Cleaning","text":"tests passing, run following functions ensure package remains clean tidydevtools::document(roclets = c('rd', 'collate', 'namespace'))haven’t done run: remotes::install_github('pat-s/styler@mlr-style')styler::style_pkg(style = styler::mlr_style)usethis::use_tidy_description()lintr::lint_package()Please fix errors indicated lintr creating pull request. Finally ensure FIXME resolved deleted generated files.now ready add learner mlr3 ecosystem! Simply open pull request  new learner template complete checklist . pull request approved merged, learner automatically appear package website.","code":""},{"path":"extending.html","id":"thanks-and-maintenance","chapter":"6 Extending","heading":"6.1.11 Thanks and Maintenance","text":"Thank contributing mlr3 ecosystem!created learner given GitHub handle, meaning now listed learner author maintainer. means learner breaks responsibility fix learner - can view status learner .","code":""},{"path":"extending.html","id":"learner-faq","chapter":"6 Extending","heading":"6.1.12 Learner FAQ","text":"Question 1How deal Parameters default?AnswerIf learner work without providing value, set reasonable default param_set$values, add tag \"required\" parameter document default properly.Question 2Where add package upstream package DESCRIPTION file?Add “Imports” section.\ninstall upstream package installation mlr3learner yet installed user.Question 3How handle arguments external “control” functions glmnet::glmnet_control()?AnswerSee “Control objects/functions learners”.Question 4How document learner uses custom default value differs default upstream package?AnswerIf set custom default mlr3learner cope one upstream package (think twice really needed!), add information help page respective learner.can use following skeleton :Question 5When \"required\" tag used defining Params purpose?AnswerThe \"required\" tag used following conditions met:upstream function run without setting parameter, .e. throw error.parameter default upstream function.mlr3 follow principle every learner constructable without setting custom parameters.\nTherefore, parameter default upstream function, custom value usually set parameter mlr3learner (remember document changes help page learner).Even though practice ensures parameter unset mlr3learner partially removes usefulness \"required\" tag, tag still useful following scenario:user sets custom parameters construction learnerHere, parameters besides ones set list unset.\nSee paradox::ParamSet information.\nparameter tagged \"required\" ParamSet, call error prompt user required parameters missing.Question 6What error run devtools::load_all()AnswerThis error warning can safely ignore !","code":"\n#' @section Custom mlr3 defaults:\n#' - `<parameter>`:\n#'   - Actual default: <value>\n#'   - Adjusted default: <value>\n#'   - Reason for change: <text>lrn = lrn(\"<id>\")\nlrn$param_set$values = list(\"<param>\" = <value>)> devtools::load_all(\".\")\nLoading mlr3extralearners\nWarning message:\n.onUnload failed in unloadNamespace() for 'mlr3extralearners', details:\n  call: vapply(hooks, function(x) environment(x)$pkgname, NA_character_)\n  error: values must be length 1,\n but FUN(X[[1]]) result is length 0"},{"path":"extending.html","id":"extending-measures","chapter":"6 Extending","heading":"6.2 Adding new Measures","text":"section showcase implement custom performance measure.good starting point writing loss function independently mlr3 (also mlr3measures package).\n, illustrate writing measure implementing root mean squared error regression problems:next step, embed root_mse() function new R6 class inheriting base classes MeasureRegr/Measure.\nclassification measures, use MeasureClassif.\nkeep simple explain important parts Measure class:class can used template performance measures.\nsomething missing, might want consider deeper dive following arguments:properties: tag measure property \"requires_task\", Task automatically passed .score() function (don’t forget add argument task signature).\npossible \"requires_learner\" need operate Learner \"requires_train_set\" want access set training indices score function.aggregator: function (defaulting mean()) controls multiple performance scores, .e. different resampling iterations, aggregated single numeric value average set micro averaging.\nignored macro averaging.predict_sets: Prediction sets (subset (\"train\", \"test\")) operate .\nDefaults “test” set.Finally, want use custom measure just like measure shipped mlr3 access via mlr_measures dictionary, can easily add :Typically good idea put measure together call mlr_measures$add() new R file just source project.","code":"\nroot_mse = function(truth, response) {\n  mse = mean((truth - response)^2)\n  sqrt(mse)\n}\n\nroot_mse(c(0, 0.5, 1), c(0.5, 0.5, 0.5))## [1] 0.4082\nMeasureRootMSE = R6::R6Class(\"MeasureRootMSE\",\n  inherit = mlr3::MeasureRegr,\n  public = list(\n    initialize = function() {\n      super$initialize(\n        # custom id for the measure\n        id = \"root_mse\",\n\n        # additional packages required to calculate this measure\n        packages = character(),\n\n        # properties, see below\n        properties = character(),\n\n        # required predict type of the learner\n        predict_type = \"response\",\n\n        # feasible range of values\n        range = c(0, Inf),\n\n        # minimize during tuning?\n        minimize = TRUE\n      )\n    }\n  ),\n\n  private = list(\n    # custom scoring function operating on the prediction object\n    .score = function(prediction, ...) {\n      root_mse = function(truth, response) {\n        mse = mean((truth - response)^2)\n        sqrt(mse)\n      }\n\n      root_mse(prediction$truth, prediction$response)\n    }\n  )\n)\nmlr3::mlr_measures$add(\"root_mse\", MeasureRootMSE)\n## source(\"measure_root_mse.R\")\nmsr(\"root_mse\")## <MeasureRootMSE:root_mse>\n## * Packages: -\n## * Range: [0, Inf]\n## * Minimize: TRUE\n## * Parameters: list()\n## * Properties: -\n## * Predict type: response"},{"path":"extending.html","id":"extending-pipeops","chapter":"6 Extending","heading":"6.3 Adding new PipeOps","text":"section showcases mlr3pipelines package can extended include custom PipeOps.\nrun following examples, need Task; using well-known “Iris” task:mlr3pipelines fundamentally built around R6. planning create custom PipeOp objects, can help familiarize .principle, PipeOp must inherit PipeOp R6 class implement .train() .predict() functions.\n, however, several auxiliary subclasses can make creation certain operations much easier.","code":"\nlibrary(\"mlr3\")\ntask = tsk(\"iris\")\ntask$data()##        Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n##   1:    setosa          1.4         0.2          5.1         3.5\n##   2:    setosa          1.4         0.2          4.9         3.0\n##   3:    setosa          1.3         0.2          4.7         3.2\n##   4:    setosa          1.5         0.2          4.6         3.1\n##   5:    setosa          1.4         0.2          5.0         3.6\n##  ---                                                            \n## 146: virginica          5.2         2.3          6.7         3.0\n## 147: virginica          5.0         1.9          6.3         2.5\n## 148: virginica          5.2         2.0          6.5         3.0\n## 149: virginica          5.4         2.3          6.2         3.4\n## 150: virginica          5.1         1.8          5.9         3.0"},{"path":"extending.html","id":"ext-pipeopcopy","chapter":"6 Extending","heading":"6.3.1 General Case Example: PipeOpCopy","text":"simple yet useful PipeOp PipeOpCopy, takes single input creates variable number output channels, receive copy input data.\nsimple example showcases important steps defining custom PipeOp.\nshow simplified version , PipeOpCopyTwo, creates exactly two copies input data.following figure visualizes PipeOp situated Pipeline significant - outputs.","code":""},{"path":"extending.html","id":"first-steps-inheriting-from-pipeop","chapter":"6 Extending","heading":"6.3.1.1 First Steps: Inheriting from PipeOp","text":"first part creating custom PipeOp inheriting PipeOp.\nmake mental note need implement .train() .predict() function, probably want initialize() well:Note, private methods, e.g. .train .predict etc prefixed ..","code":"\nPipeOpCopyTwo = R6::R6Class(\"PipeOpCopyTwo\",\n  inherit = mlr3pipelines::PipeOp,\n  public = list(\n    initialize = function(id = \"copy.two\") {\n      ....\n    },\n  ),\n  private == list(\n    .train = function(inputs) {\n      ....\n    },\n\n    .predict = function(inputs) {\n      ....\n    }\n  )\n)"},{"path":"extending.html","id":"channel-definitions","chapter":"6 Extending","heading":"6.3.1.2 Channel Definitions","text":"need tell PipeOp layout channels: many , names going , types acceptable.\ndone initialization PipeOp (using super$initialize call) giving input output data.table objects.\nmust three columns: \"name\" column giving names input output channels, \"train\" \"predict\" column naming class objects expect training prediction input / output.\nspecial value classes \"*\", indicates class accepted; simple copy operator accepts kind input, useful. one input, two output channels.convention, name single channel \"input\" \"output\", group channels [\"input1\", \"input2\", …], unless reason give specific different names. Therefore, input data.table single row <\"input\", \"*\", \"*\">, output table two rows, <\"output1\", \"*\", \"*\"> <\"output2\", \"*\", \"*\">.given PipeOp creator. initialize() thus look follows:","code":"\n    initialize = function(id = \"copy.two\") {\n      input = data.table::data.table(name = \"input\", train = \"*\", predict = \"*\")\n      # the following will create two rows and automatically fill the `train`\n      # and `predict` cols with \"*\"\n      output = data.table::data.table(\n        name = c(\"output1\", \"output2\"),\n        train = \"*\", predict = \"*\"\n      )\n      super$initialize(id,\n        input = input,\n        output = output\n      )\n    }"},{"path":"extending.html","id":"train-and-predict","chapter":"6 Extending","heading":"6.3.1.3 Train and Predict","text":".train() .predict() receive list input must give list return.\nAccording input output definitions, always get list single element input, need return list two elements. want create two copies, just create copies using c(inputs, inputs).Two things consider:.train() function must always modify self$state variable something NULL NO_OP.\n$state slot used signal PipeOp trained data, even state important PipeOp (case).\nTherefore, .train() set self$state = list()..train() function must always modify self$state variable something NULL NO_OP.\n$state slot used signal PipeOp trained data, even state important PipeOp (case).\nTherefore, .train() set self$state = list().necessary “clone” input make deep copies, don’t modify data.\nHowever, changing reference-passed object, example changing data Task, make deep copy first.\nPipeOp may never modify input object reference.necessary “clone” input make deep copies, don’t modify data.\nHowever, changing reference-passed object, example changing data Task, make deep copy first.\nPipeOp may never modify input object reference..train() .predict() functions now:","code":"\n.train = function(inputs) {\n  self$state = list()\n  c(inputs, inputs)\n}\n.predict = function(inputs) {\n  c(inputs, inputs)\n}"},{"path":"extending.html","id":"putting-it-together","chapter":"6 Extending","heading":"6.3.1.4 Putting it Together","text":"whole definition thus becomesWe can create instance PipeOp, put graph, see happens train something:","code":"\nPipeOpCopyTwo = R6::R6Class(\"PipeOpCopyTwo\",\n  inherit = mlr3pipelines::PipeOp,\n  public = list(\n    initialize = function(id = \"copy.two\") {\n      super$initialize(id,\n        input = data.table::data.table(name = \"input\", train = \"*\", predict = \"*\"),\n        output = data.table::data.table(name = c(\"output1\", \"output2\"),\n                            train = \"*\", predict = \"*\")\n      )\n    }\n  ),\n  private = list(\n    .train = function(inputs) {\n      self$state = list()\n      c(inputs, inputs)\n    },\n\n    .predict = function(inputs) {\n      c(inputs, inputs)\n    }\n  )\n)\nlibrary(\"mlr3pipelines\")\npoct = PipeOpCopyTwo$new()\ngr = Graph$new()\ngr$add_pipeop(poct)\n\nprint(gr)## Graph with 1 PipeOps:\n##        ID         State sccssors prdcssors\n##  copy.two <<UNTRAINED>>\nresult = gr$train(task)\n\nstr(result)## List of 2\n##  $ copy.two.output1:Classes 'TaskClassif', 'TaskSupervised', 'Task', 'R6' <TaskClassif:iris> \n##  $ copy.two.output2:Classes 'TaskClassif', 'TaskSupervised', 'Task', 'R6' <TaskClassif:iris>"},{"path":"extending.html","id":"ext-pipe-preproc","chapter":"6 Extending","heading":"6.3.2 Special Case: Preprocessing","text":"Many PipeOps perform operation exactly one Task, return exactly one Task. may even care “Target” / “Outcome” variable task, modification input data.\nHowever, usually important Task perform prediction data columns Task train.\ncases, auxiliary base class PipeOpTaskPreproc exists.\ninherits PipeOp , PipeOps use fall kind use-case named .inheriting PipeOpTaskPreproc, one must either implement private methods .train_task() .predict_task(), methods .train_dt(), .predict_dt(), depending whether wants operate Task object data data.tables.\nsecond case, one can optionally also overload .select_cols() method, chooses incoming Task’s features given .train_dt() / .predict_dt() functions.following show two examples: PipeOpDropNA, removes Task’s rows missing values training (implements .train_task() .predict_task()), PipeOpScale, scales Task’s numeric columns (implements .train_dt(), .predict_dt(), .select_cols()).","code":""},{"path":"extending.html","id":"example-pipeopdropna","chapter":"6 Extending","heading":"6.3.2.1 Example: PipeOpDropNA","text":"Dropping rows missing values may important training model can handle .mlr3 Tasks contain view underlying data, necessary modify data remove rows missing values.\nInstead, rows can removed using Task’s $filter method, modifies Task -place.\ndone private method .train_task().\ntake care also set $state slot signal PipeOp trained.private method .predict_task() need anything; removing missing values prediction useful, since learners handle just ignore respective rows.\nFurthermore, mlr3 expects Learner always return just many predictions given input rows, PipeOp removes Task rows training can used inside GraphLearner.inherit PipeOpTaskPreproc, sets input output data.tables us accept single Task.\nthing initialize() therefore set id (can optionally changed user).complete PipeOpDropNA can therefore written follows.\nNote inherits PipeOpTaskPreproc, unlike PipeOpCopyTwo example :test PipeOp, create small task missing values:test feeding new Graph uses PipeOpDropNA.","code":"\nPipeOpDropNA = R6::R6Class(\"PipeOpDropNA\",\n  inherit = mlr3pipelines::PipeOpTaskPreproc,\n  public = list(\n    initialize = function(id = \"drop.na\") {\n      super$initialize(id)\n    }\n  ),\n\n  private = list(\n    .train_task = function(task) {\n      self$state = list()\n      featuredata = task$data(cols = task$feature_names)\n      exclude = apply(is.na(featuredata), 1, any)\n      task$filter(task$row_ids[!exclude])\n    },\n\n    .predict_task = function(task) {\n      # nothing to be done\n      task\n    }\n  )\n)\nsmalliris = iris[(1:5) * 30, ]\nsmalliris[1, 1] = NA\nsmalliris[2, 2] = NA\nsitask = as_task_classif(smalliris, target = \"Species\")\nprint(sitask$data())##       Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n## 1:     setosa          1.6         0.2           NA         3.2\n## 2: versicolor          3.9         1.4          5.2          NA\n## 3: versicolor          4.0         1.3          5.5         2.5\n## 4:  virginica          5.0         1.5          6.0         2.2\n## 5:  virginica          5.1         1.8          5.9         3.0\ngr = Graph$new()\ngr$add_pipeop(PipeOpDropNA$new())\n\nfiltered_task = gr$train(sitask)[[1]]\nprint(filtered_task$data())##       Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n## 1: versicolor          4.0         1.3          5.5         2.5\n## 2:  virginica          5.0         1.5          6.0         2.2\n## 3:  virginica          5.1         1.8          5.9         3.0"},{"path":"extending.html","id":"example-pipeopscalealways","chapter":"6 Extending","heading":"6.3.2.2 Example: PipeOpScaleAlways","text":"often-applied preprocessing step simply center /scale data mean \\(0\\) standard deviation \\(1\\).\nfits PipeOpTaskPreproc pattern quite well.\nalways replaces columns operates , require information task’s target, needs overload .train_dt() .predict_dt() functions.\nsaves boilerplate-code getting correct feature columns task, replacing modification.scaling makes sense numeric features, want instruct PipeOpTaskPreproc give us numeric columns.\noverloading .select_cols() function: called class determine columns pass .train_dt() .predict_dt().\ninput Task transformed, return character vector features work .\noverloaded, uses columns; instead, set give us numeric columns.\nlevels() data table given .train_dt() .predict_dt() may different task’s levels, functions must also take levels argument named list column names indicating levels.\nworking numeric data, argument can ignored, used instead levels(dt[[column]]) factorial character columns.first PipeOp using $state slot something useful: save centering offset scaling coefficient use $.predict()!simplicity, using hyperparameters always scale center data.\nCompare PipeOpScaleAlways operator one defined inside mlr3pipelines package, PipeOpScale.(Note observant: check PipeOpScale.R mlr3pipelines package, notice uses “get(\"type\")” “get(\"id\")” instead “type” “id”, static code checker CRAN otherwise complain references undefined variables. “problem” data.table exclusive mlr3pipelines.)can, , create new Graph uses PipeOp test .\nCompare resulting data original “iris” Task data printed beginning:","code":"\nPipeOpScaleAlways = R6::R6Class(\"PipeOpScaleAlways\",\n  inherit = mlr3pipelines::PipeOpTaskPreproc,\n  public = list(\n    initialize = function(id = \"scale.always\") {\n      super$initialize(id = id)\n    }\n  ),\n\n  private = list(\n    .select_cols = function(task) {\n      task$feature_types[type == \"numeric\", id]\n    },\n\n    .train_dt = function(dt, levels, target) {\n      sc = scale(as.matrix(dt))\n      self$state = list(\n        center = attr(sc, \"scaled:center\"),\n        scale = attr(sc, \"scaled:scale\")\n      )\n      sc\n    },\n\n    .predict_dt = function(dt, levels) {\n      t((t(dt) - self$state$center) / self$state$scale)\n    }\n  )\n)\ngr = Graph$new()\ngr$add_pipeop(PipeOpScaleAlways$new())\n\nresult = gr$train(task)\n\nresult[[1]]$data()##        Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n##   1:    setosa      -1.3358     -1.3111     -0.89767     1.01560\n##   2:    setosa      -1.3358     -1.3111     -1.13920    -0.13154\n##   3:    setosa      -1.3924     -1.3111     -1.38073     0.32732\n##   4:    setosa      -1.2791     -1.3111     -1.50149     0.09789\n##   5:    setosa      -1.3358     -1.3111     -1.01844     1.24503\n##  ---                                                            \n## 146: virginica       0.8169      1.4440      1.03454    -0.13154\n## 147: virginica       0.7036      0.9192      0.55149    -1.27868\n## 148: virginica       0.8169      1.0504      0.79301    -0.13154\n## 149: virginica       0.9302      1.4440      0.43072     0.78617\n## 150: virginica       0.7602      0.7880      0.06843    -0.13154"},{"path":"extending.html","id":"special-case-preprocessing-with-simple-train","chapter":"6 Extending","heading":"6.3.3 Special Case: Preprocessing with Simple Train","text":"possible make even simplifications many PipeOps perform mostly operation training prediction.\npoint Task preprocessing often modify training data mostly way prediction data (way may depend training data).Consider constant feature removal, example: goal remove features variance, single factor level.\nHowever, features get removed must decided training, may depend training data.\nFurthermore, actual process removing features training prediction.simplification make therefore private method .get_state(task) sets $state slot training, private method .transform(task), gets called training prediction.\ndone PipeOpTaskPreprocSimple class.\nJust like PipeOpTaskPreproc, one can inherit overload functions get PipeOp performs preprocessing little boilerplate code.Just like PipeOpTaskPreproc, PipeOpTaskPreprocSimple offers possibility instead overload .get_state_dt(dt, levels) .transform_dt(dt, levels) methods (optionally, , .select_cols(task) function) operate data.table feature data instead whole Task.Even methods use PipeOpTaskPreprocSimple work similar way: PipeOpScaleAlways example shown also work paradigm.","code":""},{"path":"extending.html","id":"example-pipeopdropconst","chapter":"6 Extending","heading":"6.3.3.1 Example: PipeOpDropConst","text":"typical example preprocessing operation almost operation training prediction operation drops features depending criterion evaluated training.\nOne simple example dropping constant features.\nmlr3 Task class offers flexible view underlying data, efficient drop columns task directly using $select() function, .get_state_dt(dt, levels) / .transform_dt(dt, levels) functions get used; instead overload .get_state(task) .transform(task) methods..get_state() function’s result saved $state slot, want return something useful dropping features.\nchoose save names columns nonzero variance.\nbrevity, use length(unique(column)) > 1 check whether one distinct value present; sophisticated version tolerance parameter numeric values close ..transform() method evaluated training prediction, can rely $state slot present.\ncall Task$select function columns chose keep.full PipeOp written follows:can tested using first five rows “Iris” Task, one feature (\"Petal.Width\") constant:can also see $state correctly set.\nCalling $.predict() graph, even different data (whole Iris Task!) still drop \"Petal.Width\" column, .","code":"\nPipeOpDropConst = R6::R6Class(\"PipeOpDropConst\",\n  inherit = mlr3pipelines::PipeOpTaskPreprocSimple,\n  public = list(\n    initialize = function(id = \"drop.const\") {\n      super$initialize(id = id)\n    }\n  ),\n\n  private = list(\n    .get_state = function(task) {\n      data = task$data(cols = task$feature_names)\n      nonconst = sapply(data, function(column) length(unique(column)) > 1)\n      list(cnames = colnames(data)[nonconst])\n    },\n\n    .transform = function(task) {\n      task$select(self$state$cnames)\n    }\n  )\n)\nirishead = task$clone()$filter(1:5)\nirishead$data()##    Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n## 1:  setosa          1.4         0.2          5.1         3.5\n## 2:  setosa          1.4         0.2          4.9         3.0\n## 3:  setosa          1.3         0.2          4.7         3.2\n## 4:  setosa          1.5         0.2          4.6         3.1\n## 5:  setosa          1.4         0.2          5.0         3.6\ngr = Graph$new()$add_pipeop(PipeOpDropConst$new())\ndropped_task = gr$train(irishead)[[1]]\n\ndropped_task$data()##    Species Petal.Length Sepal.Length Sepal.Width\n## 1:  setosa          1.4          5.1         3.5\n## 2:  setosa          1.4          4.9         3.0\n## 3:  setosa          1.3          4.7         3.2\n## 4:  setosa          1.5          4.6         3.1\n## 5:  setosa          1.4          5.0         3.6\ngr$pipeops$drop.const$state## $cnames\n## [1] \"Petal.Length\" \"Sepal.Length\" \"Sepal.Width\" \n## \n## $affected_cols\n## [1] \"Petal.Length\" \"Petal.Width\"  \"Sepal.Length\" \"Sepal.Width\" \n## \n## $intasklayout\n##              id    type\n## 1: Petal.Length numeric\n## 2:  Petal.Width numeric\n## 3: Sepal.Length numeric\n## 4:  Sepal.Width numeric\n## \n## $outtasklayout\n##              id    type\n## 1: Petal.Length numeric\n## 2: Sepal.Length numeric\n## 3:  Sepal.Width numeric\n## \n## $outtaskshell\n## Empty data.table (0 rows and 4 cols): Species,Petal.Length,Sepal.Length,Sepal.Width\ndropped_predict = gr$predict(task)[[1]]\n\ndropped_predict$data()##        Species Petal.Length Sepal.Length Sepal.Width\n##   1:    setosa          1.4          5.1         3.5\n##   2:    setosa          1.4          4.9         3.0\n##   3:    setosa          1.3          4.7         3.2\n##   4:    setosa          1.5          4.6         3.1\n##   5:    setosa          1.4          5.0         3.6\n##  ---                                                \n## 146: virginica          5.2          6.7         3.0\n## 147: virginica          5.0          6.3         2.5\n## 148: virginica          5.2          6.5         3.0\n## 149: virginica          5.4          6.2         3.4\n## 150: virginica          5.1          5.9         3.0"},{"path":"extending.html","id":"example-pipeopscalealwayssimple","chapter":"6 Extending","heading":"6.3.3.2 Example: PipeOpScaleAlwaysSimple","text":"example show PipeOpTaskPreprocSimple can used working feature data form data.table.\nInstead calling scale() function, center scale values calculated directly saved $state slot.\n.transform_dt() function perform operation training prediction: subtract center divide scale value.\nPipeOpScaleAlways example , use .select_cols() work numeric columns.can compare PipeOp one show behaves .","code":"\nPipeOpScaleAlwaysSimple = R6::R6Class(\"PipeOpScaleAlwaysSimple\",\n  inherit = mlr3pipelines::PipeOpTaskPreprocSimple,\n  public = list(\n    initialize = function(id = \"scale.always.simple\") {\n      super$initialize(id = id)\n    }\n  ),\n\n  private = list(\n    .select_cols = function(task) {\n      task$feature_types[type == \"numeric\", id]\n    },\n\n    .get_state_dt = function(dt, levels, target) {\n      list(\n        center = sapply(dt, mean),\n        scale = sapply(dt, sd)\n      )\n    },\n\n    .transform_dt = function(dt, levels) {\n      t((t(dt) - self$state$center) / self$state$scale)\n    }\n  )\n)\ngr = Graph$new()$add_pipeop(PipeOpScaleAlways$new())\nresult_posa = gr$train(task)[[1]]\n\ngr = Graph$new()$add_pipeop(PipeOpScaleAlwaysSimple$new())\nresult_posa_simple = gr$train(task)[[1]]\nresult_posa$data()##        Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n##   1:    setosa      -1.3358     -1.3111     -0.89767     1.01560\n##   2:    setosa      -1.3358     -1.3111     -1.13920    -0.13154\n##   3:    setosa      -1.3924     -1.3111     -1.38073     0.32732\n##   4:    setosa      -1.2791     -1.3111     -1.50149     0.09789\n##   5:    setosa      -1.3358     -1.3111     -1.01844     1.24503\n##  ---                                                            \n## 146: virginica       0.8169      1.4440      1.03454    -0.13154\n## 147: virginica       0.7036      0.9192      0.55149    -1.27868\n## 148: virginica       0.8169      1.0504      0.79301    -0.13154\n## 149: virginica       0.9302      1.4440      0.43072     0.78617\n## 150: virginica       0.7602      0.7880      0.06843    -0.13154\nresult_posa_simple$data()##        Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n##   1:    setosa      -1.3358     -1.3111     -0.89767     1.01560\n##   2:    setosa      -1.3358     -1.3111     -1.13920    -0.13154\n##   3:    setosa      -1.3924     -1.3111     -1.38073     0.32732\n##   4:    setosa      -1.2791     -1.3111     -1.50149     0.09789\n##   5:    setosa      -1.3358     -1.3111     -1.01844     1.24503\n##  ---                                                            \n## 146: virginica       0.8169      1.4440      1.03454    -0.13154\n## 147: virginica       0.7036      0.9192      0.55149    -1.27868\n## 148: virginica       0.8169      1.0504      0.79301    -0.13154\n## 149: virginica       0.9302      1.4440      0.43072     0.78617\n## 150: virginica       0.7602      0.7880      0.06843    -0.13154"},{"path":"extending.html","id":"ext-pipe-hyperpars","chapter":"6 Extending","heading":"6.3.4 Hyperparameters","text":"mlr3pipelines uses paradox package define parameter spaces PipeOps.\nParameters PipeOps can modify behavior certain ways, e.g. switch centering scaling PipeOpScale operator.\nunified interface makes possible parameters whole Graphs modify individual PipeOp’s behavior.\nGraphs, encapsulated GraphLearners, can even tuned using tuning functionality mlr3tuning.Hyperparameters declared initialization, calling PipeOp’s $initialize() function, giving param_set argument.\nparam_set must ParamSet paradox package; see tuning chapter -depth paradox chapter information define parameter spaces.\nconstruction, ParamSet can accessed $param_set slot.\npossible modify ParamSet, using e.g. $add() $add_dep() functions, adding PipeOp, strongly advised .Hyperparameters can set queried $values slot.\nsetting hyperparameters, automatically checked satisfy conditions set $param_set, necessary type check .\naware always possible remove hyperparameter values.PipeOp initialized, usually parameter values—$values takes value list().\npossible set initial parameter values $initialize() constructor; must done super$initialize() call corresponding ParamSet must supplied.\nsetting $values checks current $param_set, fail $param_set set yet.using underlying library function (scale function PipeOpScale, say), usually “default” behaviour function parameter given.\ngood practice use default behaviour whenever parameter set (removed).\ncan easily done using mlr3misc library’s mlr3misc::invoke() function, functionality similar .call().","code":""},{"path":"extending.html","id":"hyperparameter-example-pipeopscale","chapter":"6 Extending","heading":"6.3.4.1 Hyperparameter Example: PipeOpScale","text":"use hyperparameters can best shown example PipeOpScale, similar example , PipeOpScaleAlways.\ndifference made presence hyperparameters.\nPipeOpScale constructs ParamSet $initialize function passes super$initialize function:user access can set get parameters.\nTypes automatically checked:PipeOpScale handles parameters can seen $.train_dt method: gets relevant parameters $values slot uses mlr3misc::invoke() call.\nadvantage calling scale() directly parameter given, default value scale() function used.Another change necessary compared PipeOpScaleAlways attributes \"scaled:scale\" \"scaled:center\" always present, depending parameters, possibly need set default values \\(1\\) \\(0\\), respectively.now even possible (bit pointless) call PipeOpScale scale center set FALSE, returns original dataset, unchanged.","code":"\nPipeOpScale$public_methods$initialize## function (id = \"scale\", param_vals = list()) \n## .__PipeOpScale__initialize(self = self, private = private, super = super, \n##     id = id, param_vals = param_vals)\n## <environment: namespace:mlr3pipelines>\npss = po(\"scale\")\nprint(pss$param_set)## <ParamSet:scale>\n##                id    class lower upper nlevels        default value\n## 1:         center ParamLgl    NA    NA       2           TRUE      \n## 2:          scale ParamLgl    NA    NA       2           TRUE      \n## 3:         robust ParamLgl    NA    NA       2 <NoDefault[3]> FALSE\n## 4: affect_columns ParamUty    NA    NA     Inf  <Selector[1]>\npss$param_set$values$center = FALSE\nprint(pss$param_set$values)## $robust\n## [1] FALSE\n## \n## $center\n## [1] FALSE\npss$param_set$values$scale = \"TRUE\"  # bad input is checked!## Error in self$assert(xs): Assertion on 'xs' failed: scale: Must be of type 'logical flag', not 'character'.\nPipeOpScale$private_methods$.train_dt## function (dt, levels, target) \n## .__PipeOpScale__.train_dt(self = self, private = private, super = super, \n##     dt = dt, levels = levels, target = target)\n## <environment: namespace:mlr3pipelines>\npss$param_set$values$scale = FALSE\npss$param_set$values$center = FALSE\n\ngr = Graph$new()\ngr$add_pipeop(pss)\n\nresult = gr$train(task)\n\nresult[[1]]$data()##        Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n##   1:    setosa          1.4         0.2          5.1         3.5\n##   2:    setosa          1.4         0.2          4.9         3.0\n##   3:    setosa          1.3         0.2          4.7         3.2\n##   4:    setosa          1.5         0.2          4.6         3.1\n##   5:    setosa          1.4         0.2          5.0         3.6\n##  ---                                                            \n## 146: virginica          5.2         2.3          6.7         3.0\n## 147: virginica          5.0         1.9          6.3         2.5\n## 148: virginica          5.2         2.0          6.5         3.0\n## 149: virginica          5.4         2.3          6.2         3.4\n## 150: virginica          5.1         1.8          5.9         3.0"},{"path":"extending.html","id":"extending-tuners","chapter":"6 Extending","heading":"6.4 Adding new Tuners","text":"section, show implement custom tuner mlr3tuning.\nmain task tuner iteratively propose new hyperparameter configurations want evaluate given task, learner validation strategy.\nsecond task decide configuration returned tuning result - usually configuration led best observed performance value.\nwant implement tuner, implement R6-Object offers .optimize method implements iterative proposal free implement .assign_result differ -mentioned default process determining result.start implementation make familiar main R6-Objects bbotk (Black-Box Optimization Toolkit).\npackage provide basic black box optimization algorithms also objects represent optimization problem (bbotk::OptimInstance) log evaluated configurations (bbotk::Archive).two ways implement new tuner:\n) new tuner can applied kind optimization problem implemented bbotk::Optimizer.\nbbotk::Optimizer can easily transformed mlr3tuning::Tuner.\nb) new custom tuner usable hyperparameter tuning, example needs access task, learner resampling objects directly implemented mlr3tuning mlr3tuning::Tuner.","code":""},{"path":"extending.html","id":"extending-tuners-summary","chapter":"6 Extending","heading":"6.4.1 Adding a new Tuner","text":"summary steps adding new tuner.\nfifth step required new tuner added via bbotk.Check tuner already exist bbotk::Optimizer mlr3tuning::Tuner GitHub repositories.Use one existing optimizers / tuners template.Overwrite .optimize private method optimizer / tuner.Optionally, overwrite default .assign_result private method.Use mlr3tuning::TunerFromOptimizer class transform bbotk::Optimizer mlr3tuning::Tuner.Add unit tests tuner optionally optimizer.Open new pull request mlr3tuning::Tuner optionally second one bbotk::Optimizer.","code":""},{"path":"extending.html","id":"tuner-template","chapter":"6 Extending","heading":"6.4.2 Template","text":"new custom tuner implemented via bbotk, use one existing optimizer template e.g. bbotk::OptimizerRandomSearch. currently two tuners based bbotk::Optimizer: mlr3hyperband::TunerHyperband mlr3tuning::TunerIrace. rather complex can still use documentation class structure template. following steps identical optimizers tuners.Rewrite meta information documentation create new class name.\nScientific sources can added R/bibentries.R added @source documentation.\nexample dictionary sections documentation auto-generated based @templateVar id <tuner_id>.\nChange parameter set optimizer / tuner document @section Parameters.\nforget change mlr_optimizers$add() / mlr_tuners$add() last line adds optimizer / tuner dictionary.","code":""},{"path":"extending.html","id":"tuner-optimize","chapter":"6 Extending","heading":"6.4.3 Optimize method","text":"$.optimize() private method main part tuner.\ntakes instance, proposes new points calls $eval_batch() method instance evaluate .\ncan go two ways: Implement iterative process call external optimization function resides another package.","code":""},{"path":"extending.html","id":"writing-a-custom-iteration","chapter":"6 Extending","heading":"6.4.3.1 Writing a custom iteration","text":"Usually, proposal evaluation done repeat-loop implement.\nPlease consider following points:can evaluate one multiple points per iterationYou don’t care termination, $eval_batch() won’t allow evaluations allowed bbotk::Terminator. implies, code repeat-loop executed.don’t care keeping track evaluations every evaluation automatically stored inst$archive.want log additional information evaluation bbotk::Objective`` thebbotk::Archiveyou can simply add columns thedata.tableobject passed $eval_batch()`.","code":""},{"path":"extending.html","id":"calling-an-external-optimization-function","chapter":"6 Extending","heading":"6.4.3.2 Calling an external optimization function","text":"Optimization functions external packages usually take objective function argument.\ncase, can pass inst$objective_function internally calls $eval_batch().\nCheck OptimizerGenSA example.","code":""},{"path":"extending.html","id":"tuner-add-result","chapter":"6 Extending","heading":"6.4.4 Assign result method","text":"default $.assign_result() private method simply obtains best performing result archive.\ndefault method can overwritten new tuner determines result optimization different way.\nnew function must call $assign_result() method instance write final result instance.\nSee mlr3tuning::TunerIrace implementation $.assign_result().","code":""},{"path":"extending.html","id":"tuner-from-optimizer","chapter":"6 Extending","heading":"6.4.5 Transform optimizer to tuner","text":"step needed implement via bbotk.\nmlr3tuning::TunerFromOptimizer class transforms bbotk::Optimizer mlr3tuning::Tuner.\nJust add bbotk::Optimizer optimizer field.\nSee mlr3tuning::TunerRandomSearch example.","code":""},{"path":"extending.html","id":"tuner-test","chapter":"6 Extending","heading":"6.4.6 Add unit tests","text":"new custom tuner thoroughly tested unit tests.\nmlr3tuning::Tuners can tested test_tuner() helper function.\nadded Tuner via bbotk::Optimizer, additionally test bbotk::Optimizer test_optimizer() helper function.","code":""},{"path":"special-tasks.html","id":"special-tasks","chapter":"7 Special Tasks","heading":"7 Special Tasks","text":"chapter explores different functions mlr3 dealing specific data sets require statistical modification undertake sensible analysis.\nFollowing topics discussed:Survival AnalysisThis sub-chapter explains conduct sound survival analysis mlr3.\nSurvival analysis used monitor period time specific event takes places.\nspecific event e.g. death, transmission disease, marriage divorce.\nTwo considerations important conducting survival analysis:Whether event occurred within frame given dataHow much time took event occurredIn summary, sub-chapter explains account considerations conduct survival analysis using mlr3proba extension package.Density EstimationThis sub-chapter explains conduct (unconditional) density estimation mlr3.\nDensity estimation used estimate probability density function continuous variable. Unconditional density estimation unsupervised task ‘value’ predict, instead densities estimated.sub-chapter explains estimate probability distributions continuous variables using mlr3proba extension package.Spatiotemporal AnalysisSpatiotemporal analysis data observations entail reference information spatial temporal characteristics.\nOne largest issues spatiotemporal data analysis inevitable presence auto-correlation data.\nAuto-correlation especially severe data marginal spatiotemporal variation.\nsub-chapter Spatiotemporal analysis provides instructions account spatiotemporal data.Ordinal AnalysisThis work progress.\nSee mlr3ordinal current state.Functional AnalysisFunctional analysis contains data consists curves varying continuum e.g. time, frequency wavelength.\ntype analysis frequently used examining measurements various time points.\nSteps accommodate functional data structures mlr3 explained functional analysis sub-chapter.Multilabel ClassificationMultilabel classification deals objects can belong one category time.\nNumerous target labels attributed single observation.\nWorking multilabel data requires one use modified algorithms, accommodate data specific characteristics.\nTwo approaches multilabel classification prominently used:problem transformation methodThe algorithm adaption methodInstructions deal multilabel classification mlr3 can found sub-chapter.Cost Sensitive ClassificationThis sub-chapter deals implementation cost-sensitive classification.\nRegular classification aims minimize misclassification rate thus types misclassification errors deemed equally severe.\nCost-sensitive classification setting costs caused different kinds errors assumed equal.\nobjective minimize expected costs.Analytical data big credit institution used use case illustrate different features.\nFirstly, sub-chapter provides guidance implement first model.\nSubsequently, sub-chapter contains instructions modify cost sensitivity measures, thresholding threshold tuning.Cluster AnalysisCluster analysis aims group data clusters objects similar end cluster.\nFundamentally, clustering classification similar.\nHowever, clustering unsupervised task observations contain true labels classification, labels needed order train model.sub-chapter explains perform cluster analysis mlr3 help mlr3cluster extension package.","code":""},{"path":"special-tasks.html","id":"survival","chapter":"7 Special Tasks","heading":"7.1 Survival Analysis","text":"Survival analysis sub-field supervised machine learning aim predict survival distribution given individual.\nArguably main feature survival analysis unlike classification regression, learners trained two features:time event takes placethe event type: either censoring death.particular time-point, individual either: alive, dead, censored.\nCensoring occurs unknown individual alive dead.\nexample, say interested patients hospital every day recorded alive dead, patient leaves unknown alive dead, hence censored.\ncensoring, ordinary regression analysis used instead.\nFurthermore, survival data contains solely positive values therefore needs transformed avoid biases.Note survival analysis accounts censored uncensored observations adjusting respective model parameters.package mlr3proba (Sonabend et al. 2021) extends mlr3 following objects survival analysis:TaskSurv define (censored) survival tasksLearnerSurv base class survival learnersPredictionSurv specialized class Prediction objectsMeasureSurv specialized class performance measuresFor good introduction survival analysis see Modelling Survival Data Medical Research (Collett 2014).","code":""},{"path":"special-tasks.html","id":"tasksurv","chapter":"7 Special Tasks","heading":"7.1.1 TaskSurv","text":"Unlike TaskClassif TaskRegr single ‘target’ argument, TaskSurv mimics \nsurvival::Surv object three-four target arguments (dependent censoring type).\nTaskSurv can constructed function as_task_surv():","code":"\nlibrary(\"mlr3\")\nlibrary(\"mlr3proba\")\nlibrary(\"survival\")\n\nas_task_surv(survival::bladder2[, -1L], id = \"interval_censored\",\n  time = \"start\", time2 = \"stop\", type = \"interval\")## <TaskSurv:interval_censored> (178 x 7)\n## * Target: start, stop, event\n## * Properties: -\n## * Features (4):\n##   - dbl (2): enum, rx\n##   - int (2): number, size\n# type = \"right\" is default\ntask = as_task_surv(survival::rats, id = \"right_censored\",\n  time = \"time\", event = \"status\", type = \"right\")\n\nprint(task)## <TaskSurv:right_censored> (300 x 5)\n## * Target: time, status\n## * Properties: -\n## * Features (3):\n##   - int (1): litter\n##   - dbl (1): rx\n##   - chr (1): sex\n# the target column is a survival object:\nhead(task$truth())## [1] 101+  49  104+  91+ 104+ 102+\n# kaplan-meier plot\nlibrary(\"mlr3viz\")\nautoplot(task)## Registered S3 method overwritten by 'GGally':\n##   method from   \n##   +.gg   ggplot2"},{"path":"special-tasks.html","id":"predict-types---crank-lp-and-distr","chapter":"7 Special Tasks","heading":"7.1.2 Predict Types - crank, lp, and distr","text":"Every PredictionSurv object can predict one :lp - Linear predictor calculated fitted coefficients multiplied test data.distr - Predicted survival distribution, either discrete continuous. Implemented distr6.crank - Continuous risk ranking.lp crank can used measures discrimination concordance index.\nWhilst lp specific mathematical prediction, crank continuous ranking identifies less likely experience event.\nfar implemented learner returns continuous ranking surv.svm.\nPredictionSurv returns lp crank identical .\nOtherwise crank calculated expectation predicted survival distribution.\nNote linear proportional hazards models, ranking (necessarily crank score ) given lp expectation distr, identical.example uses rats task shipped mlr3proba.","code":"\ntask = tsk(\"rats\")\nlearn = lrn(\"surv.coxph\")\n\ntrain_set = sample(task$nrow, 0.8 * task$nrow)\ntest_set = setdiff(seq_len(task$nrow), train_set)\n\nlearn$train(task, row_ids = train_set)\nprediction = learn$predict(task, row_ids = test_set)\n\nprint(prediction)## <PredictionSurv> for 60 observations:\n##     row_ids time status   crank      lp     distr\n##           1  101  FALSE  0.3508  0.3508 <list[1]>\n##           4   91  FALSE -3.1619 -3.1619 <list[1]>\n##          12  102  FALSE -3.8234 -3.8234 <list[1]>\n## ---                                              \n##         287   69  FALSE -3.2140 -3.2140 <list[1]>\n##         290   91  FALSE  0.3120  0.3120 <list[1]>\n##         300  102  FALSE -3.1875 -3.1875 <list[1]>"},{"path":"special-tasks.html","id":"composition","chapter":"7 Special Tasks","heading":"7.1.3 Composition","text":"Finally take look PipeOps implemented mlr3proba, used composition predict types.\nexample, predict linear predictor lot meaning , can composed survival distribution.\nSee mlr3pipelines full tutorials details PipeOps.","code":"\nlibrary(\"mlr3pipelines\")\nlibrary(\"mlr3learners\")\n# PipeOpDistrCompositor - Train one model with a baseline distribution,\n# (Kaplan-Meier or Nelson-Aalen), and another with a predicted linear predictor.\ntask = tsk(\"rats\")\n# remove the factor column for support with glmnet\ntask$select(c(\"litter\", \"rx\"))\nlearner_lp = lrn(\"surv.glmnet\")\nlearner_distr = lrn(\"surv.kaplan\")\nprediction_lp = learner_lp$train(task)$predict(task)\nprediction_distr = learner_distr$train(task)$predict(task)\nprediction_lp$distr\n\n# Doesn't need training. Base = baseline distribution. ph = Proportional hazards.\n\npod = po(\"compose_distr\", form = \"ph\", overwrite = FALSE)\nprediction = pod$predict(list(base = prediction_distr, pred = prediction_lp))$output\n\n# Now we have a predicted distr!\n\nprediction$distr\n\n# This can all be simplified by using the distrcompose pipeline\n\nglm.distr = ppl(\"distrcompositor\", learner = lrn(\"surv.glmnet\"),\n                estimator = \"kaplan\", form = \"ph\", overwrite = FALSE, graph_learner = TRUE)\nglm.distr$train(task)$predict(task)"},{"path":"special-tasks.html","id":"benchmark-experiment","chapter":"7 Special Tasks","heading":"7.1.4 Benchmark Experiment","text":"Finally, conduct small benchmark study rats task using integrated survival learners:experiment indicates Cox PH random forest better discrimination Kaplan-Meier baseline estimator, machine learning random forest consistently better interpretable Cox PH.","code":"\nlibrary(\"mlr3learners\")\n\ntask = tsk(\"rats\")\n\n# some integrated learners\nlearners = lrns(c(\"surv.coxph\", \"surv.kaplan\", \"surv.ranger\"))\nprint(learners)## [[1]]\n## <LearnerSurvCoxPH:surv.coxph>\n## * Model: -\n## * Parameters: list()\n## * Packages: survival, distr6\n## * Predict Type: distr\n## * Feature types: logical, integer, numeric, factor\n## * Properties: weights\n## \n## [[2]]\n## <LearnerSurvKaplan:surv.kaplan>\n## * Model: -\n## * Parameters: list()\n## * Packages: survival, distr6\n## * Predict Type: crank\n## * Feature types: logical, integer, numeric, character, factor, ordered\n## * Properties: missings\n## \n## [[3]]\n## <LearnerSurvRanger:surv.ranger>\n## * Model: -\n## * Parameters: num.threads=1\n## * Packages: ranger\n## * Predict Type: distr\n## * Feature types: logical, integer, numeric, character, factor, ordered\n## * Properties: importance, oob_error, weights\n# Harrell's C-Index for survival\nmeasure = msr(\"surv.cindex\")\nprint(measure)## <MeasureSurvCindex:surv.harrell_c>\n## * Packages: -\n## * Range: [0, 1]\n## * Minimize: FALSE\n## * Parameters: list()\n## * Properties: -\n## * Predict type: crank\n## * Return type: Score\nset.seed(1)\nbmr = benchmark(benchmark_grid(task, learners, rsmp(\"cv\", folds = 3)))\nbmr$aggregate(measure)##    nr      resample_result task_id  learner_id resampling_id iters\n## 1:  1 <ResampleResult[20]>    rats  surv.coxph            cv     3\n## 2:  2 <ResampleResult[20]>    rats surv.kaplan            cv     3\n## 3:  3 <ResampleResult[20]>    rats surv.ranger            cv     3\n##    surv.harrell_c\n## 1:         0.7671\n## 2:         0.5000\n## 3:         0.7737\nautoplot(bmr, measure = measure)"},{"path":"special-tasks.html","id":"density","chapter":"7 Special Tasks","heading":"7.2 Density Estimation","text":"Density estimation learning task find unknown distribution ..d. data set generated.\ninterpret broadly, distribution necessarily continuous (may possess mass density).\nconditional case, distribution predicted conditional covariates, known ‘probabilistic supervised regression’, implemented mlr3proba near-future.\nUnconditional density estimation viewed unsupervised task.\ngood overview density estimation see Density estimation statistics data analysis (Silverman 1986).package mlr3proba extends mlr3 following objects density estimation:TaskDens define density tasksLearnerDens base class density estimatorsPredictionDens specialized class Prediction objectsMeasureDens specialized class performance measuresIn example demonstrate basic functionality package faithful data datasets package.\ntask ships pre-defined TaskDens mlr3proba.Unconditional density estimation unsupervised method.\nHence, TaskDens unsupervised task inherits directly Task unlike TaskClassif TaskRegr.\nHowever, TaskDens still target argument $truth field defined :target - name variable data estimate density$truth - values target column (true density, always unknown)","code":"\nlibrary(\"mlr3\")\nlibrary(\"mlr3proba\")\n\ntask = tsk(\"precip\")\nprint(task)## <TaskDens:precip> (70 x 1)\n## * Target: -\n## * Properties: -\n## * Features (1):\n##   - dbl (1): precip\n# histogram and density plot\nlibrary(\"mlr3viz\")\nautoplot(task, type = \"overlay\")## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"special-tasks.html","id":"train-and-predict-1","chapter":"7 Special Tasks","heading":"7.2.1 Train and Predict","text":"Density learners train predict methods, though unsupervised, ‘prediction’ actually ‘estimation’.\ntraining, distr6 object created,\nsee full tutorials access probability density function, pdf, cumulative distribution function, cdf, important fields methods.\npredict method simply wrapper around self$model$pdf available self$model$cdf, .e. evaluates pdf/cdf given points.\nNote prediction points evaluate pdf cdf determined target column TaskDens object used testing.Every PredictionDens object can estimate:pdf - probability density functionSome learners can estimate:cdf - cumulative distribution function","code":"\n# create task and learner\n\ntask_faithful = TaskDens$new(id = \"eruptions\", backend = datasets::faithful$eruptions)\nlearner = lrn(\"dens.hist\")\n\n# train/test split\ntrain_set = sample(task_faithful$nrow, 0.8 * task_faithful$nrow)\ntest_set = setdiff(seq_len(task_faithful$nrow), train_set)\n\n# fitting KDE and model inspection\nlearner$train(task_faithful, row_ids = train_set)\nlearner$model## $distr\n## Histogram() \n## \n## $hist\n## $breaks\n## [1] 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5\n## \n## $counts\n## [1] 41 38  3  5 24 61 42  3\n## \n## $density\n## [1] 0.37788 0.35023 0.02765 0.04608 0.22120 0.56221 0.38710 0.02765\n## \n## $mids\n## [1] 1.75 2.25 2.75 3.25 3.75 4.25 4.75 5.25\n## \n## $xname\n## [1] \"dat\"\n## \n## $equidist\n## [1] TRUE\n## \n## attr(,\"class\")\n## [1] \"histogram\"\n## \n## attr(,\"class\")\n## [1] \"dens.hist\"\nclass(learner$model)## [1] \"dens.hist\"\n# make predictions for new data\nprediction = learner$predict(task_faithful, row_ids = test_set)"},{"path":"special-tasks.html","id":"benchmark-experiment-1","chapter":"7 Special Tasks","heading":"7.2.2 Benchmark Experiment","text":"Finally, conduct small benchmark study precip task using integrated survival learners:results experiment show sophisticated Penalized Density Estimator outperform baseline Histogram, Kernel Density Estimator least consistently better (.e. lower logloss) results.","code":"\n# some integrated learners\nlearners = lrns(c(\"dens.hist\", \"dens.kde\"))\nprint(learners)## [[1]]\n## <LearnerDensHistogram:dens.hist>\n## * Model: -\n## * Parameters: list()\n## * Packages: distr6\n## * Predict Type: pdf\n## * Feature types: integer, numeric\n## * Properties: -\n## \n## [[2]]\n## <LearnerDensKDE:dens.kde>\n## * Model: -\n## * Parameters: kernel=Epan, bandwidth=silver\n## * Packages: distr6\n## * Predict Type: pdf\n## * Feature types: integer, numeric\n## * Properties: missings\n# Logloss for probabilistic predictions\nmeasure = msr(\"dens.logloss\")\nprint(measure)## <MeasureDensLogloss:dens.logloss>\n## * Packages: -\n## * Range: [0, Inf]\n## * Minimize: TRUE\n## * Parameters: list()\n## * Properties: -\n## * Predict type: pdf\nset.seed(1)\nbmr = benchmark(benchmark_grid(task, learners, rsmp(\"cv\", folds = 3)))\nbmr$aggregate(measure)##    nr      resample_result task_id learner_id resampling_id iters dens.logloss\n## 1:  1 <ResampleResult[20]>  precip  dens.hist            cv     3        4.396\n## 2:  2 <ResampleResult[20]>  precip   dens.kde            cv     3        4.818\nautoplot(bmr, measure = measure)"},{"path":"special-tasks.html","id":"spatiotemporal","chapter":"7 Special Tasks","heading":"7.3 Spatiotemporal Analysis","text":"Data observations may entail reference information spatial temporal characteristics.\nSpatial information stored coordinates, usually named “x” “y” “lat”/“lon”.\nTreating spatiotemporal data using non-spatial data methods can lead -optimistic performance estimates.\nHence, methods specifically designed account special nature spatiotemporal data needed.mlr3 framework, following packages relate field:mlr3spatiotemporal (biased-reduced performance estimation)mlr3forecasting (time-series support)mlr3spatial (spatial prediction support)following (sub-)sections introduce potential pitfalls spatiotemporal data machine learning account .\nNote functionality covered, used packages still early lifecycles.\nwant contribute one packages mentioned , please contact Patrick Schratz.","code":""},{"path":"special-tasks.html","id":"creating-a-spatial-task","chapter":"7 Special Tasks","heading":"7.3.1 Creating a spatial Task","text":"make use spatial resampling methods, {mlr3} task aware spatial characteristic needs created.\nTwo child classes exist {mlr3spatiotempcv} purpose:TaskClassifSTTaskRegrSTTo create one , one can either pass sf object “backend” directly:use plain data.frame.\ncase, constructor TaskClassifST needs arguments:Now Task can used normal {mlr3} task kind modeling scenario.","code":"\n# create 'sf' object\ndata_sf = sf::st_as_sf(ecuador, coords = c(\"x\", \"y\"), crs = 32717)\n# create mlr3 task\ntask = TaskClassifST$new(\"ecuador_sf\",\n  backend = data_sf, target = \"slides\", positive = \"TRUE\"\n)\ndata = mlr3::as_data_backend(ecuador)\ntask = TaskClassifST$new(\"ecuador\",\n  backend = data, target = \"slides\",\n  positive = \"TRUE\", extra_args = list(coordinate_names = c(\"x\", \"y\"),\n  crs = 32717)\n)"},{"path":"special-tasks.html","id":"spatiotemporal-intro","chapter":"7 Special Tasks","heading":"7.3.2 Autocorrelation","text":"Data includes spatial temporal information requires special treatment machine learning (similar survival, ordinal task types listed special tasks chapter).\ncontrast non-spatial/non-temporal data, observations inherit natural grouping, either space time space time (Legendre 1993).\ngrouping causes observations autocorrelated, either space (spatial autocorrelation (SAC)), time (temporal autocorrelation (TAC)) space time (spatiotemporal autocorrelation (STAC)).\nsimplicity, acronym STAC used generic term following chapter different characteristics introduced .effects STAC statistical/machine learning?overarching problem STAC violates assumption observations train test datasets independent (Hastie, Friedman, Tibshirani 2001).\nassumption violated, reliability resulting performance estimates, example retrieved via cross-validation, decreased.\nmagnitude decrease linked magnitude STAC dataset, determined easily.One approach account existence STAC use dedicated resampling methods.\nmlr3spatiotemporal provides access frequently used spatiotemporal resampling methods.\nfollowing example showcases spatial dataset can used retrieve bias-reduced performance estimate learner.following examples use ecuador dataset created Jannes Muenchow.\ncontains information occurrence landslides (binary) Andes Southern Ecuador.\nlandslides mapped aerial photos taken 2000.\ndataset well suited serve example relatively small course due spatial nature observations.\nPlease refer Muenchow, Brenning, Richter (2012) detailed description dataset.account spatial autocorrelation probably present landslide data, make use one used spatial partitioning methods, cluster-based k-means grouping (Brenning 2012), (\"spcv_coords\" mlr3spatiotemporal).\nmethod performs clustering 2D space contrasts commonly used random partitioning non-spatial data.\ngrouping effect train test data separated space conducting random partitioning, thereby reducing effect STAC.contrast, using classical random partitioning approach spatial data, train test observations located side--side across full study area (visual example provided ).\nleads high similarity train test sets, resulting “better” biased performance estimates every fold CV compared spatial CV approach.\nHowever, low error rates mainly caused due STAC observations lack appropriate partitioning methods power fitted model.","code":""},{"path":"special-tasks.html","id":"sp-vs-nsp-cv","chapter":"7 Special Tasks","heading":"7.3.3 Spatial CV vs. Non-Spatial CV","text":"following spatial non-spatial CV conducted showcase mentioned performance differences.performance simple classification tree (\"classif.rpart\") evaluated random partitioning (\"repeated_cv\") four folds two repetitions.\nchosen evaluation measure “classification error” (\"classif.ce\").\ndifference spatial setting \"repeated_spcv_coords\" chosen instead \"repeated_cv\".","code":""},{"path":"special-tasks.html","id":"nsp-cv","chapter":"7 Special Tasks","heading":"7.3.3.1 Non-Spatial CV","text":"","code":"\nlibrary(\"mlr3\")\nlibrary(\"mlr3spatiotempcv\")\nset.seed(42)\n\n# be less verbose\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n\ntask = tsk(\"ecuador\")\n\nlearner = lrn(\"classif.rpart\", maxdepth = 3, predict_type = \"prob\")\nresampling_nsp = rsmp(\"repeated_cv\", folds = 4, repeats = 2)\nrr_nsp = resample(\n  task = task, learner = learner,\n  resampling = resampling_nsp)\n\nrr_nsp$aggregate(measures = msr(\"classif.ce\"))## classif.ce \n##     0.3389"},{"path":"special-tasks.html","id":"sp-cv","chapter":"7 Special Tasks","heading":"7.3.3.2 Spatial CV","text":", classification tree learner around 0.05 percentage points worse using Spatial Cross-Validation (SpCV) compared Non-Spatial Cross-Validation (NSpCV).\nmagnitude difference variable depends dataset, magnitude STAC learner .\nalgorithms higher tendency overfitting training set, difference two methods larger.","code":"\ntask = tsk(\"ecuador\")\n\nlearner = lrn(\"classif.rpart\", maxdepth = 3, predict_type = \"prob\")\nresampling_sp = rsmp(\"repeated_spcv_coords\", folds = 4, repeats = 2)\nrr_sp = resample(\n  task = task, learner = learner,\n  resampling = resampling_sp)\n\nrr_sp$aggregate(measures = msr(\"classif.ce\"))## classif.ce \n##     0.4125"},{"path":"special-tasks.html","id":"vis-spt-partitions","chapter":"7 Special Tasks","heading":"7.3.4 Visualization of Spatiotemporal Partitions","text":"Every partitioning method mlr3spatiotemporal comes generic plot() method visualize created groups.\n2D space happens via ggplot2 spatiotemporal methods 3D visualizations via plotly created.Note setting correct CRS given data important done task creation\nSpatial offsets multiple meters may occur wrong CRS supplied initially.example used already created task via sugar function tsk().\npractice however, one needs create spatiotemporal task via TaskClassifST()/TaskRegrST() set crs argument.spatial grouping k-means based approach contrasts visually ver well compared NSpCV (random) partitioning:","code":"\nautoplot(resampling_sp, task, fold_id = c(1:4), size = 0.7) *\n  ggplot2::scale_y_continuous(breaks = seq(-3.97, -4, -0.01)) *\n  ggplot2::scale_x_continuous(breaks = seq(-79.06, -79.08, -0.01))\nautoplot(resampling_nsp, task, fold_id = c(1:4), size = 0.7) *\n  ggplot2::scale_y_continuous(breaks = seq(-3.97, -4, -0.01)) *\n  ggplot2::scale_x_continuous(breaks = seq(-79.06, -79.08, -0.01))"},{"path":"special-tasks.html","id":"vis-spatial-block","chapter":"7 Special Tasks","heading":"7.3.5 Spatial Block Visualization","text":"spcv-block method makes use rectangular blocks divide study area equally-sized parts.\nblocks can visualized spatial location fold ID get better understanding influenced final partitions.","code":"\ntask = tsk(\"ecuador\")\nresampling = rsmp(\"spcv_block\", range = 1000L)\nresampling$instantiate(task)\n\n## Visualize train/test splits of multiple folds\nautoplot(resampling, task, size = 0.7,\n  fold_id = c(1, 2), show_blocks = TRUE, show_labels = TRUE) *\n  ggplot2::scale_x_continuous(breaks = seq(-79.085, -79.055, 0.01))"},{"path":"special-tasks.html","id":"choose-spt-rsmp","chapter":"7 Special Tasks","heading":"7.3.6 Choosing a Resampling Method","text":"example used \"spcv_coords\" method, mean method best method suitable task.\nEven though method quite popular, mainly chosen clear visual grouping differences compared random partitioning.fact, often multiple spatial partitioning methods can used dataset.\nrecommended (required) users familiarize implemented method decide method choose based specific characteristics dataset.\nalmost methods implemented mlr3spatiotemporal, scientific publication describing strengths weaknesses respective approach (either linked help file mlr3spatiotemporal respective dependency packages).example , cross-validation without hyperparameter tuning shown.\nnested CV desired, recommended use spatial partitioning method inner loop (= tuning level).\nSee Schratz et al. (2019) details chapter 11 Geocomputation R (Lovelace, Nowosad, Muenchow 2019)3.list implemented methods mlr3spatiotemporal can found Getting Started vignette package.want learn even field spatial partitioning, STAC problems associated , work Prof. Hanna Meyer much recommended reference.","code":""},{"path":"special-tasks.html","id":"spatial-prediction","chapter":"7 Special Tasks","heading":"7.3.7 Spatial Prediction","text":"Experimental support spatial prediction terra, raster, stars sf objects available mlr3spatial.package released CRAN, please see package vignettes usage examples.","code":""},{"path":"special-tasks.html","id":"ordinal","chapter":"7 Special Tasks","heading":"7.4 Ordinal Analysis","text":"work progress.\nSee mlr3ordinal current state implementation.","code":""},{"path":"special-tasks.html","id":"functional","chapter":"7 Special Tasks","heading":"7.5 Functional Analysis","text":"Functional data data containing ordering dimensions.\nimplies functional data consists curves varying continuum, time, frequency, wavelength.","code":""},{"path":"special-tasks.html","id":"how-to-model-functional-data","chapter":"7 Special Tasks","heading":"7.5.1 How to model functional data?","text":"two ways model functional data:Modification learner, learner suitable functional dataModification task, task matches standard- classification-learnerThe development started yet, looking contributors.\nOpen issue mlr3fda interested!","code":""},{"path":"special-tasks.html","id":"multilabel","chapter":"7 Special Tasks","heading":"7.6 Multilabel Classification","text":"Multilabel classification deals objects can belong one category time.development started yet, looking contributes.\nOpen issue mlr3multioutput interested!","code":""},{"path":"special-tasks.html","id":"cost-sens","chapter":"7 Special Tasks","heading":"7.7 Cost-Sensitive Classification","text":"regular classification aim minimize misclassification rate thus types misclassification errors deemed equally severe.\ngeneral setting cost-sensitive classification.\nCost sensitive classification assume costs caused different kinds errors equal.\nobjective cost sensitive classification minimize expected costs.Imagine analyst big credit institution.\nLet’s also assume correct decision bank result 35% profit end specific period.\ncorrect decision means bank predicts customer pay bills (hence obtain loan), customer indeed good credit.\nhand, wrong decision means bank predicts customer’s credit good standing, opposite true.\nresult loss 100% given loan.Expressed costs (instead profit), can write cost-matrix follows:exemplary data set problem German Credit task:data 70% customers able pay back credit, 30% bad customers default debt.\nmanager, doesn’t model, decide give either everybody credit give nobody credit.\nresulting costs German credit data :average loan $20,000, credit institute lose one million dollar grant everybody credit:goal find model minimizes costs (thereby maximizes expected profit).","code":"\ncosts = matrix(c(-0.35, 0, 1, 0), nrow = 2)\ndimnames(costs) = list(response = c(\"good\", \"bad\"), truth = c(\"good\", \"bad\"))\nprint(costs)##         truth\n## response  good bad\n##     good -0.35   1\n##     bad   0.00   0\nlibrary(\"mlr3\")\ntask = tsk(\"german_credit\")\ntable(task$truth())## \n## good  bad \n##  700  300\n# nobody:\n(700 * costs[2, 1] + 300 * costs[2, 2]) / 1000## [1] 0\n# everybody\n(700 * costs[1, 1] + 300 * costs[1, 2]) / 1000## [1] 0.055\n# average profit * average loan * number of customers\n0.055 * 20000 * 1000## [1] 1100000"},{"path":"special-tasks.html","id":"a-first-model","chapter":"7 Special Tasks","heading":"7.7.1 A First Model","text":"first model, choose ordinary logistic regression (implemented add-package mlr3learners).\nfirst create classification task, resample model using 10-fold cross validation extract resulting confusion matrix:calculate average costs like , can simply multiply elements confusion matrix elements previously introduced cost matrix, sum values resulting matrix:average loan $20,000, logistic regression yields following costs:Instead losing $1,000,000, credit institute now can expect profit $1,000,000.","code":"\nlibrary(\"mlr3learners\")\nlearner = lrn(\"classif.log_reg\")\nrr = resample(task, learner, rsmp(\"cv\"))\n\nconfusion = rr$prediction()$confusion\nprint(confusion)##         truth\n## response good bad\n##     good  603 153\n##     bad    97 147\navg_costs = sum(confusion * costs) / 1000\nprint(avg_costs)## [1] -0.05805\navg_costs * 20000 * 1000## [1] -1161000"},{"path":"special-tasks.html","id":"cost-sensitive-measure","chapter":"7 Special Tasks","heading":"7.7.2 Cost-sensitive Measure","text":"natural next step improve modeling step order maximize profit.\npurpose first create cost-sensitive classification measure calculates costs based cost matrix.\nallows us conveniently quantify compare modeling decisions.\nFortunately, already predefined measure Measure purpose: MeasureClassifCosts:now call resample() benchmark(), cost-sensitive measures evaluated.\ncompare logistic regression simple featureless learner random forest package ranger :expected, featureless learner performing comparably bad.\nlogistic regression random forest work equally well.","code":"\ncost_measure = msr(\"classif.costs\", costs = costs)\nprint(cost_measure)## <MeasureClassifCosts:classif.costs>\n## * Packages: -\n## * Range: [-Inf, Inf]\n## * Minimize: TRUE\n## * Parameters: normalize=TRUE\n## * Properties: requires_task\n## * Predict type: response\nlearners = list(\n  lrn(\"classif.log_reg\"),\n  lrn(\"classif.featureless\"),\n  lrn(\"classif.ranger\")\n)\ncv3 = rsmp(\"cv\", folds = 3)\nbmr = benchmark(benchmark_grid(task, learners, cv3))\nbmr$aggregate(cost_measure)##    nr      resample_result       task_id          learner_id resampling_id\n## 1:  1 <ResampleResult[20]> german_credit     classif.log_reg            cv\n## 2:  2 <ResampleResult[20]> german_credit classif.featureless            cv\n## 3:  3 <ResampleResult[20]> german_credit      classif.ranger            cv\n##    iters classif.costs\n## 1:     3      -0.06926\n## 2:     3       0.05504\n## 3:     3      -0.05260"},{"path":"special-tasks.html","id":"thresholding","chapter":"7 Special Tasks","heading":"7.7.3 Thresholding","text":"Although now correctly evaluate models cost-sensitive fashion, models unaware classification costs.\nassume costs wrong classification decisions (false positives false negatives).\nlearners natively support cost-sensitive classification (e.g., XXX).\nHowever, concentrate generic approach works models can predict probabilities class labels: thresholding.learners can calculate probability \\(p\\) positive class.\n\\(p\\) exceeds threshold \\(0.5\\), predict positive class, negative class otherwise.binary classification case credit data, primarily want minimize errors model predicts “good”, truth “bad” (.e., number false positives) expensive error.\nnow increase threshold values \\(> 0.5\\), reduce number false negatives.\nNote increase number false positives simultaneously, , words, trading false positives false negatives.Instead manually trying different threshold values, one uses use optimize() find good threshold value w.r.t. performance measure:Note function optimize() intended unimodal functions therefore may converge local optimum .\nSee better alternatives find good threshold values.","code":"\n# fit models with probability prediction\nlearner = lrn(\"classif.log_reg\", predict_type = \"prob\")\nrr = resample(task, learner, rsmp(\"cv\"))\np = rr$prediction()\nprint(p)## <PredictionClassif> for 1000 observations:\n##     row_ids truth response prob.good prob.bad\n##          10   bad      bad    0.2509 0.749051\n##          22  good     good    0.7771 0.222947\n##          25  good     good    0.9976 0.002448\n## ---                                          \n##         966  good     good    0.7834 0.216569\n##         968  good     good    0.5171 0.482902\n##         986  good      bad    0.4357 0.564271\n# helper function to try different threshold values interactively\nwith_threshold = function(p, th) {\n  p$set_threshold(th)\n  list(confusion = p$confusion, costs = p$score(measures = cost_measure, task = task))\n}\n\nwith_threshold(p, 0.5)## $confusion\n##         truth\n## response good bad\n##     good  611 152\n##     bad    89 148\n## \n## $costs\n## classif.costs \n##      -0.06185\nwith_threshold(p, 0.75)## $confusion\n##         truth\n## response good bad\n##     good  477  78\n##     bad   223 222\n## \n## $costs\n## classif.costs \n##      -0.08895\nwith_threshold(p, 1.0)## $confusion\n##         truth\n## response good bad\n##     good    0   1\n##     bad   700 299\n## \n## $costs\n## classif.costs \n##         0.001\n# TODO: include plot of threshold vs performance\n# simple wrapper function which takes a threshold and returns the resulting model performance\n# this wrapper is passed to optimize() to find its minimum for thresholds in [0.5, 1]\nf = function(th) {\n  with_threshold(p, th)$costs\n}\nbest = optimize(f, c(0.5, 1))\nprint(best)## $minimum\n## [1] 0.7601\n## \n## $objective\n## classif.costs \n##       -0.0922\n# optimized confusion matrix:\nwith_threshold(p, best$minimum)$confusion##         truth\n## response good bad\n##     good  472  73\n##     bad   228 227"},{"path":"special-tasks.html","id":"threshold-tuning-1","chapter":"7 Special Tasks","heading":"7.7.4 Threshold Tuning","text":"start, load required packages:","code":"\nlibrary(\"mlr3\")\nlibrary(\"mlr3pipelines\")\nlibrary(\"mlr3tuning\")## Loading required package: paradox"},{"path":"special-tasks.html","id":"adjusting-thresholds-two-strategies","chapter":"7 Special Tasks","heading":"7.7.5 Adjusting thresholds: Two strategies","text":"Currently mlr3pipelines offers two main strategies towards adjusting classification thresholds.\ncan either expose thresholds hyperparameter Learner using PipeOpThreshold.\nallows us tune thresholds via outside optimizer mlr3tuning.Alternatively, can also use PipeOpTuneThreshold automatically tunes threshold learner fit.blog-post, ’ll go strategies.","code":""},{"path":"special-tasks.html","id":"pipeopthreshold","chapter":"7 Special Tasks","heading":"7.7.6 PipeOpThreshold","text":"PipeOpThreshold can put directly Learner.simple example :Note, predict_type = “prob” required po(\"threshold\") effect.thresholds now exposed hyperparameter GraphLearner created:can now tune thresholds outside follows:tuning, define hyperparameters want tune .\nexample, tune thresholds parameter threshold pipeop.\ncan easily imagine, can also jointly tune additional hyperparameters, .e. rpart’s cp parameter.Task aim optimize binary task, can simply specify threshold param:now create AutoTuner, automatically tunes supplied learner ParamSet supplied .Inside trafo, simply collect set params named vector via map_dbl store \nthreshold.thresholds slot expected learner., create AutoTuner, automatically tunes supplied learner ParamSet supplied .One drawback strategy , requires us fit new model new threshold setting.\nsetting threshold computing performance relatively cheap, fitting learner often\ncomputationally demanding.\nbetter strategy therefore often optimize thresholds separately model fit.","code":"\ngr = lrn(\"classif.rpart\", predict_type = \"prob\") %>>% po(\"threshold\")\nl = as_learner(gr)\nl$param_set## <ParamSetCollection>\n##                               id    class lower upper nlevels        default\n##  1:             classif.rpart.cp ParamDbl     0     1     Inf           0.01\n##  2:     classif.rpart.keep_model ParamLgl    NA    NA       2          FALSE\n##  3:     classif.rpart.maxcompete ParamInt     0   Inf     Inf              4\n##  4:       classif.rpart.maxdepth ParamInt     1    30      30             30\n##  5:   classif.rpart.maxsurrogate ParamInt     0   Inf     Inf              5\n##  6:      classif.rpart.minbucket ParamInt     1   Inf     Inf <NoDefault[3]>\n##  7:       classif.rpart.minsplit ParamInt     1   Inf     Inf             20\n##  8: classif.rpart.surrogatestyle ParamInt     0     1       2              0\n##  9:   classif.rpart.usesurrogate ParamInt     0     2       3              2\n## 10:           classif.rpart.xval ParamInt     0   Inf     Inf             10\n## 11:         threshold.thresholds ParamUty    NA    NA     Inf <NoDefault[3]>\n##     value\n##  1:      \n##  2:      \n##  3:      \n##  4:      \n##  5:      \n##  6:      \n##  7:      \n##  8:      \n##  9:      \n## 10:     0\n## 11:   0.5\nlibrary(\"paradox\")\nps = ps(threshold.thresholds = p_dbl(lower = 0, upper = 1))\nat = AutoTuner$new(\n  learner = l,\n  resampling = rsmp(\"cv\", folds = 3L),\n  measure = msr(\"classif.ce\"),\n  search_space = ps,\n  terminator = trm(\"evals\", n_evals = 5L),\n  tuner = tnr(\"random_search\")\n)\n\nat$train(tsk(\"german_credit\"))## INFO  [14:24:30.509] [bbotk] Starting to optimize 1 parameter(s) with '<OptimizerRandomSearch>' and '<TerminatorEvals> [n_evals=5, k=0]' \n## INFO  [14:24:30.551] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:24:30.858] [bbotk] Result of batch 1: \n## INFO  [14:24:30.859] [bbotk]  threshold.thresholds classif.ce runtime_learners \n## INFO  [14:24:30.859] [bbotk]                 0.122      0.292            0.175 \n## INFO  [14:24:30.859] [bbotk]                                 uhash \n## INFO  [14:24:30.859] [bbotk]  fb694b5b-8690-4ffb-8947-d7b898d00bed \n## INFO  [14:24:30.862] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:24:31.167] [bbotk] Result of batch 2: \n## INFO  [14:24:31.168] [bbotk]  threshold.thresholds classif.ce runtime_learners \n## INFO  [14:24:31.168] [bbotk]               0.02976      0.299            0.171 \n## INFO  [14:24:31.168] [bbotk]                                 uhash \n## INFO  [14:24:31.168] [bbotk]  c0df4816-e0f2-43fa-ac44-c5d0ac8fcc6e \n## INFO  [14:24:31.171] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:24:31.481] [bbotk] Result of batch 3: \n## INFO  [14:24:31.483] [bbotk]  threshold.thresholds classif.ce runtime_learners \n## INFO  [14:24:31.483] [bbotk]                0.3881      0.285            0.168 \n## INFO  [14:24:31.483] [bbotk]                                 uhash \n## INFO  [14:24:31.483] [bbotk]  71b897b1-143b-4c5c-9a94-a8aac3ef30a1 \n## INFO  [14:24:31.487] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:24:31.792] [bbotk] Result of batch 4: \n## INFO  [14:24:31.794] [bbotk]  threshold.thresholds classif.ce runtime_learners \n## INFO  [14:24:31.794] [bbotk]                0.3959      0.285             0.17 \n## INFO  [14:24:31.794] [bbotk]                                 uhash \n## INFO  [14:24:31.794] [bbotk]  dd138d46-fbfc-4a65-9f28-af569470c8b2 \n## INFO  [14:24:31.797] [bbotk] Evaluating 1 configuration(s) \n## INFO  [14:24:32.113] [bbotk] Result of batch 5: \n## INFO  [14:24:32.115] [bbotk]  threshold.thresholds classif.ce runtime_learners \n## INFO  [14:24:32.115] [bbotk]                0.2493      0.276             0.18 \n## INFO  [14:24:32.115] [bbotk]                                 uhash \n## INFO  [14:24:32.115] [bbotk]  0fa262f2-ea55-4f8a-84c6-7d381a7f77c2 \n## INFO  [14:24:32.123] [bbotk] Finished optimizing after 5 evaluation(s) \n## INFO  [14:24:32.123] [bbotk] Result: \n## INFO  [14:24:32.124] [bbotk]  threshold.thresholds learner_param_vals  x_domain classif.ce \n## INFO  [14:24:32.124] [bbotk]                0.2493          <list[2]> <list[1]>      0.276"},{"path":"special-tasks.html","id":"pipeoptunethreshold","chapter":"7 Special Tasks","heading":"7.7.7 PipeOpTunethreshold","text":"PipeOpTuneThreshold hand works together PipeOpLearnerCV.\ndirectly optimizes cross-validated predictions made PipeOp.\ndone order avoid -fitting threshold tuning.simple example :Note, predict_type = “prob” required po(\"tunethreshold\") work.\nAdditionally, note time threshold parameter exposed, automatically tuned internally.Note can set rsmp(\"intask\") resampling strategy “learner_cv” order evaluate\npredictions “training” data. generally advised, might lead -fitting\nthresholds can significantly reduce runtime.information, see post Threshold Tuning mlr3 gallery.","code":"\ngr = po(\"learner_cv\", lrn(\"classif.rpart\", predict_type = \"prob\")) %>>% po(\"tunethreshold\")\nl2 = as_learner(gr)\nl2$param_set## <ParamSetCollection>\n##                                         id    class lower upper nlevels\n##  1:        classif.rpart.resampling.method ParamFct    NA    NA       2\n##  2:         classif.rpart.resampling.folds ParamInt     2   Inf     Inf\n##  3: classif.rpart.resampling.keep_response ParamLgl    NA    NA       2\n##  4:                       classif.rpart.cp ParamDbl     0     1     Inf\n##  5:               classif.rpart.keep_model ParamLgl    NA    NA       2\n##  6:               classif.rpart.maxcompete ParamInt     0   Inf     Inf\n##  7:                 classif.rpart.maxdepth ParamInt     1    30      30\n##  8:             classif.rpart.maxsurrogate ParamInt     0   Inf     Inf\n##  9:                classif.rpart.minbucket ParamInt     1   Inf     Inf\n## 10:                 classif.rpart.minsplit ParamInt     1   Inf     Inf\n## 11:           classif.rpart.surrogatestyle ParamInt     0     1       2\n## 12:             classif.rpart.usesurrogate ParamInt     0     2       3\n## 13:                     classif.rpart.xval ParamInt     0   Inf     Inf\n## 14:           classif.rpart.affect_columns ParamUty    NA    NA     Inf\n## 15:                  tunethreshold.measure ParamUty    NA    NA     Inf\n## 16:                tunethreshold.optimizer ParamUty    NA    NA     Inf\n## 17:                tunethreshold.log_level ParamUty    NA    NA     Inf\n##            default      value\n##  1: <NoDefault[3]>         cv\n##  2: <NoDefault[3]>          3\n##  3: <NoDefault[3]>      FALSE\n##  4:           0.01           \n##  5:          FALSE           \n##  6:              4           \n##  7:             30           \n##  8:              5           \n##  9: <NoDefault[3]>           \n## 10:             20           \n## 11:              0           \n## 12:              2           \n## 13:             10          0\n## 14:  <Selector[1]>           \n## 15: <NoDefault[3]> classif.ce\n## 16: <NoDefault[3]>      gensa\n## 17:  <function[1]>       warn"},{"path":"special-tasks.html","id":"cluster","chapter":"7 Special Tasks","heading":"7.8 Cluster Analysis","text":"Cluster analysis type unsupervised machine learning goal group data clusters, cluster contains similar observations.\nsimilarity based specified metrics task application dependent.\nCluster analysis closely related classification sense observation needs assigned cluster class.\nHowever, unlike classification problems observation labeled, clustering works data sets without true labels class assignments.package mlr3cluster extends mlr3 following objects cluster analysis:TaskClust define clustering tasksLearnerClust base class clustering learnersPredictionClust specialized class Prediction objectsMeasureClust specialized class performance measuresSince clustering type unsupervised learning, TaskClust slightly different TaskRegr TaskClassif objects.\nspecifically:truth() function missing observations labeled.target field empty return character(0) accessed anyway.Additionally, LearnerClust provides two extra fields absent supervised learners:assignments returns cluster assignments training data. return NULL accessed training.save_assignments boolean field controls whether store training set assignments learner.Finally, PredictionClust contains additional two fields:partition stores cluster partitions.prob stores cluster probabilities observation.","code":""},{"path":"special-tasks.html","id":"train-and-predict-2","chapter":"7 Special Tasks","heading":"7.8.1 Train and Predict","text":"Clustering learners provide train predict methods.\nanalysis typically consists building clusters using available data.\nconsistent rest library, refer process training.learners can assign new observations existing groups predict.\nHowever, prediction always make sense, case hierarchical clustering.\nhierarchical clustering, goal build hierarchy nested clusters either splitting large clusters smaller ones merging smaller clusters bigger ones.\nfinal result tree dendrogram can change new data point added.\nconsistency rest ecosystem, mlr3cluster offers predict method hierarchical clusterers simply assigns points specified number clusters cutting resulting tree corresponding level.\nMoreover, learners estimate probability observation belonging given cluster.\npredict_types field gives list prediction types learner.training, model field stores learner’s model looks different learner depending underlying library.\npredict returns PredictionClust object gives simplified view learned model.\ndata given predict method one learner trained, predict simply returns cluster assignments “training” observations.\nhand, test set contains new data, predict estimate cluster assignments data set.\nlearners support estimating cluster partitions new data instead return assignments training data print warning message.following example, $k$-means learner applied US arrest data set.\nclass labels predicted contribution task features assignment respective class visualized.","code":"\nlibrary(\"mlr3\")\nlibrary(\"mlr3cluster\")\nlibrary(\"mlr3viz\")\nset.seed(1L)\n\n# create an example task\ntask = tsk(\"usarrests\")\nprint(task)## <TaskClust:usarrests> (50 x 4)\n## * Target: -\n## * Properties: -\n## * Features (4):\n##   - int (2): Assault, UrbanPop\n##   - dbl (2): Murder, Rape\nautoplot(task)\n# create a k-means learner\nlearner = lrn(\"clust.kmeans\")\n\n# assigning each observation to one of the two clusters (default in clust.kmeans)\nlearner$train(task)\nlearner$model## K-means clustering with 2 clusters of sizes 21, 29\n## \n## Cluster means:\n##   Assault Murder  Rape UrbanPop\n## 1   255.0 11.857 28.11    67.62\n## 2   109.8  4.841 16.25    64.03\n## \n## Clustering vector:\n##  [1] 1 1 1 1 1 1 2 1 1 1 2 2 1 2 2 2 2 1 2 1 2 1 2 1 2 2 2 1 2 2 1 1 1 2 2 2 2 2\n## [39] 2 1 2 1 1 2 2 2 2 2 2 2\n## \n## Within cluster sum of squares by cluster:\n## [1] 41637 54762\n##  (between_SS / total_SS =  72.9 %)\n## \n## Available components:\n## \n## [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n## [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\n# make \"predictions\" for the same data\nprediction = learner$predict(task)\nautoplot(prediction, task)"},{"path":"special-tasks.html","id":"measures","chapter":"7 Special Tasks","heading":"7.8.2 Measures","text":"difference supervised unsupervised learning ground truth data unsupervised learning.\nsupervised setting, classification, need compare predictions true labels.\nSince clustering example unsupervised learning, true labels can compare.\nHowever, can still measure quality cluster assignments quantifying closely objects within cluster related (cluster cohesion) well distinct different clusters (cluster separation).assess quality clustering, built-evaluation metrics available.\nOne within sum squares (WSS) calculates sum squared differences observations centroids.\nWSS useful quantifies cluster cohesion.\nrange measure \\([0, \\infty)\\) smaller value means clusters compact.Another measure silhouette quality index quantifies well point belongs assigned cluster versus neighboring cluster.\nSilhouette values \\([-1, 1]\\) range.Points silhouette closer :1 well clustered0 lie two clusters-1 likely placed wrong clusterThe following example conducting benchmark experiment various learners iris data set without target variable assessing quality learner within sum squares silhouette measures.experiment shows using k-means algorithm three centers produces better within sum squares score learner considered. However, pam (partitioning around medoids) learner two clusters performs best considering silhouette measure takes account cluster cohesion separation.","code":"\ndesign = benchmark_grid(\n  tasks = TaskClust$new(\"iris\", iris[-5]),\n  learners = list(\n    lrn(\"clust.kmeans\", centers = 3L),\n    lrn(\"clust.pam\", k = 2L),\n    lrn(\"clust.cmeans\", centers = 3L)),\n  resamplings = rsmp(\"insample\"))\nprint(design)##               task                  learner               resampling\n## 1: <TaskClust[43]> <LearnerClustKMeans[36]> <ResamplingInsample[19]>\n## 2: <TaskClust[43]>    <LearnerClustPAM[36]> <ResamplingInsample[19]>\n## 3: <TaskClust[43]> <LearnerClustCMeans[36]> <ResamplingInsample[19]>\n# execute benchmark\nbmr = benchmark(design)\n\n# define measure\nmeasures = list(msr(\"clust.wss\"), msr(\"clust.silhouette\"))\nbmr$aggregate(measures)##    nr      resample_result task_id   learner_id resampling_id iters clust.wss\n## 1:  1 <ResampleResult[20]>    iris clust.kmeans      insample     1     78.85\n## 2:  2 <ResampleResult[20]>    iris    clust.pam      insample     1    153.33\n## 3:  3 <ResampleResult[20]>    iris clust.cmeans      insample     1     79.03\n##    clust.silhouette\n## 1:           0.5555\n## 2:           0.7158\n## 3:           0.5493"},{"path":"special-tasks.html","id":"visualization","chapter":"7 Special Tasks","heading":"7.8.3 Visualization","text":"Cluster analysis mlr3 integrated mlr3viz provides number useful plots. plots shown .Silhouette plots can help visually assess quality analysis help choose number clusters given data set.\nred dotted line shows mean silhouette value bar represents data point.\npoints cluster index around higher mean silhouette, number clusters chosen well.plot shows points cluster 5 almost points clusters 4, 2 1 average silhouette index.\nmeans lot observations lie either border clusters likely assigned wrong cluster.Setting number centers two improves average silhouette score well overall quality clustering almost points cluster 1 higher lot points cluster 2 close mean silhouette.\nHence, two centers might better choice number clusters.","code":"\ntask = TaskClust$new(\"iris\", iris[-5])\nlearner = lrn(\"clust.kmeans\")\nlearner$train(task)\nprediction = learner$predict(task)\n\n# performing PCA on task and showing assignments\nautoplot(prediction, task, type = \"pca\")\n# same as above but with probability ellipse that assumes normal distribution\nautoplot(prediction, task, type = \"pca\", frame = TRUE, frame.type = 'norm')\ntask = tsk(\"usarrests\")\nlearner = lrn(\"clust.agnes\")\nlearner$train(task)\n\n# dendrogram for hierarchical clustering\nautoplot(learner)\n# advanced dendrogram options from `factoextra::fviz_dend`\nautoplot(learner,\n  k = learner$param_set$values$k, rect_fill = TRUE,\n  rect = TRUE, rect_border = c(\"red\", \"cyan\"))\n# silhouette plot allows to visually inspect the quality of clustering\ntask = TaskClust$new(\"iris\", iris[-5])\nlearner = lrn(\"clust.kmeans\")\nlearner$param_set$values = list(centers = 5L)\nlearner$train(task)\nprediction = learner$predict(task)\nautoplot(prediction, task, type = \"sil\")\nlearner = lrn(\"clust.kmeans\")\nlearner$param_set$values = list(centers = 2L)\nlearner$train(task)\nprediction = learner$predict(task)\nautoplot(prediction, task, type = \"sil\")"},{"path":"interpretation.html","id":"interpretation","chapter":"8 Model Interpretation","heading":"8 Model Interpretation","text":"principle, generic frameworks model interpretation applicable models fitted mlr3 just extracting fitted models Learner objects.However, two popular frameworks,iml Subsection 8.1,DALEX Subsection 8.2, andadditionally come convenience mlr3.","code":""},{"path":"interpretation.html","id":"iml","chapter":"8 Model Interpretation","heading":"8.1 IML","text":"Author: Shawn Stormiml R package interprets behavior explains predictions machine learning models. functions provided iml package model-agnostic gives flexibility use machine learning model.chapter provides examples use iml mlr3. information refer IML github IML book","code":""},{"path":"interpretation.html","id":"penguin-task","chapter":"8 Model Interpretation","heading":"8.1.1 Penguin Task","text":"understand iml can offer, start thorough example. goal example figure species penguins given set features. palmerpenguins::penguins data set used alternative iris data set.\npenguins data sets contains 8 variables 344 penguins:get started run:penguins = na.omit(penguins) omit 11 cases missing values.\nomitted, error running learner data points N/features.explained Section 2.3, specific learners can queried mlr_learners.\nSection 2.5 recommended classifiers use predict_type prob instead directly predicting label.\ndone example.\npenguins[(names(penguins) != \"species\")] data features y penguinsspecies.\nlearner$train(task_peng) trains model learner$model stores model training command.\nPredictor holds machine learning model data.\ninterpretation methods iml need machine learning model data wrapped Predictor object.Next core functionality iml. example three separate interpretation methods used: FeatureEffects, FeatureImp ShapleyFeatureEffects computes effects given features model prediction. Different methods implemented: Accumulated Local Effect (ALE) plots, Partial Dependence Plots (PDPs) Individual Conditional Expectation (ICE) curves.FeatureEffects computes effects given features model prediction. Different methods implemented: Accumulated Local Effect (ALE) plots, Partial Dependence Plots (PDPs) Individual Conditional Expectation (ICE) curves.Shapley computes feature contributions single predictions Shapley value – approach cooperative game theory (Shapley Value).Shapley computes feature contributions single predictions Shapley value – approach cooperative game theory (Shapley Value).FeatureImp computes importance features calculating increase model’s prediction error permuting feature ().FeatureImp computes importance features calculating increase model’s prediction error permuting feature ().","code":"\ndata(\"penguins\", package = \"palmerpenguins\")\nstr(penguins)## tibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n##  $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n##  $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n##  $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n##  $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n##  $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n##  $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n##  $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\nlibrary(\"iml\")\nlibrary(\"mlr3\")\nlibrary(\"mlr3learners\")\nset.seed(1)\npenguins = na.omit(penguins)\ntask_peng = as_task_classif(penguins, target = \"species\")\nlearner = lrn(\"classif.ranger\")\nlearner$predict_type = \"prob\"\nlearner$train(task_peng)\nlearner$model## Ranger result\n## \n## Call:\n##  ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      probability = self$predict_type == \"prob\", case.weights = task$weights$weight,      num.threads = 1L) \n## \n## Type:                             Probability estimation \n## Number of trees:                  500 \n## Sample size:                      333 \n## Number of independent variables:  7 \n## Mtry:                             2 \n## Target node size:                 10 \n## Variable importance mode:         none \n## Splitrule:                        gini \n## OOB prediction error (Brier s.):  0.0179\nx = penguins[which(names(penguins) != \"species\")]\nmodel = Predictor$new(learner, data = x, y = penguins$species)"},{"path":"interpretation.html","id":"featureeffects","chapter":"8 Model Interpretation","heading":"8.1.2 FeatureEffects","text":"addition commands following two need ran:\nFigure 8.1: Plot results FeatureEffects. FeatureEffects computes plots feature effects prediction models\neffect stores object FeatureEffect computation results can plotted. example, features provided penguins data set used.features except year provide meaningful interpretable information. clear year doesn’t provide anything significance. bill_length_mm shows example bill length smaller roughly 40mm, high chance penguin Adelie.","code":"\nnum_features = c(\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\", \"year\")\neffect = FeatureEffects$new(model)\nplot(effect, features = num_features)"},{"path":"interpretation.html","id":"shapley","chapter":"8 Model Interpretation","heading":"8.1.3 Shapley","text":"\nFigure 8.2: Plot results Shapley. \\(\\phi\\) gives increase decrease probability given values vertical axis\n\\(\\phi\\) provides insight probability given values vertical axis. example, penguin less likely Gentoo bill_depth=18.7 much likely Adelie Chinstrap.","code":"\nx = penguins[which(names(penguins) != \"species\")]\nmodel = Predictor$new(learner, data = penguins, y = \"species\")\nx.interest = data.frame(penguins[1, ])\nshapley = Shapley$new(model, x.interest = x.interest)\nplot(shapley)"},{"path":"interpretation.html","id":"featureimp","chapter":"8 Model Interpretation","heading":"8.1.4 FeatureImp","text":"\nFigure 8.3: Plot results FeatureImp. FeatureImp visualizes importance features given prediction model\nFeatureImp shows level importance features classifying penguins. clear see bill_length_mm high importance one concentrate different boundaries feature attempting classify three species.","code":"\neffect = FeatureImp$new(model, loss = \"ce\")\neffect$plot(features = num_features)"},{"path":"interpretation.html","id":"independent-test-data","chapter":"8 Model Interpretation","heading":"8.1.5 Independent Test Data","text":"also interesting see well model performs test data set. section, exactly recommended Section 2.4, 80% penguin data set used training set 20% test set:First, compare feature importance training test set\nFigure 8.4: FeatImp train (left) test (right)\nresults train set FeatureImp similar, expected.\nfollow similar approach compare feature effects:\nFigure 8.5: FeatEffect train data set\n\nFigure 8.6: FeatEffect test data set\ncase FeatureImp, test data results show either - underestimate feature importance / feature effects compared results entire penguin data set used.\ngood opportunity reader attempt resolve estimation playing amount features amount data used test train data sets FeatureImp FeatureEffects.\nsure change line train_set = sample(task_peng$nrow, 0.8 * task_peng$nrow) randomly sample data .","code":"\ntrain_set = sample(task_peng$nrow, 0.8 * task_peng$nrow)\ntest_set = setdiff(seq_len(task_peng$nrow), train_set)\nlearner$train(task_peng, row_ids = train_set)\nprediction = learner$predict(task_peng, row_ids = test_set)\n# plot on training\nmodel = Predictor$new(learner, data = penguins[train_set,], y = \"species\")\neffect = FeatureImp$new(model, loss = \"ce\" )\nplot_train = plot(effect, features = num_features)\n\n# plot on test data\nmodel = Predictor$new(learner, data = penguins[test_set, ], y = \"species\")\neffect = FeatureImp$new(model, loss = \"ce\" )\nplot_test = plot(effect, features = num_features)\n\n# combine into single plot\nlibrary(\"patchwork\")\nplot_train + plot_test\nmodel = Predictor$new(learner, data = penguins[train_set,], y = \"species\")\neffect = FeatureEffects$new(model)\nplot(effect, features = num_features)\nmodel = Predictor$new(learner, data = penguins[test_set,], y = \"species\")\neffect = FeatureEffects$new(model)\nplot(effect, features = num_features)"},{"path":"interpretation.html","id":"dalex","chapter":"8 Model Interpretation","heading":"8.2 DALEX","text":"Authors: Przemysław Biecek, Szymon Maksymiuk","code":""},{"path":"interpretation.html","id":"interpretability-dalex-introduction","chapter":"8 Model Interpretation","heading":"8.2.1 Introduction","text":"DALEX package X-rays predictive model helps explore, explain visualize behaviour. package implements collection methods Explanatory Model Analysis. based unified grammar summarised Figure 8.7.following sections, present subsequent methods available DALEX package based random forest model trained football players worth prediction FIFA 20 data. show methods analyzing model level single prediction global level - whole data set.structure chapter following:Section 8.2.2 introduce FIFA 20 dataset section 8.2.3 train random regression forest using ranger package.Section 8.2.4 introduces general logic beyond DALEX explainers.Section 8.2.5 introduces methods dataset level model exploration.Section 8.2.6 introduces methods instance-level model exploration.\nFigure 8.7: Taxonomy methods model exploration presented chapter. Left part overview methods instance level exploration right part related dataset level model exploration.\n","code":""},{"path":"interpretation.html","id":"interpretability-data-fifa","chapter":"8 Model Interpretation","heading":"8.2.2 Read data: FIFA","text":"Examples presented chapter based data retrieved FIFA video game. use data scrapped sofifa website. raw data available kaggle. basic data cleaning, processed data top 5000 football players available DALEX package name fifa.every player, 42 features available.table overview 42 features three selected players.\nOne features, called value_eur, worth footballer euros. next section, build prediction model, estimate worth player based player characteristics.order get stable model remove four variables .e. nationality, overall, potential, wage_eur.","code":"\nlibrary(\"DALEX\")\nfifa[1:2,c(\"value_eur\", \"age\", \"height_cm\", \"nationality\", \"attacking_crossing\")]##                   value_eur age height_cm nationality attacking_crossing\n## L. Messi           95500000  32       170   Argentina                 88\n## Cristiano Ronaldo  58500000  34       187    Portugal                 84\ndim(fifa)## [1] 5000   42\nfifa[,c('nationality', 'overall', 'potential', 'wage_eur')] <- NULL\nfor (i in 1:ncol(fifa))           fifa[,i] <- as.numeric(fifa[,i])"},{"path":"interpretation.html","id":"interpretability-train-ranger","chapter":"8 Model Interpretation","heading":"8.2.3 Train a model: Ranger","text":"DALEX package works model regardless internal structure. Examples package works shown random forest model implemented ranger package.use mlr3 package build predictive model.\nFirst, let’s load required packages.can define regression task - prediction value_eur variable:Finally, train mlr3’s ranger learner 250 trees. Note example brevity split data train/test data. model built whole data.","code":"\nlibrary(\"mlr3\")\nlibrary(\"mlr3learners\")\nfifa_task <- as_task_regr(fifa, target = \"value_eur\")\nfifa_ranger <- lrn(\"regr.ranger\")\nfifa_ranger$param_set$values <- list(num.trees = 250)\nfifa_ranger$train(fifa_task)\nfifa_ranger## <LearnerRegrRanger:regr.ranger>\n## * Model: ranger\n## * Parameters: num.trees=250\n## * Packages: ranger\n## * Predict Type: response\n## * Feature types: logical, integer, numeric, character, factor, ordered\n## * Properties: importance, oob_error, weights"},{"path":"interpretation.html","id":"interpretability-architecture","chapter":"8 Model Interpretation","heading":"8.2.4 The general workflow","text":"Working explanations DALEX package always consists three steps schematically shown pipe .functions DALEX package can work models structure. possible first step create adapter allows downstream functions access model consistent fashion. general, adapter created DALEX::explain.default() function, models created mlr3 package convenient use DALEXtra::explain_mlr3().functions DALEX package can work models structure. possible first step create adapter allows downstream functions access model consistent fashion. general, adapter created DALEX::explain.default() function, models created mlr3 package convenient use DALEXtra::explain_mlr3().Explanations determined functions DALEX::model_parts(), DALEX::model_profile(), DALEX::predict_parts() DALEX::predict_profile(). functions takes model adapter first argument. arguments describe function works. present following section.Explanations determined functions DALEX::model_parts(), DALEX::model_profile(), DALEX::predict_parts() DALEX::predict_profile(). functions takes model adapter first argument. arguments describe function works. present following section.Explanations can visualized generic function plot summarised generic function print(). explanation data frame additional class attribute. plot function creates graphs using ggplot2 package, can easily modified usual ggplot2 decorators.Explanations can visualized generic function plot summarised generic function print(). explanation data frame additional class attribute. plot function creates graphs using ggplot2 package, can easily modified usual ggplot2 decorators.show cascade functions based FIFA example.get started exploration model behaviour need create explainer. DALEX::explain.default function handles types predictive models. DALEXtra package generic versions common ML frameworks. Among DALEXtra::explain_mlr3() function works mlr3 models.function performs series internal checks output bit verbose. Turn verbose = FALSE argument make less wordy.","code":"model %>%\n  explain_mlr3(data = ..., y = ..., label = ...) %>%\n  model_parts() %>%\n  plot()\nlibrary(\"DALEX\")\nlibrary(\"DALEXtra\")\n\nranger_exp <- explain_mlr3(fifa_ranger,\n        data     = fifa,\n        y        = fifa$value_eur,\n        label    = \"Ranger RF\",\n        colorize = FALSE)## Preparation of a new explainer is initiated\n##   -> model label       :  Ranger RF \n##   -> data              :  5000  rows  38  cols \n##   -> target variable   :  5000  values \n##   -> predict function  :  yhat.LearnerRegr  will be used (  default  )\n##   -> predicted values  :  No value for predict function target column. (  default  )\n##   -> model_info        :  package mlr3 , ver. 0.12.0 , task regression (  default  ) \n##   -> predicted values  :  numerical, min =  466803 , mean =  7472317 , max =  88264233  \n##   -> residual function :  difference between y and yhat (  default  )\n##   -> residuals         :  numerical, min =  -8558527 , mean =  970.2 , max =  17805600  \n##   A new explainer has been created!"},{"path":"interpretation.html","id":"interpretability-dataset-level","chapter":"8 Model Interpretation","heading":"8.2.5 Dataset level exploration","text":"DALEX::model_parts() function calculates importance variables using permutations based importance.Results can visualized generic plot(). chart 38 variables unreadable, max_vars argument, limit number variables plot.know variables important, can use Partial Dependence Plots show model, average, changes changes selected variables. example, show average relation particular variables players’ value., result explanation can presented generic function plot().general trend player characteristics . higher skills higher player’s worth. single exception – variable Age.","code":"\nfifa_vi <- model_parts(ranger_exp)\nhead(fifa_vi)##              variable mean_dropout_loss     label\n## 1        _full_model_           1416582 Ranger RF\n## 2           value_eur           1416582 Ranger RF\n## 3           height_cm           1476876 Ranger RF\n## 4 goalkeeping_kicking           1477638 Ranger RF\n## 5           weight_kg           1482370 Ranger RF\n## 6    movement_balance           1486915 Ranger RF\nplot(fifa_vi, max_vars = 12, show_boxplots = FALSE)\nselected_variables <- c(\"age\", \"movement_reactions\",\n                \"skill_ball_control\", \"skill_dribbling\")\n\nfifa_pd <- model_profile(ranger_exp,\n                variables = selected_variables)$agr_profiles\nfifa_pd## Top profiles    : \n##              _vname_   _label_ _x_  _yhat_ _ids_\n## 1 skill_ball_control Ranger RF   5 6426909     0\n## 2    skill_dribbling Ranger RF   7 6671599     0\n## 3    skill_dribbling Ranger RF  11 6662567     0\n## 4    skill_dribbling Ranger RF  12 6650203     0\n## 5    skill_dribbling Ranger RF  13 6649258     0\n## 6    skill_dribbling Ranger RF  14 6649165     0\nlibrary(\"ggplot2\")\nplot(fifa_pd) +\n  scale_y_continuous(\"Estimated value in Euro\", labels = scales::dollar_format(suffix = \"€\", prefix = \"\")) +\n  ggtitle(\"Partial Dependence profiles for selected variables\")"},{"path":"interpretation.html","id":"interpretability-instance-level","chapter":"8 Model Interpretation","heading":"8.2.6 Instance level explanation","text":"Time see model behaves single observation/player\ncan done player, example use Cristiano Ronaldo.function predict_parts instance-level version model_parts function introduced previous section. background behind method see Introduction Break .generic plot() function shows estimated contribution variables final prediction.Cristiano striker, therefore characteristics influence worth related attack, like attacking_volleys skill_dribbling. variable negative attribution age.Another way inspect local behaviour model use SHapley Additive exPlanations (SHAP). locally shows contribution variables single observation, just like Break .previous section, ’ve introduced global explanation - Partial Dependence Plots. Ceteris Paribus instance level version plot. shows response model observation change one variable others stay unchanged. Blue dot stands original value.","code":"\nronaldo <- fifa[\"Cristiano Ronaldo\",]\nronaldo_bd_ranger <- predict_parts(ranger_exp,\n                        new_observation = ronaldo)\nhead(ronaldo_bd_ranger)##                                       contribution\n## Ranger RF: intercept                       7472317\n## Ranger RF: movement_reactions = 96        12189418\n## Ranger RF: skill_ball_control = 92         5463949\n## Ranger RF: attacking_finishing = 94        4643519\n## Ranger RF: mentality_positioning = 95      5531779\n## Ranger RF: skill_dribbling = 89            4102550\nplot(ronaldo_bd_ranger)\nronaldo_shap_ranger <- predict_parts(ranger_exp,\n                        new_observation = ronaldo,\n                        type = \"shap\")\n\nplot(ronaldo_shap_ranger) +\n  scale_y_continuous(\"Estimated value in Euro\", labels = scales::dollar_format(suffix = \"€\", prefix = \"\"))\nselected_variables <- c(\"age\", \"movement_reactions\",\n                \"skill_ball_control\", \"skill_dribbling\")\n\nronaldo_cp_ranger <- predict_profile(ranger_exp, ronaldo, variables = selected_variables)\n\nplot(ronaldo_cp_ranger, variables = selected_variables) +\n  scale_y_continuous(\"Estimated value of Christiano Ronaldo\", labels = scales::dollar_format(suffix = \"€\", prefix = \"\"))"},{"path":"appendix.html","id":"appendix","chapter":"9 Appendix","heading":"9 Appendix","text":"","code":""},{"path":"appendix.html","id":"list-learners","chapter":"9 Appendix","heading":"9.1 Integrated Learners","text":"Learners available one following packages:mlr3: debug learner rpart learners.mlr3learners: opinionated selection default learners.mlr3proba: base learners survival probabilistic regression.mlr3cluster: learners unsupervised clustering.mlr3extralearners: experimental learners regression, classification survival.Use interactive search table look learners.","code":""},{"path":"appendix.html","id":"list-measures","chapter":"9 Appendix","heading":"9.2 Integrated Performance Measures","text":"Also see overview website mlr3measures.","code":""},{"path":"appendix.html","id":"list-filters","chapter":"9 Appendix","heading":"9.3 Integrated Filter Methods","text":"","code":""},{"path":"appendix.html","id":"fs-filter-list","chapter":"9 Appendix","heading":"9.3.1 Standalone filter methods","text":"","code":""},{"path":"appendix.html","id":"fs-filter-embedded-list","chapter":"9 Appendix","heading":"9.3.2 Learners With Embedded Filter Methods","text":"","code":"##  [1] \"classif.featureless\" \"classif.ranger\"      \"classif.rpart\"      \n##  [4] \"classif.xgboost\"     \"regr.featureless\"    \"regr.ranger\"        \n##  [7] \"regr.rpart\"          \"regr.xgboost\"        \"surv.ranger\"        \n## [10] \"surv.rpart\"          \"surv.xgboost\""},{"path":"appendix.html","id":"list-pipeops","chapter":"9 Appendix","heading":"9.4 Integrated Pipe Operators","text":"","code":""},{"path":"appendix.html","id":"compare-frameworks","chapter":"9 Appendix","heading":"9.5 Framework Comparison","text":", collected examples, mlr3pipelines compared different software packages,\nmlr, recipes sklearn.diving deeper, give short introduction PipeOps.","code":""},{"path":"appendix.html","id":"an-introduction-to-pipeops","chapter":"9 Appendix","heading":"9.5.1 An introduction to “PipeOp”s","text":"example, create linear Pipeline.\nscaling input features, rotate data using principal component analysis.\ntransformation, use simple Decision Tree learner classification.exemplary data, use “iris” classification task.\nobject contains famous iris dataset meta-information, target variable.quickly split data train test set:Pipeline (Graph) contains multiple pipeline operators (“PipeOp”s), PipeOp transforms data flows .\nuse case, require 3 transformations:PipeOp scales dataA PipeOp performs PCAA PipeOp contains Decision Tree learnerA list available PipeOps can obtained fromFirst define required PipeOps:","code":"\nlibrary(\"mlr3\")\ntask = tsk(\"iris\")\ntest.idx = sample(seq_len(task$nrow), 30)\ntrain.idx = setdiff(seq_len(task$nrow), test.idx)\n# Set task to only use train indexes\ntask$row_roles$use = train.idx\nlibrary(\"mlr3pipelines\")\npo()## <DictionaryPipeOp> with 64 stored values\n## Keys: boxcox, branch, chunk, classbalancing, classifavg, classweights,\n##   colapply, collapsefactors, colroles, copy, datefeatures, encode,\n##   encodeimpact, encodelmer, featureunion, filter, fixfactors, histbin,\n##   ica, imputeconstant, imputehist, imputelearner, imputemean,\n##   imputemedian, imputemode, imputeoor, imputesample, kernelpca,\n##   learner, learner_cv, missind, modelmatrix, multiplicityexply,\n##   multiplicityimply, mutate, nmf, nop, ovrsplit, ovrunite, pca, proxy,\n##   quantilebin, randomprojection, randomresponse, regravg,\n##   removeconstants, renamecolumns, replicate, scale, scalemaxabs,\n##   scalerange, select, smote, spatialsign, subsample, targetinvert,\n##   targetmutate, targettrafoscalerange, textvectorizer, threshold,\n##   tunethreshold, unbranch, vtreat, yeojohnson\nop1 = po(\"scale\")\nop2 = po(\"pca\")\nop3 = po(\"learner\", learner = lrn(\"classif.rpart\"))"},{"path":"appendix.html","id":"a-quick-glance-into-a-pipeop","chapter":"9 Appendix","heading":"9.5.1.1 A quick glance into a PipeOp","text":"order get better understanding respective PipeOps , quickly look one detail:important slots PipeOp :$train(): function used train PipeOp.$predict(): function used predict PipeOp.$train() $predict() functions define core functionality PipeOp.\nmany cases, order leak information training set test set imperative treat train test data separately.\nrequire $train() function learns appropriate transformations training set $predict() function applies transformation future data.case PipeOpPCA means following:$train() learns rotation matrix input saves matrix additional slot, $state.\nreturns rotated input data stored new Task.$predict() uses rotation matrix stored $state order rotate future, unseen data.\nreturns new Task.","code":""},{"path":"appendix.html","id":"constructing-the-pipeline","chapter":"9 Appendix","heading":"9.5.1.2 Constructing the Pipeline","text":"can now connect PipeOps constructed earlier Pipeline.\ncan using %>>% operator.result operation “Graph”.\nGraph connects input output PipeOp following PipeOp.\nallows us specify linear processing pipelines.\ncase, connect output scaling PipeOp input PCA PipeOp output PCA PipeOp input PipeOpLearner.can now train Graph using iris Task.now train graph, data flows graph follows:Task flows PipeOpScale.\nPipeOp scales column data contained Task returns new Task contains scaled data output.scaled Task flows PipeOpPCA.\nPCA transforms data returns (possibly smaller) Task, contains transformed data.transformed data flows learner, case classif.rpart.\nused train learner, result saves model can used predict new data.order predict new data, need save relevant transformations data went training.\nresult, PipeOp saves state, information required appropriately transform future data stored.\ncase, mean standard deviation column PipeOpScale, PCA rotation matrix PipeOpPCA learned model PipeOpLearner.","code":"\nlinear_pipeline = op1 %>>% op2 %>>% op3\nlinear_pipeline$train(task)## $classif.rpart.output\n## NULL\n# predict on test.idx\ntask$row_roles$use = test.idx\nlinear_pipeline$predict(task)## $classif.rpart.output\n## <PredictionClassif> for 30 observations:\n##     row_ids      truth   response\n##          50     setosa     setosa\n##          26     setosa     setosa\n##         116  virginica  virginica\n## ---                              \n##          57 versicolor versicolor\n##          24     setosa     setosa\n##         110  virginica  virginica"},{"path":"appendix.html","id":"mlr3pipelines-vs.-mlr","chapter":"9 Appendix","heading":"9.5.2 mlr3pipelines vs. mlr","text":"order showcase benefits mlr3pipelines mlr’s Wrapper mechanism, compare case imputing missing values filtering top 2 features applying learner.mlr wrappers generally less verbose require little less code, heavily inhibits flexibility.\nexample, wrappers can generally process data parallel.","code":""},{"path":"appendix.html","id":"mlr","chapter":"9 Appendix","heading":"9.5.2.1 mlr","text":"","code":"\nlibrary(\"mlr\")\n# We first create a learner\nlrn = makeLearner(\"classif.rpart\")\n# Wrap this learner in a FilterWrapper\nlrn.wrp = makeFilterWrapper(lrn, fw.abs = 2L)\n# And wrap the resulting wrapped learner into an ImputeWrapper.\nlrn.wrp = makeImputeWrapper(lrn.wrp, classes = list(factor = imputeConstant(\"missing\")))\n\n# Afterwards, we can train the resulting learner on a task\ntrain(lrn, iris.task)"},{"path":"appendix.html","id":"mlr3pipelines","chapter":"9 Appendix","heading":"9.5.2.2 mlr3pipelines","text":"fact mlr’s wrappers applied inside-, .e. reverse order often confusing.\nway straight-forward mlr3pipelines, simply chain different methods using %>>%.\nAdditionally, mlr3pipelines offers way greater possibilities respect kinds Pipelines can constructed.\nmlr3pipelines, allow construction parallel conditional pipelines.\npreviously possible.","code":"\nlibrary(\"mlr3\")\nlibrary(\"mlr3pipelines\")\nlibrary(\"mlr3filters\")\n\nimpute = po(\"imputeoor\")\nfilter = po(\"filter\", filter = flt(\"variance\"), filter.nfeat = 2L)\nrpart = po(\"learner\", lrn(\"classif.rpart\"))\n\n# Assemble the Pipeline\npipeline = impute %>>% filter %>>% rpart\n# And convert to a 'GraphLearner'\nlearner = as_learner(pipeline)"},{"path":"appendix.html","id":"mlr3pipelines-vs.-sklearn.pipeline.pipeline","chapter":"9 Appendix","heading":"9.5.3 mlr3pipelines vs. sklearn.pipeline.Pipeline","text":"order broaden horizon, compare Python sklearn’s Pipeline methods.\nsklearn.pipeline.Pipeline sequentially applies list transforms fitting final estimator.\nIntermediate steps pipeline transforms, .e. steps can learn data, also transform data flows .\npurpose pipeline assemble several steps can cross-validated together setting different parameters.\n, enables setting parameters various steps.thus conceptually similar mlr3pipelines.\nSimilarly mlr3pipelines, can tune full Pipeline using various tuning methods.\nPipeline mainly supports linear pipelines.\nmeans, can execute parallel steps, example Bagging, support conditional execution, .e. PipeOpBranch.\ntime, different transforms pipeline can cached, makes tuning configuration space Pipeline efficient, executing steps multiple times can avoided.compare functionality available mlr3pipelines sklearn.pipeline.Pipeline give comparison.following example obtained sklearn documentation showcases Pipeline first Selects feature performs PCA original data, concatenates resulting datasets applies Support Vector Machine.","code":""},{"path":"appendix.html","id":"sklearn","chapter":"9 Appendix","heading":"9.5.3.1 sklearn","text":"","code":"from sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n\niris = load_iris()\n\nX, y = iris.data, iris.target\n\n# This dataset is way too high-dimensional. Better do PCA:\npca = PCA(n_components=2)\n\n# Maybe some original features where good, too?\nselection = SelectKBest(k=1)\n\n# Build estimator from PCA and Univariate selection:\ncombined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n\n# Use combined features to transform dataset:\nX_features = combined_features.fit(X, y).transform(X)\n\nsvm = SVC(kernel=\"linear\")\n\n# Do grid search over k, n_components and C:\npipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])\n\nparam_grid = dict(features__pca__n_components=[1, 2, 3],\n                  features__univ_select__k=[1, 2],\n                  svm__C=[0.1, 1, 10])\n\ngrid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, verbose=10)\ngrid_search.fit(X, y)"},{"path":"appendix.html","id":"mlr3pipelines-1","chapter":"9 Appendix","heading":"9.5.3.2 mlr3pipelines","text":"summary, can achieve similar results comparable number lines, time offering greater flexibility respect kinds pipelines want optimize .\ntime, experiments using mlr3 can now arbitrarily parallelized using futures.","code":"library(\"mlr3verse\")\niris = tsk(\"iris\")\n\n# Build the steps\ncopy = po(\"copy\", 2)\npca = po(\"pca\")\nselection = po(\"filter\", filter = flt(\"variance\"))\nunion = po(\"featureunion\", 2)\nsvm = po(\"learner\", lrn(\"classif.svm\", kernel = \"linear\", type = \"C-classification\"))\n\n# Assemble the Pipeline\npipeline = copy %>>% gunion(list(pca, selection)) %>>% union %>>% svm\nlearner = as_learner(pipeline)\n\n# For tuning, we define the resampling and the Parameter Space\nresampling = rsmp(\"cv\", folds = 5L)\n\nlibrary(\"paradox\")\nsearch_space = ps(\n  classif.svm.cost = p_dbl(lower = 0.1, upper = 1),\n  pca.rank. = p_int(lower = 1, upper = 3),\n  variance.filter.nfeat = p_int(lower = 1, upper = 2)\n)\n\ninstance = TuningInstanceSingleCrit$new(\n  task = iris,\n  learner = learner,\n  resampling = resampling,\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"none\"),\n  search_space = search_space\n)\n\ntuner = tnr(\"grid_search\", resolution = 10)\ntuner$optimize(instance)\n\nSet the learner to the optimal values and train\nlearner$param_set$values = instance$result_learner_param_vals"},{"path":"appendix.html","id":"mlr3pipelines-vs-recipes","chapter":"9 Appendix","heading":"9.5.4 mlr3pipelines vs recipes","text":"recipes new package, covers applications steps mlr3pipelines.\npackages feature possibility connect different pre- post-processing methods using pipe-operator.\nrecipes package tightly integrates tidymodels ecosystem, much functionality integrated can used recipes.\ncompare recipes mlr3pipelines using example recipes vignette.aim analysis predict whether customers pay back loans given information customers.\norder , build model following:first imputes missing values using k-nearest neighborsAll factor variables converted numerics using dummy encodingThe data first centered scaled.order validate algorithm, data first split train test set using initial_split, training, testing.\nrecipe trained train data (see steps ) applied test data.","code":""},{"path":"appendix.html","id":"recipes","chapter":"9 Appendix","heading":"9.5.4.1 recipes","text":"Afterwards, transformed data can used train predict:","code":"\nlibrary(\"tidymodels\")\nlibrary(\"rsample\")\ndata(\"credit_data\", package = \"modeldata\")\n\nset.seed(55)\ntrain_test_split = initial_split(credit_data)\ncredit_train = training(train_test_split)\ncredit_test = testing(train_test_split)\n\nrec = recipe(Status ~ ., data = credit_train) %>%\n  step_knnimpute(all_predictors()) %>%\n  step_dummy(all_predictors(), -all_numeric()) %>%\n  step_center(all_numeric()) %>%\n  step_scale(all_numeric())\n\ntrained_rec = prep(rec, training = credit_train)\n\n# Apply to train and test set\ntrain_data <- bake(trained_rec, new_data = credit_train)\ntest_data  <- bake(trained_rec, new_data = credit_test)\n# Train\nrf = rand_forest(mtry = 12, trees = 200, mode = \"classification\") %>%\n  set_engine(\"ranger\", importance = 'impurity') %>%\n  fit(Status ~ ., data = train_data)\n\n# Predict\nprds = predict(rf, test_data)"},{"path":"appendix.html","id":"mlr3pipelines-2","chapter":"9 Appendix","heading":"9.5.4.2 mlr3pipelines","text":"analysis can performed mlr3pipelines.\nNote, now impute via knn instead via sampling.","code":"\nlibrary(\"data.table\")\nlibrary(\"mlr3\")\nlibrary(\"mlr3learners\")\nlibrary(\"mlr3pipelines\")\ndata(\"credit_data\", package = \"modeldata\")\nset.seed(55)\n\n# Create the task\ntask = as_task_classif(credit_data, target = \"Status\")\n\n# Build up the Pipeline:\ng = po(\"imputesample\", id = \"impute\") %>>%\n  po(\"encode\", method = \"one-hot\") %>>%\n  po(\"scale\") %>>%\n  po(\"learner\", lrn(\"classif.ranger\", num.trees = 200, mtry = 12))\n\n# We can visualize what happens to the data using the `plot` function:\ng$plot()\n\n# And we can use `mlr3's` full functionality be wrapping the Graph into a GraphLearner.\nglrn = as_learner(g)\nresample(task, glrn, rsmp(\"holdout\"))"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
